<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>今天你DEBUG了吗</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-08-11T10:22:09.566Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Patrick Yan</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>数据挖掘-理论与算法（公开课笔记一）</title>
    <link href="http://yoursite.com/2020/08/11/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98-%E7%90%86%E8%AE%BA%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%88%E5%85%AC%E5%BC%80%E8%AF%BE%E7%AC%94%E8%AE%B0%E4%B8%80%EF%BC%89/"/>
    <id>http://yoursite.com/2020/08/11/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98-%E7%90%86%E8%AE%BA%E4%B8%8E%E7%AE%97%E6%B3%95%EF%BC%88%E5%85%AC%E5%BC%80%E8%AF%BE%E7%AC%94%E8%AE%B0%E4%B8%80%EF%BC%89/</id>
    <published>2020-08-11T10:18:37.000Z</published>
    <updated>2020-08-11T10:22:09.566Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://img-blog.csdnimg.cn/20200811181626788.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><a id="more"></a><h2 id="2-Data-Preprocessing-数据预处理"><a href="#2-Data-Preprocessing-数据预处理" class="headerlink" title="2 Data Preprocessing 数据预处理"></a>2 Data Preprocessing 数据预处理</h2><h3 id="2-1-1-Data-Cleaning-数据清洗"><a href="#2-1-1-Data-Cleaning-数据清洗" class="headerlink" title="2.1.1 Data Cleaning 数据清洗"></a>2.1.1 Data Cleaning 数据清洗</h3><p>获取的数据可能不可用，存在缺数据、数据错误、噪音等问题，这些都会导致程序无法运行。因此在处理之前要进行清洗等操作。</p><p>对于缺失少量的数据，可以进行适当的推测，或者规则界定，或者直接填均值，这个处理方法主要依经验判断。 More art than science.</p><p>离群点和异常点是有区别的，要谨慎的区分两者。（如姚明身高可以称为离群点，但不是异常点）</p><h3 id="2-2-1-Outliers-amp-Duplicate-detection-异常值与重复检测"><a href="#2-2-1-Outliers-amp-Duplicate-detection-异常值与重复检测" class="headerlink" title="2.2.1 Outliers &amp; Duplicate detection 异常值与重复检测"></a>2.2.1 Outliers &amp; Duplicate detection 异常值与重复检测</h3><p>异常是相对的，在计算异常值时要考虑相对性，可以尝试与近邻值相比较。</p><p>相同的信息可以用不同的形式来表示，而计算机可能无法识别这些重复。</p><p>我们可以尝试使用滑动窗口，将某数据与部分之前的数据进行比（前提是相似的数据是紧邻的）<br><img src="https://img-blog.csdnimg.cn/2020080516351878.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="2-3-1-Type-conversion-amp-sampling-类型转换与采样"><a href="#2-3-1-Type-conversion-amp-sampling-类型转换与采样" class="headerlink" title="2.3.1 Type conversion &amp; sampling 类型转换与采样"></a>2.3.1 Type conversion &amp; sampling 类型转换与采样</h3><p>数据有很多形式，在编码时需要选择合适的方式进行处理，才能进行数据挖掘。在编码时，编码不同方式对问题结构和挖掘结果都会产生影响。</p><p>采样：数据并不是都需要的，而且IO操作对数据库并不友好，我们可以根据需要可以进行适当的采样，能有效的降低数据量，加快处理速度。</p><p>不平衡的数据集对挖掘结果也会有影响，在采样时也需要谨慎选择（可以尝试“复制”一些较少点加入数据集）。在做分类的情况下，当数据过多时，也可以尝试识别出边缘点并进行加入数实验据集，可以有效地降低实验时的数据量；当某一类别的数据过少，可以多采集一点，或生成一些邻近的点。如：<br><img src="https://img-blog.csdnimg.cn/20200806144007746.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="2-4-1-Data-Description-amp-Visualization-数据描述与可视化"><a href="#2-4-1-Data-Description-amp-Visualization-数据描述与可视化" class="headerlink" title="2.4.1 Data Description &amp; Visualization 数据描述与可视化"></a>2.4.1 Data Description &amp; Visualization 数据描述与可视化</h3><p>数据标准化</p><ul><li><img src="https://img-blog.csdnimg.cn/20200806145913666.png" alt="在这里插入图片描述">可以归一化到[0,1]之间</li><li><img src="https://img-blog.csdnimg.cn/20200806145931179.png" alt="在这里插入图片描述"></li></ul><h3 id="2-5-1-Feature-Selection-特征选择"><a href="#2-5-1-Feature-Selection-特征选择" class="headerlink" title="2.5.1 Feature Selection 特征选择"></a>2.5.1 Feature Selection 特征选择</h3><p>在做数据挖掘的时候，要在多个属性中进行挑选，否则会影响算法的效率与资源的消耗，所以属性需要分好坏。</p><p><strong>熵-(Entropy)</strong>：在数据挖掘中，代表变量/系统的不确定性。<br><img src="https://img-blog.csdnimg.cn/20200806151014650.png#pic_center" alt="在这里插入图片描述"><br>此处b一般取2，但也可以为其他值，取2时单位为比特；b取e时，单位为纳特。</p><p><strong>信息增益-(information gain)</strong>：在数据挖掘中代表属性的价值。<br>ΔH(x)=H(x1)−H(x2)ΔH(x) ，即增加属性前后熵的差值。</p><p>属性组的优劣并不是单纯的单个属性好坏的叠加，因为属性间可能有影响或者关联。</p><p><strong>搜索最优属性-(Subset Seach)</strong>：为了在众多属性中挑选最优的属性组，会有众多的排列组合，所以需要通过适当的算法。</p><p><strong>Branch and Bound</strong>：寻找最优的属性组。建立属性树，如有单调的情况，可以通过剪枝方法加速。启发式、模拟退火等算法同样适用。</p><h3 id="2-6-1-Feature-Extraction-特征提取"><a href="#2-6-1-Feature-Extraction-特征提取" class="headerlink" title="2.6.1 Feature Extraction 特征提取"></a>2.6.1 Feature Extraction 特征提取</h3><p>引例：照片把三维信息压缩（投影）为二维，信息损失很多，人们通过照片的特征，依旧可以识别照片中的物体。</p><p>不同的属性有不同的区分度，越重要的属性越能区分物体，更少产生重复。所以属性有好坏。</p><p><strong>主成分分析技术-PCA(principle components analysis)</strong>：把空间中的点投影到空间中的一根线上（相当于n维降到1维），相当于降维，投影方向（相当于属性）有好坏。最优情况为最大特征向量。另外，投影后实际上相当于丢失了一部分信息。</p><h3 id="2-7-1-Linear-Discriminant-Analysis-线性判别分析"><a href="#2-7-1-Linear-Discriminant-Analysis-线性判别分析" class="headerlink" title="2.7.1 Linear Discriminant Analysis 线性判别分析"></a>2.7.1 Linear Discriminant Analysis 线性判别分析</h3><p>PCA是一种无监督学习，虽然可以进行降维，但是在分类时，PCA可能无法进行合理的投影方向选择，导致无法进行分类。</p><p><img src="https://img-blog.csdnimg.cn/20200806154859358.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>举例：如果向1轴投影，可以明显区分两个数据集，但是PCA会向2轴投影，导致两个数据在低维混合，看不出分类。</p><p><strong>线性判别分析-LDA(Liner Discriminant Analysis)</strong>：相对PCA，LDA在投影的同时保留类的区分信息（指相同组中数据尽可能近，不同组中数据尽可能远）</p><p><strong>费舍尔准则-(Fisher Criterion)</strong>：用于评估投影效果好坏（整体上J越大越好）<br><img src="https://img-blog.csdnimg.cn/20200806155140221.png" alt="在这里插入图片描述"><br>其中μ为组中心点位置（越远越好），S为各组离散程度（越小越好）。<br>调整投影方向，J值变化。<br><img src="https://img-blog.csdnimg.cn/20200806155305237.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="2-7-2-LDA-Example-线性判别分析例子"><a href="#2-7-2-LDA-Example-线性判别分析例子" class="headerlink" title="2.7.2 LDA Example 线性判别分析例子"></a>2.7.2 LDA Example 线性判别分析例子</h3><p>PCA与LDA的结果不一定相同。<br><img src="https://img-blog.csdnimg.cn/20200806160712616.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>LDA不局限于二分类，可以容易的拓展到n类的情况，此时J的分子为各个组中心点离所有点中心点的距离。此时n分类最高可以投影到n−1维上。如下，是三类问题投影到两个轴上（这两个轴不一定正交）。<br><img src="https://img-blog.csdnimg.cn/20200806161213942.PNG#pic_center" alt="在这里插入图片描述"><br>往两个轴投影如下：<br><img src="https://img-blog.csdnimg.cn/20200806161355302.PNG#pic_center" alt="在这里插入图片描述"><br>在一些特殊情况下（两类比较近μ1=μ2或者两个分类维互相垂直），LDA的分类可能不如PCA，在使用时要谨慎分析。<br><img src="https://img-blog.csdnimg.cn/2020080616180288.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="3-Bayes-amp-Decision-Tree-Classifiers-贝叶斯和决策树"><a href="#3-Bayes-amp-Decision-Tree-Classifiers-贝叶斯和决策树" class="headerlink" title="3 Bayes &amp; Decision Tree Classifiers 贝叶斯和决策树"></a>3 Bayes &amp; Decision Tree Classifiers 贝叶斯和决策树</h2><h3 id="3-1-1-Bayes、Decision-Tree-贝叶斯、决策树（概率基础）"><a href="#3-1-1-Bayes、Decision-Tree-贝叶斯、决策树（概率基础）" class="headerlink" title="3.1.1 Bayes、Decision Tree 贝叶斯、决策树（概率基础）"></a>3.1.1 Bayes、Decision Tree 贝叶斯、决策树（概率基础）</h3><p>分类是一种有监督学习，会有标签，这也是分类与聚类的差别。</p><h3 id="3-2-1-Naive-Bayes-Classifier-朴素贝叶斯公式"><a href="#3-2-1-Naive-Bayes-Classifier-朴素贝叶斯公式" class="headerlink" title="3.2.1 Naive Bayes Classifier 朴素贝叶斯公式"></a>3.2.1 Naive Bayes Classifier 朴素贝叶斯公式</h3><p>注意区分：先验后验、相关独立。</p><p><strong>拉普拉斯平滑-(Laplace Smoothing)</strong>：如果出现数据库中从未存储的特殊点，一项为0会导致整个概率公式值为0。<br><img src="https://img-blog.csdnimg.cn/20200807165636583.png" alt="在这里插入图片描述"><br>（加1使得结果不会为0）</p><p>贝叶斯公式的实际应用：<br>我们可以提取文章中的部分单词，根据出现与否计算文章分类概率，但是这种情况会导致计算量非常大。进一步改进，词袋模型-(Word Bag)，不考虑单词出现的位置，根据单词在文章中出现的频率来计算文章分类概率。</p><h3 id="3-3-1-Decision-Tree-决策树"><a href="#3-3-1-Decision-Tree-决策树" class="headerlink" title="3.3.1 Decision Tree 决策树"></a>3.3.1 Decision Tree 决策树</h3><p>将不同因素设定规则，分层计算，形成决策树。决策树是可解释的(if…then…)，是决策树的一大优势。</p><p>根据判断顺序不同，一个数据集可以建立很多树。运用奥卡姆剃刀，我们可以选择同样分类效率下，尽量简单的决策树。</p><h3 id="3-4-1-Decision-Tree-Framework-决策树的建立策略"><a href="#3-4-1-Decision-Tree-Framework-决策树的建立策略" class="headerlink" title="3.4.1 Decision Tree Framework 决策树的建立策略"></a>3.4.1 Decision Tree Framework 决策树的建立策略</h3><p>把分类能力强的节点放在靠近根节点的情况可以大大缩小树的规模，于是决策树的建立与属性判断有关。</p><p><strong>熵-(Entropy)</strong>：在变量选择中，代表属性的不确定性。最大值为1，此时最不确定。最小值为0，此时最确定。<br><img src="https://img-blog.csdnimg.cn/20200807172730339.png" alt="在这里插入图片描述"><br>此处b一般取2，但也可以为其他值，取2时单位为比特。</p><p><strong>过学习-(Overfitting)</strong>：在训练集中a比b效果好，但在测试集中b比a好，此时就发生了过学习（通用概念）</p><p>决策树并不是越大越好，也存在过学习(overfitting)的情况。为避免过学习，提高泛化能力，有两种剪枝策略：</p><ul><li>限制树的深度</li><li>进行剪枝（将某些子节点合并到父节点上）<br><img src="https://img-blog.csdnimg.cn/20200807173859785.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li></ul><p>简单的决策树 ID3 建立过程：</p><ol><li>寻找能把数据集分的最开的属性，进行分割</li><li>如果子集纯，那么停止分割，如果不纯，继续从1分割（如果没有属性，停止分割，标签少数服从多数）</li></ol><p><img src="https://img-blog.csdnimg.cn/20200807173423588.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="4-Neural-Networks-神经网络"><a href="#4-Neural-Networks-神经网络" class="headerlink" title="4 Neural Networks 神经网络"></a>4 Neural Networks 神经网络</h2><h3 id="4-1-1-Perceptrons-感知机"><a href="#4-1-1-Perceptrons-感知机" class="headerlink" title="4.1.1 Perceptrons 感知机"></a>4.1.1 Perceptrons 感知机</h3><p>神经网络是计算机程序对大脑中神经元的简化抽象模拟。</p><p>单个神经元开关速度（0,1互换）相对计算机比较慢，但人脑是多个神经元联合分布式处理的，处理速度很快。</p><p><strong>感知机-(Perceptrons)</strong>：单个的神经元，n个输入，n+1个权重（<strong>w<sub>0</sub></strong>，避免所有点都过原点），输出0或1。<br><img src="https://img-blog.csdnimg.cn/20200811151049881.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="4-2-1-感知机的应用"><a href="#4-2-1-感知机的应用" class="headerlink" title="4.2.1 感知机的应用"></a>4.2.1 感知机的应用</h3><p>感知机可以理解为一个线性分类器，对于线性不可分等问题无能为力。</p><p><strong>梯度下降法-(Gradient Descent)</strong>：取权重时不同权重对结果影响不同，所以让误差不断梯度下降。</p><p><strong>批处理学习-(Batch Learning)</strong>：计算一批数据，将更改值保存在Δw中，计算完后统一修改w。</p><p><strong>随机学习-(Stochastic Learning)</strong>：一旦出现误差，就修改w。</p><h3 id="4-3-1-Multilayer-Perception-多层感知机神经网络"><a href="#4-3-1-Multilayer-Perception-多层感知机神经网络" class="headerlink" title="4.3.1 Multilayer Perception 多层感知机神经网络"></a>4.3.1 Multilayer Perception 多层感知机神经网络</h3><p><img src="https://img-blog.csdnimg.cn/20200811152204850.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>多层感知机通过在输入和输出间添加隐含层来解决线性不可分的问题，提升效率。</p><p>多次感知机解决线性不可分问题：将输入的线性不可分问题转化为线性可分问题，输出到隐层，由隐含层解决并输出。</p><h3 id="4-4-1-分类器学习算法"><a href="#4-4-1-分类器学习算法" class="headerlink" title="4.4.1 分类器学习算法"></a>4.4.1 分类器学习算法</h3><p><strong>误差反向传播算法-BP(Backpropagation Rule)</strong>：思想类似于感知机，用误差对某个权重的输入求偏导。但在多层感知机网络中，由于隐含层的期望值并不知道，所以没法得知隐含层的误差，因此输出后根据结果加权反向反馈，修改参数。</p><p>在求导时，很容易使参数掉进局部最优点，导致误差不在下降，而神经网络恰恰有很多局部最优点。为了解决这个问题，</p><ol><li>我们可以尝试多次从不同点、不同方向出发，寻找最优参数。</li><li>也可以增加“冲量”，相当于“惯性”，让参数点冲过一些比较小的局部最优点。</li><li>也可以尝试改变学习率，较大的学习率可以直接大踏步跳过一些比较小的局部点，</li><li>而特殊的学习率会在一些特殊情况产生震荡，左右横跳导致无法收敛，多次学习也是解决办法。</li></ol><p>↑↑↑可以结合小球滚斜坡、走路上下坡理解局部最优点↑↑↑</p><p>同样，神经网络也有过学习问题。所以我们既要设置训练误差-(Training Error)—–随时间逐步降低，又要设置<strong>校验误差-(Validation Error)</strong>—–随时加先降后升。在校验误差拐点停止训练即可。</p><h3 id="4-5-1-Beyond-BP-Networks-其他的神经网络算法"><a href="#4-5-1-Beyond-BP-Networks-其他的神经网络算法" class="headerlink" title="4.5.1 Beyond BP Networks 其他的神经网络算法"></a>4.5.1 Beyond BP Networks 其他的神经网络算法</h3><p><strong>Elman Network</strong>：此算法有一定的记忆性，通过之前的输入推出答案，输出不仅仅取决于当前的输入，还取决于之前的输入。</p><p><strong>Hopfield Network</strong>：是一个全连接神经网络。类似于人大脑的记忆功能，利用收敛到局部最小值，实现联想记忆。</p><p>同样，不存在万能的神经网络，每个网络算法都有自己的适应性，而且神经网络的训练时间很长，但训练完成后反应很快。</p><p>神经网络的可解释性很差（只有权重）。</p><h2 id="5-Support-Vector-Machines-支持向量机"><a href="#5-Support-Vector-Machines-支持向量机" class="headerlink" title="5 Support Vector Machines 支持向量机"></a>5 Support Vector Machines 支持向量机</h2><h3 id="5-1-1-Support-Vector-Machine（SVM）-支持向量机"><a href="#5-1-1-Support-Vector-Machine（SVM）-支持向量机" class="headerlink" title="5.1.1 Support Vector Machine（SVM） 支持向量机"></a>5.1.1 Support Vector Machine（SVM） 支持向量机</h3><p>原理：输入维度向高维映射并进行分类（升维），仍然是线性分类器的一种，真正有效的点只有support vector，最大化margin。<br><img src="https://img-blog.csdnimg.cn/20200811163441130.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>B和A相比，把点分得更均匀，无偏。<br><img src="https://img-blog.csdnimg.cn/20200811163744652.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p><strong>边缘-(margin)</strong>：训练集中分界面可以上下平移的区域，margin越大，错误越不会影响结果，参数越好。</p><p><strong>支持点-(Support Vectors)</strong>：导致平移边界的点，决定了分界面能移动的范围。其他的点对分界没有影响，可忽略。<br><img src="https://img-blog.csdnimg.cn/20200811163918817.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="5-2-1-Linear-SVM-线性SVM"><a href="#5-2-1-Linear-SVM-线性SVM" class="headerlink" title="5.2.1 Linear SVM 线性SVM"></a>5.2.1 Linear SVM 线性SVM</h3><p>目标：先分样本（必须分对所有的点），再求最优值</p><p>数学工具：拉格朗日乘数法</p><p>但事实上很难完美的分对所有点，所以放宽条件（soft margin），允许少数点可以不满足约束条件。</p><p>本质上还是一个线性分类器，仍然无法解决线性不可分问题。（soft margin只能解决数据有噪点的情况）</p><h3 id="5-3-1-Non-linear-SVM-非线性SVM"><a href="#5-3-1-Non-linear-SVM-非线性SVM" class="headerlink" title="5.3.1 Non-linear SVM 非线性SVM"></a>5.3.1 Non-linear SVM 非线性SVM</h3><p>目的：弥补线性SVM不能解决线性不可分问题的缺陷。</p><p>原理：输入坐标点向新坐标空间映射并进行分类，可以是升维，也可以是同维度转化。如下图，第一个线性可分，但第二个线性不可分，则将第二个映射到新的空间。<br><img src="https://img-blog.csdnimg.cn/20200811165813899.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>映射的结果并不唯一，如下图。<br><img src="https://img-blog.csdnimg.cn/20200811170208941.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200811170233439.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>映射方法一般是固定的，m维一般升到 <strong>m<sup>2</sup>/2</strong> 维进行分类。</p><p>通过引入<strong>核函数（Kernel Function）</strong> 进行等价变换，让高维空间的计算等价于低维空间的运算，有效降低运算量。</p><p>在处理文本数据时，可以使用字符串核函数，处理字符串的子串，得到相应的式子进行分类。</p><p>模型能力指数（model capacity）：若h个点无论怎么打标签，模型都能区分（shatter），那么该模型能力指数就是h。<br><img src="https://img-blog.csdnimg.cn/20200811173103773.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20200811181626788.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center&quot; alt=&quot;在这里插入图片描述&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
    
      <category term="SVM" scheme="http://yoursite.com/tags/SVM/"/>
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="Decision Tree" scheme="http://yoursite.com/tags/Decision-Tree/"/>
    
  </entry>
  
  <entry>
    <title>有序/无序分类变量的统计推断</title>
    <link href="http://yoursite.com/2020/08/04/%E6%9C%89%E5%BA%8F_%E6%97%A0%E5%BA%8F%E5%88%86%E7%B1%BB%E5%8F%98%E9%87%8F%E7%9A%84%E7%BB%9F%E8%AE%A1%E6%8E%A8%E6%96%AD/"/>
    <id>http://yoursite.com/2020/08/04/%E6%9C%89%E5%BA%8F_%E6%97%A0%E5%BA%8F%E5%88%86%E7%B1%BB%E5%8F%98%E9%87%8F%E7%9A%84%E7%BB%9F%E8%AE%A1%E6%8E%A8%E6%96%AD/</id>
    <published>2020-08-04T11:22:11.000Z</published>
    <updated>2020-08-04T11:22:37.040Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://img-blog.csdnimg.cn/20200804191101247.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><a id="more"></a><h2 id="1-有序分类变量的统计推断——非参数检验"><a href="#1-有序分类变量的统计推断——非参数检验" class="headerlink" title="1 有序分类变量的统计推断——非参数检验"></a>1 有序分类变量的统计推断——非参数检验</h2><h3 id="1-1-非参数检验概述"><a href="#1-1-非参数检验概述" class="headerlink" title="1.1 非参数检验概述"></a>1.1 非参数检验概述</h3><p>如果想要检验两个正态总体是否具有相同的均数 ，做t检验即可，这是一个典型的参数统计方法。参数统计方法往往假设统计总体的分布形态已知，但是在更多的实际场合常常由于缺乏足够信息 ，无法合理地去假设一个总体具有某种分布形式，此时就不能去使用相应的参数方法了。推而广之，不能使用参数方法的情形可能是当不知道所研究样本来着总体的具体分布，或已知总体分布与检验所要求的条件不符；数据的测量尺度是名义和顺序尺度，甚至某些变量可能无法稍确测量 ，均值、方差的计算已经没有意义时，但是，此时有的人却忽略参数统计方法的前提，仍然牵强地使用参数方法，面对由此得到的不合理结果却不知问题何在。 实际上，正确的思路应当是放弃对总体分布参数的依赖，转而寻求更多的纯粹来自数据的信息，这就是所谓的非参数统计方法。</p><p>非参数统计方法主要用于那些总体分布不能用有限个实参数来刻画，或者不考虑被研究的对象为何种分布以及分布是否已知的情形，它对总体分布几乎没有什么假定，只是有时对分布的形状做一些诸如连续、对称等简单假设。但实际上，并非说在推断中什么分布参数都不利于，而是指推断过程和结论均与原总体参数无关。例如，最常用的秩和检验就是基于秩次的分布特征推导出来的。</p><p>和参数方法相比，非参数检验方法的优势如下：</p><ul><li>稳健性。因为对总体分布的约束条件大大放宽，不至于因为统计中的假设过程过分理想化而无法切合实际情况，不至于对个别偏离较大的数据太敏感</li><li>对数据的测量尺度无约束时，对数据的要求也不严格，什么数据类型都可以做</li><li>适用于小样本、无分布样本、数据污染样本、混杂样本等</li></ul><p>非参数检验预备知识<br><img src="https://img-blog.csdnimg.cn/20200804112335700.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200804112427383.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="1-2-两个配对样本的非参数检验"><a href="#1-2-两个配对样本的非参数检验" class="headerlink" title="1.2 两个配对样本的非参数检验"></a>1.2 两个配对样本的非参数检验</h3><p>事实上，配对样本的非参数检验方法的基本逻辑和参数检验并无区别，也是首先 求出配对数据的差值。然后考察差值总体的中心位置是否为0。只是由于不再涉及分布类型，因此不能使用均数这一与总体分布有关的参数加以检验。一般而言，相应的假设都被归结为考察总体中位数是否为0。</p><ul><li>H<sub>0</sub>：差值的总体中位数M<sub>d</sub>=0；</li><li>H<sub>1</sub>：差值的总体中位数M<sub>d</sub>≠0；</li></ul><p>但是，仅有假设是不够的，还需要一个合适的统计量。构建统计量的方法如下：</p><ul><li>Sign符号检验<br>原理：如果两个配对样本实际上无区别，则将样本数据相减所得的差值应当大致有一半正，一半负。那么若H<sub>0</sub>成立，则S<sup>+</sup>，S<sup>-</sup>大体相等，都服从二项分布B(n,0.5)。显然，符号检验没有利用这些差值的大小所包含的信息，因此它虽然简单易行，但检验效能较低，精度较差。一般而言，这种方法更适用于对无法用数字计量的情况进行比较，如资料本身就是二分类，对于连续性的资料则最好不使用。</li><li>Wilcoxon符号秩检验<br>既考虑样本差值的符号，又考虑到差值的顺序。若差值d<sub>i</sub>为连续变量并且服从正态分布，则一般可以用t检验，若不是正态分布，就只能采用非参数分析方法。</li><li>McNemar<br>配对卡方检验，只适用于二分类资料，它考察的重点是两组间分类的差异，对于相同的分类忽略不计，适用于自身对照设计，用于分析处理前后的变化情况。</li><li>Marginal Homogeneity<br>是McNemar法向多分类情形的扩展，适用于资料为有序分类的情况。</li></ul><h3 id="1-3-两个独立样本的非参数检验"><a href="#1-3-两个独立样本的非参数检验" class="headerlink" title="1.3 两个独立样本的非参数检验"></a>1.3 两个独立样本的非参数检验</h3><ul><li>Mann-Whitney U检验<br>它是和参数t检验相对应的一种非参数检验方法，用来检验两个独立样本是否取自同一总体，就是两样本秩和检验，在检验时利用了大小次序，即检验A样本中的数值是否多数都大于B样本。</li><li>Kolmogorov-Smirnov Z检验<br>和单样本检验中K-S检验是一类的，可以对连续性资料的分布情况加以考察。</li><li>Moses Extreme Reactions检验<br>结果为单侧检验。如果施加的处理使得某些个体出现正向效应，而另一些个体出现负向效应，就应当采用该检验方法。</li><li>Wald-Wolfowitz Runs检验<br>属于游程检验的一种，即检验的是总体分布情况是否相同。</li></ul><h3 id="1-4-多个独立样本的非参数检验"><a href="#1-4-多个独立样本的非参数检验" class="headerlink" title="1.4 多个独立样本的非参数检验"></a>1.4 多个独立样本的非参数检验</h3><ul><li>Kruska-Wallis检验</li><li>Median中位数法</li></ul><h3 id="1-5-多个配对样本的非参数检验"><a href="#1-5-多个配对样本的非参数检验" class="headerlink" title="1.5 多个配对样本的非参数检验"></a>1.5 多个配对样本的非参数检验</h3><ul><li>Friedman检验</li><li>Kendall协和系数</li><li>Cochran检验方法</li></ul><h2 id="2-无序分类变量的统计推断——卡方检验"><a href="#2-无序分类变量的统计推断——卡方检验" class="headerlink" title="2 无序分类变量的统计推断——卡方检验"></a>2 无序分类变量的统计推断——卡方检验</h2><h3 id="2-1-卡方检验概述"><a href="#2-1-卡方检验概述" class="headerlink" title="2.1 卡方检验概述"></a>2.1 卡方检验概述</h3><p><img src="https://img-blog.csdnimg.cn/20200804182546763.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200804182559540.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200804182637318.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>卡方检测的用途</p><ul><li>检验某个连续变量的分布是否与某种理论分布相一致。如是否符合正态分布 、是否服从均匀分布、是否服从Poisson分布等</li><li>检验某个分类变应各类的出现概率是否等于指定慨事。如在36选7的彩票抽奖中，每个数字出现的概率是否各为1 /36；掷硬币时正反两面出现的慨率是否均为 0.5</li><li>检验某两个分类变量是否相互独立。如吸烟(二分类变量：是、否)是否与呼吸道疾病(二分类变量：是、否)有关；产品原料种类(多分类变量)是否与产品合格(二分类变量)有关</li><li>检验控制某种或某几种分类变量的作用以后，另两个分类变量是否相互独立。如在上例中，控制性别、年龄因素影响以后 ，吸烟是否和呼吸道疾病有关 ，控制产品加工工艺的影响后，产品原料类别是否与产品合格有关</li><li>检验某两种方法的结果是否一致。如果用两种诊断方法对同一批人进行诊断，其结果是否一致：采用两种方法对客户进行价值类别预测，预测结果是否一致</li></ul><h3 id="2-2-单样本案例：考察抽样数据的性别分布"><a href="#2-2-单样本案例：考察抽样数据的性别分布" class="headerlink" title="2.2 单样本案例：考察抽样数据的性别分布"></a>2.2 单样本案例：考察抽样数据的性别分布</h3><p><img src="https://img-blog.csdnimg.cn/20200804184454646.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="2-3-两样本案例：不同收入级别家庭的轿车拥有率比较"><a href="#2-3-两样本案例：不同收入级别家庭的轿车拥有率比较" class="headerlink" title="2.3 两样本案例：不同收入级别家庭的轿车拥有率比较"></a>2.3 两样本案例：不同收入级别家庭的轿车拥有率比较</h3><p><img src="https://img-blog.csdnimg.cn/20200804184913205.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200804185032730.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200804185056978.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="2-4-一致性检验与配对卡方检验"><a href="#2-4-一致性检验与配对卡方检验" class="headerlink" title="2.4 一致性检验与配对卡方检验"></a>2.4 一致性检验与配对卡方检验</h3><p>关联程度的测量：卡方检验从定性的角度指出是否存在相关性，而各种关联指标从定量的角度指出相关的程度如何。不同的指标适用于不同类型的变量。</p><ul><li>RR值是一个概率的比值，是指实验组人群反应阳性概率与对照组人群反应阳性概率的比值，用于反映实验因素与反应阳性的关联程度</li><li>OR值是比值的比，是反应阳性人群中实验因素有无的比例与反应阴性人群中实验因素有无的比例之比。在下列两个条件均满足时可用于估计 RR 值：所关注的事件发生的概率比较小(&lt;0. I)，这个条件保证比数比能对相对危险度有一个好的近似；所涉及的研究是病例对照研究</li><li>在SPSS中，在交叉表过程的”统计量” 子对话框中选巾”风险”复选框会自动给出 OR与RR值</li></ul><p>Kappa检验与配对卡方检验：Kappa 一致性检验用于对两种方法结果的一致程度进行评价；配对卡方检验则用于分析两种分类方法的分类结果是否有差异</p><h3 id="2-5-分层卡方检验"><a href="#2-5-分层卡方检验" class="headerlink" title="2.5 分层卡方检验"></a>2.5 分层卡方检验</h3><p>分层卡方检验：分层卡方是把研究对象分解成不同层次，按各层对象来进行行变量与列变量的独立性研究。可在去除分层因素混杂的影响下更准确地对行列变量的独立性进行研究。在SPSS中，在交叉表过程的”统计量”子对话框中选中”Cochran’ s and Mantel- Haenszel统计量”复选框会自动给出分层卡方检验结果。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20200804191101247.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center&quot; alt=&quot;在这里插入图片描述&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="统计学" scheme="http://yoursite.com/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6/"/>
    
    
      <category term="非参数检验" scheme="http://yoursite.com/tags/%E9%9D%9E%E5%8F%82%E6%95%B0%E6%A3%80%E9%AA%8C/"/>
    
      <category term="卡方检验" scheme="http://yoursite.com/tags/%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C/"/>
    
  </entry>
  
  <entry>
    <title>连续变量的统计推断</title>
    <link href="http://yoursite.com/2020/08/03/%E8%BF%9E%E7%BB%AD%E5%8F%98%E9%87%8F%E7%9A%84%E7%BB%9F%E8%AE%A1%E6%8E%A8%E6%96%AD/"/>
    <id>http://yoursite.com/2020/08/03/%E8%BF%9E%E7%BB%AD%E5%8F%98%E9%87%8F%E7%9A%84%E7%BB%9F%E8%AE%A1%E6%8E%A8%E6%96%AD/</id>
    <published>2020-08-03T11:14:41.000Z</published>
    <updated>2020-08-03T11:14:56.620Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://img-blog.csdnimg.cn/20200803191249417.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><a id="more"></a><h2 id="1-t检验"><a href="#1-t检验" class="headerlink" title="1 t检验"></a>1 t检验</h2><h3 id="1-1-t检验概述"><a href="#1-1-t检验概述" class="headerlink" title="1.1 t检验概述"></a>1.1 t检验概述</h3><p>在针对连续变量的统计推断方法中，最常用的有t检验和方差分析两种，其中t检验是最基本的检验方法。</p><p>对于$\overline{X}$ - μ 仅看这一个数字很难判断出这种差异究竟是大是小。为此需要找到某种方式对这一差值进行标准化。标准化的思路是将该差值除以某种表示离散程度的指标。标准化变换：<br><img src="https://img-blog.csdnimg.cn/20200801175444851.PNG#pic_center" alt="在这里插入图片描述"><br>其中，样本均数$\overline{X}$的分布规律为正态分布n(μ，σ<sup>2</sup>/N)，<img src="https://img-blog.csdnimg.cn/2020080117571560.PNG#pic_center" alt="则U服从标准正态分布N（0,1）。"><br>U检验看上去虽然很好，却实际上毫无用处，因为σ<sub>$\overline{X}$</sub>在 计算中需要使用总体标准差，但在实际工作中和总体均数一样也常常未知，能够使用的仅仅是样尔标准差s。</p><p>如果用样本标准量来代替总体标准差来进行计算，即s<sub>$\overline{X}$</sub> = s / 根号n ，则由于样本标准差s会随样本而变。相应的标化统计量的变异程度要大于 U，它的密度曲线看上去有些像标准正态分布但是尖一些而且尾巴长一些，这种分布称为t分布，相应的标化后统计量也就被称为t统计量。显然，t统计量的分布规律是和样本量有关的，更准确地说是和自由度(v/df)有关的。自由度是信息量的度量，描述了样本数据能自由取值的个数，在t分布中由于有给定的样本均数这一限定，所以自由度为 v = n - 1。从图中可以看出，自由度增加时它的分布就逐渐接近标准正态分布了。因此，在样本量较大时，可以用标准正态分布来近似t分布。<br><img src="https://img-blog.csdnimg.cn/20200801163917676.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200801180719874.PNG#pic_center" alt="在这里插入图片描述"><br>t检验仍然采用小概率反证法原理，其基本思想是：首先假设H<sub>0</sub>成立 ，然后考察在H<sub>0</sub>成立的条件下，按照现有样本量做随机抽样在相应的总体中抽到现有样本，以及比现有样本与总体的差异更大的样本的累积概率，如果相应的概率 P&lt;=α( 检验水准)，因 拒绝H<sub>0</sub>假设，接受对立的H<sub>1</sub>假设，认为现有样本井非来自于所假定的总体。</p><p>另外，根据具体的设计方案和希望解决的问题不同，又可以将t检验分为单样本t检验、两样本t检验和配对t检验等，但它们的基本原理都是相同的。</p><p>t检验在SPSS中基本上被集中在“比较均值”子菜单中，具体如下：</p><ul><li>单样本t检验过程：进行样本均数与已知总体均数的比较</li><li>独立样本t检验过程：进行两样本均数差别的比较，及通常所说的两组资料的t检验</li><li>配对样本t检验过程：进行配对资料的均数比较，即配对t检验</li></ul><h3 id="1-2-成组设计两样本均数的比较"><a href="#1-2-成组设计两样本均数的比较" class="headerlink" title="1.2 成组设计两样本均数的比较"></a>1.2 成组设计两样本均数的比较</h3><p>作为参数方法，t检验也有适用的条件，但相对而言它比较稳健，对使用条件的违反有一定的耐受性。但如果使用条件被严重违反，则可以采用校正的t检验，或者换用非参数方法来进行分析。</p><p>在应用t检验进行两样本均数的比较时，要求数据满足以下条件：</p><ul><li>独立性，各观察值之间是相互独立的，不能相互影响</li><li>正态性，各个样本均来自于正态分布的总体</li><li>方差齐性，各样本所在总体的方差相等</li></ul><h2 id="2-单因素方差分析"><a href="#2-单因素方差分析" class="headerlink" title="2 单因素方差分析"></a>2 单因素方差分析</h2><h3 id="2-1-方差分析概述"><a href="#2-1-方差分析概述" class="headerlink" title="2.1 方差分析概述"></a>2.1 方差分析概述</h3><p>方差分析（ANOVA）的理论基础：将总变异分解为由研究因素所造成的部分和 由抽样误差所造成的部分，通过比较来自于不同部分的变异，借助F分布做出统计推断。后人又将线性模型的思想引入方差分析，更是为这一方法提供了近乎无穷的发展空间。</p><p>单因素方差分析所针对的是多组均数间的比较。它的基本思想：方差分析是基于变异分解的思想进行的，在单因素方差分析中，整个样本的变异可以看成由如下两个部分构成：</p><p>总变异 = 随机变异 + 处理因素导致的变异</p><p>其中随机变异是永远存在的，确定处理因素导致的变异是否存在就是所要达到的研究目标，即只要能证明它不等于0，就等同于证明了处理因素的确存在影响。</p><p><img src="https://img-blog.csdnimg.cn/20200803184705646.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>方差分析的检验统计量可以简单地理解为利用随机误差作为尺度来衡量各组间的变异，即<br>F = 组间变异测量指标 / 组内变异测量指标</p><p>则在H<sub>0</sub>成立时，处理所造成的各组间均数的差异应为0（理论上应为0，但由于抽样误差不可能恰好为0），即<br>μ1 = μ2 = μ3 = … … = μk</p><p><img src="https://img-blog.csdnimg.cn/20200803185223868.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/2020080318530773.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>单因素方差分析的应用条件</p><ul><li>观察对象是来自于所研究因素的各个水平之下的独立随机抽样(Independence)</li><li>每个水平下的应变量应当服从正态分布( Normality)</li><li>各水平下的总体具有相同的方差(Homoscedascity)<br>其实，与t检验的应用条件大同小异，概括起来就是独立性、正态性和方差齐性。</li></ul><p>方差分析拒绝H<sub>0</sub>只能说明各组之间存在差异，但并不足以说明各组之间的关系。利用多重比较可以初步判断各组间的关系。</p><p>多重比较可以分为事前计划好的比较和事后比较。前者往往借助于 Contrast，而很多种不同的方法，这些方法的核心问题都是如何控制总的一类错误的大小。</p><p>在分组变量包含次序信息时，如果方差分析给出了各组间差异有统计学意义的结论，井且 Means-Plot 提示各组均数的某种趋势时，可以利用趋势分析探讨观察值与分组变量间间的数量依存关系。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20200803191249417.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center&quot; alt=&quot;在这里插入图片描述&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="统计学" scheme="http://yoursite.com/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6/"/>
    
    
      <category term="t检验" scheme="http://yoursite.com/tags/t%E6%A3%80%E9%AA%8C/"/>
    
      <category term="方差分析" scheme="http://yoursite.com/tags/%E6%96%B9%E5%B7%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>分布类型的检验</title>
    <link href="http://yoursite.com/2020/08/01/%E5%88%86%E5%B8%83%E7%B1%BB%E5%9E%8B%E7%9A%84%E6%A3%80%E9%AA%8C/"/>
    <id>http://yoursite.com/2020/08/01/%E5%88%86%E5%B8%83%E7%B1%BB%E5%9E%8B%E7%9A%84%E6%A3%80%E9%AA%8C/</id>
    <published>2020-08-01T07:56:42.000Z</published>
    <updated>2020-08-01T07:57:19.714Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://img-blog.csdnimg.cn/20200801155450792.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><a id="more"></a><h2 id="1-假设检验的基本思想"><a href="#1-假设检验的基本思想" class="headerlink" title="1 假设检验的基本思想"></a>1 假设检验的基本思想</h2><h3 id="1-1-假设检验的标准步骤"><a href="#1-1-假设检验的标准步骤" class="headerlink" title="1.1 假设检验的标准步骤"></a>1.1 假设检验的标准步骤</h3><ul><li>小概率事件（发生概率很小，如P&lt;=0.05）</li><li>小概率反证法<br>原理：对于一个小慨率事件而言，其对立面发生的可能性显然要大大高于这一小概率事件，可以认为小概率事件在一次试验中不应当发生。因此可以首先假定需要考察的假设是成立的，然后基于此进行推导，来计算一下在该假设所代表的总体中进行抽样研究得到当前样本(及更极端样本)的概率是多少。 如果结果显示这是一个小概率事件，则意味着如果假设是成立的，则在一次抽样研究中竟然就发生了小慨率事件。显然违反小概率原理，因此可以按照反证法的思路推翻所给出的假设，认为它们实际上是不成立的，这就是小概率反证法原理。</li><li>标准步骤：<br>建立假设 &gt; 确立检验水准 &gt; 进行试验 &gt; 选定检验方法 &gt; 确定P值，给出推断结论<br><img src="https://img-blog.csdnimg.cn/20200801151042539.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200801143551591.png#pic_center" alt="在这里插入图片描述"></li></ul><h3 id="1-2-假设检验的两类错误"><a href="#1-2-假设检验的两类错误" class="headerlink" title="1.2 假设检验的两类错误"></a>1.2 假设检验的两类错误</h3><p><img src="https://img-blog.csdnimg.cn/20200801143821196.PNG#pic_center" alt="在这里插入图片描述"></p><ul><li>第一类错误：无效假设H<sub>0</sub>实际上是正确的，但由于抽样误差的原因，或者说恰好发生小概率事件的原因，使得人们错误地拒绝了它，从而犯了”弃真”的错误，统计学上称它为”第一类错误” 。犯第一类错误的概率是人为指定的，就等于检验水准α。</li><li>第二类错误：无效假设H<sub>0</sub>实际上是不正确的，但由于抽样误差的原因，检验中得到的P值大于检验水准，使得人们未能拒绝H<sub>0</sub>，从而犯了”存伪”的错误，统叶学中称它为”第二类错误”，用宇母β表示。和第一类错误不同，犯第二类错误的概率大小在进行假设检验时一般井不知道，但可以根据相关信息进行估计。</li></ul><h2 id="2-正态分布检验"><a href="#2-正态分布检验" class="headerlink" title="2 正态分布检验"></a>2 正态分布检验</h2><h3 id="2-1-K-S-检验的原理"><a href="#2-1-K-S-检验的原理" class="headerlink" title="2.1 K-S 检验的原理"></a>2.1 K-S 检验的原理</h3><p><img src="https://img-blog.csdnimg.cn/20200801150924229.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="2-2-案例"><a href="#2-2-案例" class="headerlink" title="2.2 案例"></a>2.2 案例</h3><p><img src="https://img-blog.csdnimg.cn/20200801152011508.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="3-二项分布检验"><a href="#3-二项分布检验" class="headerlink" title="3 二项分布检验"></a>3 二项分布检验</h2><h3 id="3-1-二项分布检验的原理"><a href="#3-1-二项分布检验的原理" class="headerlink" title="3.1 二项分布检验的原理"></a>3.1 二项分布检验的原理</h3><p>二项分布是对二分类变量的拟合优度检验，用于考察每个类别中观察值的频数与特定二项分布下的预期频数间是否存在统计学差异。其检验原理实际上和K-S检验的原理相同，只是这里使用的是二分变量，是一个离散分布的检验情况。</p><h3 id="3-2-案例"><a href="#3-2-案例" class="headerlink" title="3.2 案例"></a>3.2 案例</h3><p><img src="https://img-blog.csdnimg.cn/20200801151808932.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="4-游程检验"><a href="#4-游程检验" class="headerlink" title="4 游程检验"></a>4 游程检验</h2><h3 id="4-1-游程检验原理"><a href="#4-1-游程检验原理" class="headerlink" title="4.1 游程检验原理"></a>4.1 游程检验原理</h3><p>游程检验是对二分变量的随机检验，它可用于判断观察值的顺序是否为随机的。对于两分类变量，连续数个相同取值的记录称为一个游程，比如下面这个序列<br>0 0 1 1 0 1 1 1 0 0 0 1 0 0 1 0 0 0 1 0<br>它有6个0的游程，其长度为1、2、3的各有2个，并有5个1的游程，其中3个长度为1,1个长度为2，1个长度为3。上面的序列总共有 11 个游程。如用U表示序列总的游程数 ，那么对于上面的序列来讲， U= 11。</p><p>根据游程检验的假设，如果序列是真随机序列，那么游程的总数应当不太多也不太少，比较适中。如果游程的总数极少，就意味着样本由于缺乏独立性，内部存在着一定的趋势或结构，这可能是由于观察值间不独立( 如传染病的发病) ，或者是来自不同总体；若样本中存在大量的游程，则可能存在系统的短周期波动影响着观察结果，同样不能认为序列是随机的。</p><p>SPSS的Runs过程提供了基于游程个数的检验方法，对于连续性变量，该过程首先要将变量值进行分类，然后进行检验。</p><h3 id="4-2-案例"><a href="#4-2-案例" class="headerlink" title="4.2 案例"></a>4.2 案例</h3><p><img src="https://img-blog.csdnimg.cn/20200801153635956.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200801153807493.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="5-蒙特卡罗方法"><a href="#5-蒙特卡罗方法" class="headerlink" title="5 蒙特卡罗方法"></a>5 蒙特卡罗方法</h2><h3 id="5-1-蒙特卡罗方法原理"><a href="#5-1-蒙特卡罗方法原理" class="headerlink" title="5.1 蒙特卡罗方法原理"></a>5.1 蒙特卡罗方法原理</h3><p>蒙特卡罗方法又称为统计模拟法、随机抽样技术，其名称听起来很神奇，但实际上原理非常简单。就是使用随机数(或更常见的伪随机数)来解决很多计算问题的方法，用下面这个例于就可以解释清楚：假设要计算一个不规则图形的面积，图形的不规则程度和计算方法(比如是否使用积分)的复杂程度是成正比的。要使用蒙特卡罗方法解决这个问题，可以假设有一袋豆于，把豆子均匀地朝这个图形撒，然后数这个图形之中有多少颗豆子，这里要假定豆子都在一个平面上，相互之间没有重叠，那么最终图形内豆子的数目就是图形的面积。当豆子越小，撒的量越多时，结果就越精确。</p><p><img src="https://img-blog.csdnimg.cn/20200801154938204.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200801155138157.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20200801155450792.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center&quot; alt=&quot;在这里插入图片描述&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="统计学" scheme="http://yoursite.com/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6/"/>
    
    
      <category term="假设检验" scheme="http://yoursite.com/tags/%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C/"/>
    
  </entry>
  
  <entry>
    <title>连续变量的统计描述与参数估计</title>
    <link href="http://yoursite.com/2020/07/31/%E8%BF%9E%E7%BB%AD%E5%8F%98%E9%87%8F%E7%9A%84%E7%BB%9F%E8%AE%A1%E6%8F%8F%E8%BF%B0%E4%B8%8E%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/"/>
    <id>http://yoursite.com/2020/07/31/%E8%BF%9E%E7%BB%AD%E5%8F%98%E9%87%8F%E7%9A%84%E7%BB%9F%E8%AE%A1%E6%8F%8F%E8%BF%B0%E4%B8%8E%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/</id>
    <published>2020-07-31T07:26:12.000Z</published>
    <updated>2020-07-31T07:26:47.977Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://img-blog.csdnimg.cn/20200731152400446.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><a id="more"></a><h2 id="1-连续变量的统计描述"><a href="#1-连续变量的统计描述" class="headerlink" title="1 连续变量的统计描述"></a>1 连续变量的统计描述</h2><p><img src="https://img-blog.csdnimg.cn/20200731151125515.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="1-1-集中趋势的描述指标"><a href="#1-1-集中趋势的描述指标" class="headerlink" title="1.1 集中趋势的描述指标"></a>1.1 集中趋势的描述指标</h3><ul><li>算数平均数</li><li>中位数</li><li>截尾均数</li></ul><h3 id="1-2-离散趋势的描述指标"><a href="#1-2-离散趋势的描述指标" class="headerlink" title="1.2 离散趋势的描述指标"></a>1.2 离散趋势的描述指标</h3><ul><li>全距/极差</li><li>方差或标准差</li><li>百分位数、四分位数和四分位间距<br>百分位数Px是一种位置指标，Px将一组观察值分为两部分，理论上有x%的观察值比它小，有（100-x）%的观察值比它大</li></ul><h3 id="1-3-正态分布的描述指标"><a href="#1-3-正态分布的描述指标" class="headerlink" title="1.3 正态分布的描述指标"></a>1.3 正态分布的描述指标</h3><ul><li><p>偏度<br>描述变量取值分布形态的统计量，指分布不对称的方向和程度。样本的偏度系数记为g1，偏度是与正态分布相比较而言的统计量。<br><img src="https://img-blog.csdnimg.cn/20200731151515136.PNG#pic_center" alt="在这里插入图片描述"></p></li><li><p>峰度<br>描述变量取值分布形态陡缓程度的统计量，指分布图形的尖峭程度或峰凸程度。样本的峰度系数记为g2，峰度也是与正态分布相比较而言的统计量。<br><img src="https://img-blog.csdnimg.cn/20200731151525931.png#pic_center" alt="在这里插入图片描述"></p></li></ul><h2 id="2-连续变量的参数估计"><a href="#2-连续变量的参数估计" class="headerlink" title="2 连续变量的参数估计"></a>2 连续变量的参数估计</h2><p><img src="https://img-blog.csdnimg.cn/20200731151705253.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="2-1-正态分布"><a href="#2-1-正态分布" class="headerlink" title="2.1 正态分布"></a>2.1 正态分布</h3><p>标准正态分布（u分布/z分布）</p><h3 id="2-2-参数的点估计"><a href="#2-2-参数的点估计" class="headerlink" title="2.2 参数的点估计"></a>2.2 参数的点估计</h3><p><strong>所选统计量是否适用于作为参数估计量</strong></p><ul><li>无偏性：虽然估计量的值不全等于参数，但应当在真实值附近摆动</li><li>一致性：样本量越大，估计值离真实值的差异应当越小</li><li>有效性：如果有两个统计量都符合上述要求，则应当选取误差更小的一个作为估计值<br>例如均数和中位数，实际上两者在反映正态分布的集中趋势时，在无偏性和一致性上是一样好的，但中位数误差更大，所以应当尽量使用样本均数来反映正态分布的集中趋势</li></ul><p><strong>方法</strong></p><ul><li>矩法<br>在许多情况下，样本统计量本身往往就是相应的总体参数的最佳估计值，此时就可以直接取相应的样本统计量作为总体参数的点估计值。<br>例如样本均数、方差、标准差都是相应总体均数、方差、标准差的矩估计量。</li><li>极大似然法<br>原理：在已知总体分布，但未知其参数值时，在待估计参数的可能取值范围内进行搜索，使似然函数值最大的那个数值即为极大似然估计值<br>优点在于估计量通常能满足一致性、有效性等要求，且具有不变性。<br>不变性：当原始数据进行某种函数变换后，相应估计量的同一函数变换值仍是新样本的极大似然估计量。</li><li>稳健估计值<br>该统计量受数据异常值的影响较小，而且对大部分的分布而言都很好<br>M估计、R估计</li></ul><h3 id="2-3-参数的区间估计"><a href="#2-3-参数的区间估计" class="headerlink" title="2.3 参数的区间估计"></a>2.3 参数的区间估计</h3><p>虽然原始数据可能服从各种各样的分布，但是根据中心极限定理，当样本量n足够大(如n&gt;50)时,其抽样均数都会近似服从正态分布，而此正态分布所对应的标准差就可用来表示抽样误差的大小，此即标准误。</p><p>区间估计的计算</p><h2 id="3-Bootstrap方法"><a href="#3-Bootstrap方法" class="headerlink" title="3 Bootstrap方法"></a>3 Bootstrap方法</h2><p><img src="https://img-blog.csdnimg.cn/20200731152232154.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p><strong>目的</strong></p><ul><li>判断原参数估计值是否准确</li><li>计算出更准确的可信区间，判断得出的统计学结论是否正确</li></ul><p><strong>思想</strong><br>在原始数据的范围内做有放回的抽样，样本含量为n，原始数据中的每个观察单位每次被抽到的概率相等，为1/n，所得样本称为Bootstrap样本。于是可得到任何一个参数θ的一个估计值θ<sup>（b）</sub>。</p><p><strong>方法</strong></p><ul><li>参数法<br>需假定θ<sup>（b）</sub>的分布状况</li><li>非参数法<br>无任何限制</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20200731152400446.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center&quot; alt=&quot;在这里插入图片描述&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="统计学" scheme="http://yoursite.com/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6/"/>
    
    
      <category term="统计描述" scheme="http://yoursite.com/tags/%E7%BB%9F%E8%AE%A1%E6%8F%8F%E8%BF%B0/"/>
    
      <category term="参数估计" scheme="http://yoursite.com/tags/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>《数据挖掘与数据化运营实战》（第13章）</title>
    <link href="http://yoursite.com/2020/07/27/%E3%80%8A%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%8C%96%E8%BF%90%E8%90%A5%E5%AE%9E%E6%88%98%E3%80%8B%EF%BC%88%E7%AC%AC13%E7%AB%A0%EF%BC%89/"/>
    <id>http://yoursite.com/2020/07/27/%E3%80%8A%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%8C%96%E8%BF%90%E8%90%A5%E5%AE%9E%E6%88%98%E3%80%8B%EF%BC%88%E7%AC%AC13%E7%AB%A0%EF%BC%89/</id>
    <published>2020-07-27T11:02:40.000Z</published>
    <updated>2020-07-27T11:02:53.107Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://img-blog.csdnimg.cn/20200727190128697.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><a id="more"></a><h2 id="13-漏斗模型和路径分析"><a href="#13-漏斗模型和路径分析" class="headerlink" title="13 漏斗模型和路径分析"></a>13 漏斗模型和路径分析</h2><p>在互联网数据化运营实践中，有一类数据分析应用是互联网行业所独有的，那就是漏斗模型和路径分析的应用。</p><ul><li><strong>漏斗模型</strong>通常是对用户在网页浏览中一些关键节点的转化程度所进行的描述，比如从浏览到实际购买产品都需要经历三个步骤：浏览商品、将商品加入到购物车、将购物车的东西提交到订单，直到订单完成在线支付，上面的三个步骤走下来，买家人数越来越少，这个过程就是漏斗模型，漏斗模型的主要分析目的是针对网站运营过程中的各个关键环节进行分析，然后针对转换率低的环节进行纠正。</li><li><strong>路径分析</strong>通常是指对用户的每一个网络行为进行精细跟踪和记录，并在此基础上通过分析、挖掘得到用户的详细网络行为路径特点、每一步的转化特点、每一步的来源和去向，从而帮助互联网企业分析用户的网络行为等。</li></ul><p>从严格意义上来说，漏斗模型是路径分析的特殊情况，是针对少数关键节点的路径分析。</p><h3 id="13-1-网络日志和布点"><a href="#13-1-网络日志和布点" class="headerlink" title="13.1 网络日志和布点"></a>13.1 网络日志和布点</h3><p>用户在网上进行浏览时的每一步都会被记录下来，从而形成海量的日志数据。互联网日志的数据体系分为日志布点、日志采集、日志解析和日志分析4个部分。</p><ul><li>日志布点<br>指在页面上安排记录关键用户行为的小程序，用户按照预设规则对网页进行访问的时候，布点的规则程序就会将用户相关的数据发送到一个指定的服务器，从而达到日志采集的目的。包括页面级布点（应用范围最广，该类布点会覆盖网站的所有页面，其内容包括：IP地址、用户名、Cookie相关信息及浏览器类型）、点击级布点（通常会在用户点击某个链接、按钮、筛选框等特定事件时触发）、追踪日志布点（当某一个特定的页面有很多来源是，为了清楚的情分不同的来源，就需要用到追踪日志布点）。</li><li>日志采集<br>进行日志采集，通常会设定专门的日志采集服务器，主要目的是大流量多线程地将日志记录下来。</li><li>日志解析<br>由于日志数据是不同于通常数据源的非结构化数据，其主要目的是提高读写效率，因此日志解析的目的就是讲非结构化数据转化成为结构化数据。</li><li>日志分析<br>日志分析的主要内容包括日常流量监控（PV、UV）、来源去向分析及路径分析。</li></ul><h3 id="13-2-漏斗模型与路径分析的主要区别与联系"><a href="#13-2-漏斗模型与路径分析的主要区别与联系" class="headerlink" title="13.2 漏斗模型与路径分析的主要区别与联系"></a>13.2 漏斗模型与路径分析的主要区别与联系</h3><p>漏斗模型是路径分析的特殊形式，是专门针对关键环节进行的路径分析。</p><p>漏斗模型与路径分析的主要区别：</p><ul><li><strong>侧重点不同</strong>，漏斗模型更多、更主要用于网站和产品的运营监控和管理。</li><li><strong>两者思考的方式和粒度不同</strong>，漏斗模型更多时候要经过抽象的过程来搭建漏斗的每一个环节，漏斗中的每个环节更多时候是抽象出来的，而不一定是完全按照原始的数据直接放进漏斗中的，而路径分析更多的时候是就事论事，不需要经过抽象、转化、整合这些过程。</li><li><strong>分析的思维方向有别</strong>，漏斗模型的思维方式通常是逆向的，即先确定要分析的关键环节，然后抽取相应的数据，计算其转化率。</li><li><strong>分析技术有差别</strong>，漏斗模型的分析技术更直观、更直接、更容易理解，就是根据两个关键环节的先后顺序，计算出从头到尾的转化率即可。</li></ul><h3 id="13-3-漏斗模型的主要应用场景"><a href="#13-3-漏斗模型的主要应用场景" class="headerlink" title="13.3 漏斗模型的主要应用场景"></a>13.3 漏斗模型的主要应用场景</h3><p><img src="https://img-blog.csdnimg.cn/2020072718201650.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><ul><li>运营过程的监控和运营效率的分析和完善<br>只要有流程、有转化，就一定会采用漏斗模型作为其中的一种手段来加以监控、分析和管理。</li><li>用户关键路径分析</li><li>产品优化</li></ul><h3 id="13-4-路径分析的主要应用场景"><a href="#13-4-路径分析的主要应用场景" class="headerlink" title="13.4 路径分析的主要应用场景"></a>13.4 路径分析的主要应用场景</h3><p>漏斗模型可以看做是路径分析的特殊形式，相比而言，路径分析更加全面、更加丰富、更加基础。</p><ul><li>用户典型、频繁的路径模式识别</li><li>用户行为特征的识别</li><li>网站产品设计和优化的依据和参考</li><li>网站运营和产品运营的过程监控关于管理</li></ul><h3 id="13-5-路径分析的主要算法"><a href="#13-5-路径分析的主要算法" class="headerlink" title="13.5 路径分析的主要算法"></a>13.5 路径分析的主要算法</h3><ul><li>社会网络分析方法（Social Network Analysis）<br>社会网络分析，也叫链接分析，在社会网络分析方法中，最常见最成熟的一种方法就是中心性分析方法，中心性是对于社会关系网中参与者的著名成都进行度量的标准，与网络搜索和超链接分析有非常紧密的关系。</li><li>基于序列的关联分析（Sequence Analysis）<br>基于序列的关联分析，又称序列分析，这种分析方法时在关联分析的基础上，进一步考虑了关联品之间的先后顺序，即只分析先后顺序的关联关系。</li><li>最朴素的遍历方法<br>最朴素的遍历方法，因为最直观、最直接、最容易让人理解，把某个页面的所有来源以及相应的流量大小整理出来，同时把浏览该页面的下一个页面的所有去向和相应的流量整理出来。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20200727190128697.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center&quot; alt=&quot;在这里插入图片描述&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="产品/运营" scheme="http://yoursite.com/categories/%E4%BA%A7%E5%93%81-%E8%BF%90%E8%90%A5/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="数据化运营" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%8C%96%E8%BF%90%E8%90%A5/"/>
    
  </entry>
  
  <entry>
    <title>《数据挖掘与数据化运营实战》（第11-12章）</title>
    <link href="http://yoursite.com/2020/07/27/%E3%80%8A%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%8C%96%E8%BF%90%E8%90%A5%E5%AE%9E%E6%88%98%E3%80%8B%EF%BC%88%E7%AC%AC11-12%E7%AB%A0%EF%BC%89/"/>
    <id>http://yoursite.com/2020/07/27/%E3%80%8A%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%8C%96%E8%BF%90%E8%90%A5%E5%AE%9E%E6%88%98%E3%80%8B%EF%BC%88%E7%AC%AC11-12%E7%AB%A0%EF%BC%89/</id>
    <published>2020-07-27T09:22:31.000Z</published>
    <updated>2020-07-27T09:22:43.266Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://img-blog.csdnimg.cn/20200727172053532.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><a id="more"></a><h2 id="11-用户特征分析的典型应用和技术小窍门"><a href="#11-用户特征分析的典型应用和技术小窍门" class="headerlink" title="11 用户特征分析的典型应用和技术小窍门"></a>11 用户特征分析的典型应用和技术小窍门</h2><h3 id="11-1-用户特征分析所适用的典型业务场景"><a href="#11-1-用户特征分析所适用的典型业务场景" class="headerlink" title="11.1 用户特征分析所适用的典型业务场景"></a>11.1 用户特征分析所适用的典型业务场景</h3><p>用户特征分析不仅仅是数据化运营的的基础，即使是传统行业，只要企业足够关注用户，一定定会进行用户特征分析。</p><p><strong>典型业务场景：</strong></p><ul><li>寻找目标客户<br>包括虚拟的目标用户特征分析，这种场景主要适用于企业还没有实际使用的用户，业务方按照业务逻辑假设或者圈定一些典型的特征；真实的目标用户特征分析。</li><li>寻找运营抓手<br>运营抓手指的是通过运营的方式可以用于改善和提升客户满意度的一些特定行为字段，常见的所谓运营抓手包括用户的一些主动行为，之所以强调主动行为，是因为只有主动的行为才是用户自身努力可以达到的，因此只有主动行为才是可以通过运营的方式传达给用户，并且用户可以通过主观努力来改善和提升的；而被动行为是不以用户主观意识为转移的，无法通过运营的手段有效提升和改善。主动行为包括：用户登录网站的天数、用户发布的商品信息条数等；被动行为包括：用户能否卖出产品、用户能否收到足够的买家询盘等。</li><li>用户群体细分的依据</li><li>新品开发的线索和依据</li></ul><h3 id="11-2-用户特征分析的典型分析思路和技术"><a href="#11-2-用户特征分析的典型分析思路和技术" class="headerlink" title="11.2 用户特征分析的典型分析思路和技术"></a>11.2 用户特征分析的典型分析思路和技术</h3><p>单纯从业务方对用户群体的熟悉程度来考虑，可以将用户特征分析拆分成3种分析类型：</p><ul><li><strong>基于预先定义的划分</strong>，该种方法如果是对业务和客户已有深刻的了解，那么可以基于特定的业务需求目的，直接按照特定的分析字段和分析指标进行划分。</li><li><strong>基于数据分析的划分</strong>，主要的分析技术：RFM、聚类技术、决策树的规则整理、预测（响应）模型的核心变量、假设检验方法、Excel透视表的应用等。</li><li><strong>复合划分</strong>，综合采用上述两种方法进行划分。 </li></ul><p><strong>RFM</strong><br>RFM分析方法是指通过影响企业销售和利润的客户行为字段里最重要的三个变量：</p><ul><li>R—客户新鲜度，指客户最近一次购买公司产品的时间；</li><li>F—客户消费频度，至客户特定时间段里购买公司产品的次数；</li><li>M—客户消费金额，指客户在特定时间段里消费公司产品的总金额。</li></ul><p>在该方法中，3个变量的排列顺序是严格的，有轻重缓急和先后次序，客户新鲜度、客户消费频度、客户消费金额。RFM分析方法首先会将3个字段进行分箱处理，即离散化处理，使之成为类别型变量，然后按照低、中、高三个类别进行组合，会有27种组合，最优质的客户是新鲜度高、消费频度最多、消费金额最大的用户。</p><p><strong>聚类技术的应用</strong><br>如果参与聚类的变量数量较少，为了能够更好的支持用户特征分析的应用，非常有必要在聚类的基础上增加更多的与业务目标和商业备用相关的非聚类变量进行综合考虑。</p><p><strong>决策树技术的应用</strong><br>决策树技术最大的应用优势在于其结论非常直观易懂，容易被人理解。</p><p><strong>预测（响应）模型中的核心自变量</strong><br>如果要使用预测模型的思路和方法，那么要注意模型本身的目标变量与用户特征分析中的业务需求保持一致。</p><p><strong>假设检验的应用</strong><br>通过假设检验来筛选有显著性差异的核心变量，是用户特征分析应用中选择特征字段的一个有效方法。</p><h3 id="11-3-特征提炼后的评价体系"><a href="#11-3-特征提炼后的评价体系" class="headerlink" title="11.3 特征提炼后的评价体系"></a>11.3 特征提炼后的评价体系</h3><ul><li>结论是否与当初的分析需求相一致</li><li>结论是否容易被业务方理解</li><li>通过这些主要结论来圈定用户基数是否足够大</li><li>结论是否方便业务方开发出有效的个性化运营方案</li></ul><h2 id="12-运营效果分析的典型应用和技术小窍门"><a href="#12-运营效果分析的典型应用和技术小窍门" class="headerlink" title="12 运营效果分析的典型应用和技术小窍门"></a>12 运营效果分析的典型应用和技术小窍门</h2><p>业务落地应用得到检验，有两类检验：</p><ul><li>模型本身是否稳定，即在新数据中得到的验证结果是否与模型拟合时的表现相一致；</li><li>运营效果分析，好的模型、好的分析报告能否在业务实践中通过业务图但对的工作有效转化成为生产力。</li></ul><h3 id="12-1-为什么要做运营效果分析"><a href="#12-1-为什么要做运营效果分析" class="headerlink" title="12.1 为什么要做运营效果分析"></a>12.1 为什么要做运营效果分析</h3><ul><li>衡量运营工作的效率和效果</li><li>指导运营技巧的优胜劣汰</li><li>提升运营团队的专业能力</li><li>增强运营工作的商业价值</li></ul><h3 id="12-2-统计技术在数据化运营中最重要最常见的应用"><a href="#12-2-统计技术在数据化运营中最重要最常见的应用" class="headerlink" title="12.2 统计技术在数据化运营中最重要最常见的应用"></a>12.2 统计技术在数据化运营中最重要最常见的应用</h3><p>在效果分析类型的业务场景中，统计技术里的假设检验是应用的最集中、最普遍、最频繁的，并且能够有效提供最终的评判结论。</p><h4 id="12-2-1-为什么要进行假设检验？"><a href="#12-2-1-为什么要进行假设检验？" class="headerlink" title="12.2.1 为什么要进行假设检验？"></a>12.2.1 为什么要进行假设检验？</h4><ul><li>为了精确地区分运营效果的差别到底是随机因素引起的，还是因为运营的因素引起的，以及在多大的置信度内可以肯定是因为随机因素引起的，或者是因为运营的因素引起的。</li><li>在很多情况下，效果的评估是基于样本的观测进行的，为了从样本的结论里推论出总体的结论，也必须进行假设检验来判断样本的差异能够代表总体的差异，同时还要确定样本的差异在多大的置信度内可以代表总体的差异。</li></ul><p>假设检验应用最密切、最常用的一些技术和方法：<br>包括T检验、F检验、非参数检验、卡方检验、控制变量的方法和ABtest方法。</p><h4 id="12-2-2-假设检验的基本思想"><a href="#12-2-2-假设检验的基本思想" class="headerlink" title="12.2.2 假设检验的基本思想"></a>12.2.2 假设检验的基本思想</h4><p>在日常生活中，经常会碰到对于总体的一些判断，比如用户群体的活跃度提升是否显著，答案要么是显著、要么是不显著，即是非判断，这两种选择对应的就是两个假设，一个是原假设H<sub>0</sub>，一个是备选假设H<sub>1</sub>。</p><p>在一次观察或者试验中几乎不可能发现的事情，称之为小概率事件，小概率事件再一次试验中发生的概率被称为显著性水平。</p><p>假设检验的基本思想和原理就是小概率事件原理，即观测小概率事件在假设成立的情况下是否会发生。如果在一次试验中，小概率发生了，那么说明假设在一定显著性水平下不可靠，因此有理由拒绝原假设，而接受备选假设；如果在一次试验中，小概率事件没有发生，只能说明没有足够的理由相信假设是错误了，但是并不能说明假设是正确的。</p><ul><li>第I类错误：当原假设为真时，却否定它而犯的错误，称为弃真错误</li><li>第II类错误：当原假设为假时，却肯定它而犯的错误，称为纳伪错误</li></ul><p>上述两类错误在其他条件不变的情况下是相反的，即α增大时，β就减小；α减小时，β就增大。α错误容易受分析人员的控制，因此在假设检验中，通常会先控制第I类错误发生的概率α，具体表现为：在假设检验之前先指定一个α的具体数值，通常取0.05，也可0.1、0.001。</p><h4 id="12-2-3-T检验"><a href="#12-2-3-T检验" class="headerlink" title="12.2.3 T检验"></a>12.2.3 T检验</h4><p>T检验主要用于检验两组样本的均值相等的原假设。在某些场合中，各组观察值是独立的，比如两组测试样本群体，一组是运营组，一组是对照组，运营组的样本是用来进行有针对性的运营活动，而对照组的样本则会刻意避免有针对性的运营活动，前者的独立对比是在两组观察值相互独立的情况下进行的，称为独立组样本的比较，通常采用独立组样本T检验方式，后者的配对比较是在观察值本身进行前后对比，称为配对组样本的比较，通常采用配对组样本T检验方式。</p><p><strong>两组独立样本T检验的假设和检验</strong></p><p>两组独立样本T检验要求数据符合以下数据符合以下三个条件：</p><ul><li>观察值之间是独立的，即观察值相互之间没有牵连关系</li><li>每组观察值来自正态分布的总体，这个要求决定了数据必须是区间型的变量</li><li>两个独立组的方差相等</li></ul><p><strong>两组独立样本的非参数检验</strong></p><p>虽然两组观察值是各自独立的，但是每组观察值不一定来自正态分布的总体，同时两个独立样本组的方差不一定相等，通常采用的方式是Wilcoxon秩和的一种比较两个独立组观察值的非参数检验。</p><p><strong>配对差值的T检验</strong></p><p>使用配对组差值进行T检验的条件：</p><ul><li>每组观察值与其他观察值之间相互独立</li><li>配对差值来自正态分布</li></ul><p><strong>配对差值的非参数检验</strong></p><p>如果每对观察值与其他观察值相互之间是独立的，但是每组观察值不一定来自正态分布，这个时候就不能采用配对差值的T检验了，而应该使用配对差值的Wilcoxon秩和检验。</p><h4 id="12-2-4-方差分析"><a href="#12-2-4-方差分析" class="headerlink" title="12.2.4 方差分析"></a>12.2.4 方差分析</h4><p>当我们的分析不限于两个独立样本组的时候，而是扩展到多个样本组，T检验就不适用了，在这个情况下，就需要进行方差分析（ANOVA）或者F检验。</p><p>方差分析是利用样本数据检验两个以上的总体均值是否有差异来进行分析的一种方法，能够解决多个总体的均值是否相等的检验问题；在研究多个变量对不同总体的影响时，它也是分析各个自变量对因变量影响的方法。</p><p>方差分析满足以下三个前提条件：</p><ul><li>各组观察值是来自于正态分布的总体的随机样本</li><li>各组观察值之间是相互独立的</li><li>各组观察值具有同方差性</li></ul><p>根据分析因素的个数不同，方差分析可以分为单因素方差分析和多因素方差分析。</p><ul><li>多因素方差分析：指当有两个或以上的因素对因变量产生影响时，采用此方法，利用假设检验的过程来判断多个因素是否对目标变量产生明显的影响。</li><li>单因素方差分析：是运营效果分析实践中最常见的，比如，针对从同样的总体中随机抽取多个样本组，只是随后的运营策略有所不同，同时比较运营后的行为指标有所差异的场景。这时，单因素就是运营策略，希望通过假设检验来验证运营策略的不同是否真的导致了随后各样本组的行为指标之间有差异。</li></ul><p><strong>单因素方差分析</strong><br>单因素方差分析（ANOVA）主要研究单个因素对目标变量的影响，这种方式将通过因素的不同水平对目标变量进行分组计算，得到组间和组内方法，并利用方差比较对分组所形成的总体均值进行比较，从而对各总体均值相等的原假设进行检验。</p><h4 id="12-2-5-多个样本组的非参数检验"><a href="#12-2-5-多个样本组的非参数检验" class="headerlink" title="12.2.5 多个样本组的非参数检验"></a>12.2.5 多个样本组的非参数检验</h4><p>如果多个样本组的数据不是来自正态分布的总体，或者各样本组的方差不相等，在这些场景中就不能使用方差分析的方法了，而只能采用非参数检验的方法。</p><h4 id="12-2-6-卡方检验"><a href="#12-2-6-卡方检验" class="headerlink" title="12.2.6 卡方检验"></a>12.2.6 卡方检验</h4><p>卡方检验是一种应用非常广泛的假设检验方法，属于非参数检验的范畴，主要是比较两个和两个以上的样本率，以及对两个分类变量的关联性进行关联分析，其根本思想是比较理论频数和实际频数的吻合程度或者拟合度。</p><h4 id="12-2-7-控制变量的方法"><a href="#12-2-7-控制变量的方法" class="headerlink" title="12.2.7 控制变量的方法"></a>12.2.7 控制变量的方法</h4><p>控制变量就是指在分析某个核心因素针对不同群体的运营效果时，为了防止其他因素的干扰，而人为的将考虑到的其他因素，即一些潜在的、重要的、可能影响运营效果的因素进行固话或者排除，从而在一个人为控制的比较单纯的数据中专门分析核心因素的影响。</p><h4 id="12-2-8-AB-test"><a href="#12-2-8-AB-test" class="headerlink" title="12.2.8 AB test"></a>12.2.8 AB test</h4><p>AB test ，最容易想起的就是他实在网页设计优化中的一种比较策略，同一个功能页面，设计两种不同的页面布局，通过技术手段将两种不同风格的页面设计随机分配给浏览该功能页面的不同访问者，根据随机分配的页面浏览转换效果，来评价不同设计风格的优劣。</p><p>AB Test 最基本的含义就是对于一个运营活动的效果进行评价，在使用此方法时，一定要实现把同一类客户群体随机分成A组和B组，一组进行运营，一组不进行运营，这样才能比较合理的评估运营的效果。</p><p>使用AB Test方法时的注意点：</p><ul><li>参与AB Test的客户群体总是来自同一个总体的，应具有相同的特征或属性</li><li>与AB Test相关的其他业务因素应该一致，也就是说除了要分析的特定运营条件外，其他的业务因素应该一致</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20200727172053532.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center&quot; alt=&quot;在这里插入图片描述&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="产品/运营" scheme="http://yoursite.com/categories/%E4%BA%A7%E5%93%81-%E8%BF%90%E8%90%A5/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="数据化运营" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%8C%96%E8%BF%90%E8%90%A5/"/>
    
  </entry>
  
  <entry>
    <title>《数据挖掘与数据化运营实战》（第10章）</title>
    <link href="http://yoursite.com/2020/07/26/%E3%80%8A%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%8C%96%E8%BF%90%E8%90%A5%E5%AE%9E%E6%88%98%E3%80%8B%EF%BC%88%E7%AC%AC10%E7%AB%A0%EF%BC%89/"/>
    <id>http://yoursite.com/2020/07/26/%E3%80%8A%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%8C%96%E8%BF%90%E8%90%A5%E5%AE%9E%E6%88%98%E3%80%8B%EF%BC%88%E7%AC%AC10%E7%AB%A0%EF%BC%89/</id>
    <published>2020-07-26T09:29:18.000Z</published>
    <updated>2020-07-26T09:29:25.379Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://img-blog.csdnimg.cn/20200726172700318.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><a id="more"></a><h2 id="10-预测响应（分类）模型的典型应用和技术小窍门"><a href="#10-预测响应（分类）模型的典型应用和技术小窍门" class="headerlink" title="10 预测响应（分类）模型的典型应用和技术小窍门"></a>10 预测响应（分类）模型的典型应用和技术小窍门</h2><p>预测响应模型是数据挖掘中最常见的应用模型，最直接的涉及的精细化运营的客户分层以及随后的个性化区别对待。</p><p>预测响应模型涉及的几种算法：神经网络、决策树、逻辑回归、多元线性回归。</p><h3 id="10-1-神经网络"><a href="#10-1-神经网络" class="headerlink" title="10.1 神经网络"></a>10.1 神经网络</h3><p>神经网络是一组互相连接的输入、输出单元，其中每个连接都会与一个权重相关联。在学习阶段，通过调整这些连接的权重就能够预测输入观察值的正确类标号。人工神经网络的结构大致分为两类：前向型网络、反馈型网络。</p><ul><li>前向型网络：是指传播方向是从输入和输出端，并且没有任何的反馈。</li><li>反馈型网络：是传播方向上除了从输入端到输出端之外，还有回环或反馈存在。<br><img src="https://img-blog.csdnimg.cn/20200726162554975.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li></ul><p>神经网络通过输入多个非线性模型，以及不同模型之间的加权互联，最终得到一个输出模型，具体来说，多元输入层是指一些自变量，这些自变量通过加权结合到中间的层次，称为隐蔽层（所谓的黑箱部分），隐蔽层主要包含的是非线性函数，也称转换函数或者挤压函数。</p><p>利用神经网络模型建模的过程中，有5个重大的影响因素：</p><ul><li>层数</li><li>每层中输入变量的数量</li><li>联系的种类</li><li>联系的程度</li><li>转换函数<br><img src="https://img-blog.csdnimg.cn/20200726163533419.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li></ul><p>大部分神经网络模型的学习过程，都是通过不断地改变权重来使误差达到总误差的最小绝对值。比如，以常见的前向型网络模型为例，其设计原理如下：</p><ul><li><strong>层数</strong>。对于一定的输入层和输出层，需要有多少个隐蔽层，从理论上，两层就足够了，在实践中，经常是一层隐蔽层就足够了。</li><li><strong>每层中输入变量的数量</strong>。输出层的变量由具体的分析背景来决定的，而隐蔽层的数量为输入数与输出数的乘积开平方，输入层的数量应该尽量精简。</li><li><strong>联系的程度</strong>。一般都选择所有层次间全部联系。</li><li><strong>转换函数</strong>。选用逻辑斯蒂回归作为主要的转换函数，因为逻辑斯蒂函数可以提供在最短时间内的最佳拟合。</li><li>样本开发样本要足够充分，避免过拟合现象发生。</li></ul><p><strong>神经网络的优势</strong></p><ul><li>有良好的自组织学习能力，神经网络可以根据外界数据的变化来不断修正自身的行为，对未经训练的数据模式的分类能力也比较强。</li><li>有比较优秀的在数据中挑选非线性关系的能力，能够有效发现非线性的内在规律，在复杂的业务逻辑中，数据间非线性关系出现的机会远比线性关系多得多，神经网络的这种能力能够提高应用价值和贡献潜力。</li><li>由于神经网络复杂的结构，在很多场合中应用的效果优于其他的建模方法，对异常值不敏感。</li><li>对噪声数据有比较高的承受能力。</li></ul><p><strong>神经网络的缺点和注意事项</strong></p><ul><li>神经网络需要比较长的模型训练时间。</li><li>对于神经网络模型，少而精的变量能够充分发挥神经网络的模型效率，但是神经网络本身不能挑选变量，因此变量的挑选对神经网络尤其重要。</li><li>模型搭建后直接投入使用，可能会得不到想要的效果，可以多尝试几种神经网络模型，多次验证后，挑选最稳定的模型进行使用。</li><li>神经网络本身对缺失值非常敏感，因此需要对缺失值进行替换、删除、赋值等操作。</li><li>具有过度拟合的倾向，可能导致模型应用于新数据时效率显著下降，因此需要仔细验证，在确定稳定的前提下进行使用。</li></ul><h3 id="10-2-决策树技术"><a href="#10-2-决策树技术" class="headerlink" title="10.2 决策树技术"></a>10.2 决策树技术</h3><p>决策树，其建模过程就是一棵树的成长，从树根、树干、树枝、树叶等。在决策树里，所分析的数据样本形成一个树根，经过层层分枝，最终形成若干个结点，每个结点代表一个结论。从决策树的根部到叶结点的一条路径就形成了对相应对象的类别预测。目前最常用的3种决策树算法分别是：CHAID、CART和ID3，包括后来的C4.5，乃至C5.0。</p><p><strong>决策树的原理和核心要素</strong><br>构造决策树采用的是自定向下的贪婪算法，它会在给个结点选择分类效果最好的属性进行分类，然后继续这个过程，直到这棵树能准确的分类训练样本，或者所有的属性被用完。决策树算法的核心是在对每一个结点进行测试后，选择最佳的属性，并且对决策树进行剪枝处理。</p><p>最常见的节点属性选择方法标准：有信息增益、信息增益率、Gini指数、卡方检验。</p><p>决策树的剪枝处理包括两种形式：先剪枝和后剪枝</p><ul><li><strong>先剪枝</strong>：就是决策树生长之前，就认为定好树的层数，以及每个节点所允许的最少的样本数量，而且在给定的节点不再分裂。</li><li><strong>后剪枝</strong>：让树先充分生长，然后剪去子树，删除节点的分支并用树叶替换，后剪枝的方法更常用，CART算法就包含了后剪枝的方法，他使用的是代价复杂度兼职算法，即将树的代价复杂度看作是树中树叶节点的个数和树的错误率的函数，C4.5使用的是悲观剪枝方法，类似于代价复杂度剪枝算法。</li></ul><p><strong>CHAID算法</strong>又称卡方自动相互关系检测，采用的是局部最优的原则，利用卡方检验来选择对因变量有影响的自变量，首先对所有自变量进行逐一检测，利用卡方检验确定每个自变量和因变量之间的关系，具体来说，就是在检验时，每次从自变量里抽取两个既定值，与因变量进行卡方检验，如果卡方检验显示两者关系不显著，则证明上述两个既定值可以合并，如果合并过程中将会不断减少自变量的取值数量，知道该自变量的所有取值都显现显著性为止，在对每个自变量进行类似处理后，通过比较找出最显著的自变量，并且按自变量最终取值对样本进行分割，形成若干个新的生长节点。</p><p><strong>CART算法</strong>采用的检验标准是基于Gini系数的，不是卡方检验的，CHAID采用的是局部最优的原则，而CART采用的是总体优化，而且CART所生产的二叉树。</p><p><strong>ID 3算法</strong>，迭代的二分器，最大的特点在于自变量的挑选标准是基于信息增益度量的，即选择具有最高信息增益的属性作为节点的分裂属性。</p><p><strong>决策树的应用优势</strong></p><ul><li>决策树非常直观，决策树的搭建和应用的速度比较快，并且可以处理区间型变量和类别型变量，但是要强调的是“可以处理区间型变量”不代表“快速处理区间型变量”，如果输入变量只是类别型变量或者次序型变量，搭建速度很快，但是对于区间型变量，视数据规模，决策树的搭建速度可能会有所不同。</li><li>决策树对于数据的分布没有特别严格的要求。</li><li>对缺失值很宽容，几乎不做任何处理就可以应用。</li><li>不容易受数据中极端值的影响。</li><li>可以同时对付数据中线性和非线性的关系</li></ul><p><strong>决策树的缺点和注意事项</strong></p><ul><li>决策树的最大缺点是其原理的贪心算法。</li><li>如果目标变量是连续性变量，不适用于决策树，最后改用线性回归算法去解决。</li><li>决策树没有像回归或者聚类那样的丰富多样的检测指标和评价方法。</li><li>当某些自变量的类别数量比较多，或者自变量是区间型时，决策树过拟合的危险性会增加。</li><li>决策树算法对区间型自变量进行分箱操作时，无论是否考虑了顺序因素，都有可能因为分箱导致丧失某些重要信息。</li></ul><h3 id="10-3-逻辑回归技术"><a href="#10-3-逻辑回归技术" class="headerlink" title="10.3 逻辑回归技术"></a>10.3 逻辑回归技术</h3><p>回归分析主要是包括逻辑回归技术和多元线性回归技术。</p><p>逻辑回归的原理：当目标变量是二元变量的时候，逻辑回归分析是一个非常成熟的主流模型算法。对于二元的目标变量来说，逻辑回归的目的就是要预测一组自变量数值相对应的因变量是“是”的概率，概率是介于【0,1】之间的，需要用到专门的Sigmoid函数。</p><p><strong>可能性比率（ODDS）</strong>是指一件事情发生的概率除以这件事情不发生的概率后得到的值。</p><ul><li>可能性比率为5，说明一件事件的可能性比不发生的可能性高5倍</li><li>可能性比率小于1， 说明一件事情的发生的概率低于50%</li><li>可能性比率大于1，说明一件事情的发生的概率高于50%，但最大值可以是无穷大</li></ul><p>逻辑回归使用的参数估计方法通常是最大似然法，利用最大似然法进行参数估计<br>对数似然函数。</p><p><strong>回归中变量中的筛选方法</strong></p><ul><li><strong>向前引入法</strong>。采用回归模型逐个引入自变量，刚开始模型中没有自变量，然后引入第一个自变量进入回归方程，并进行F检验和T检验，计算残差平方和，如果通过检验，则保留该变量，接着引入第二个变量，进行计算，从理论上来说，增加了一个变量，回归平方和应该增加，残差平方和应该减小，引入一个变量前后的残差平方和之差就是新引进的该自变量的偏回归平方和，如果改值明显偏大，则说明新引进的自变量对目标变量有显著影响繁殖则没有显著影响，向前迎入法最大的缺点是最先引入回归方程的变量在随后不会被剔除出去，会对后面的引入的变量的评估造成干扰。</li><li><strong>向后剔除法</strong>。向后剔除法正好与向前引入法相反，即首先把所有的变量一次性放进回归模型中进行F检验和T检验，然后逐个删除不显著的变量，删除的原则是根据偏回归平方和的大小决定的，如果偏回归平方和很大则保留，反之则删除，最大的缺点就是可能会引入一些不重要的变量，并且变量一旦被删除之后，就没有机会重新进入回归模型中。</li><li><strong>逐步回归法</strong>。该方法综合了上述两种方法的特点，自变量仍然是逐个进入回归模型，在引入变量时，需要利用偏回归平方和进行检验，只有显著时才可以加入，当新的变量加入模型后，又要重新对原来的老变量进行偏回归平方和的检验，一旦某变量变得不显著时就要立即删除该变量，如此循环往复，直到留下来的老变量均不可删除，并且新的变量无法加入为止。</li></ul><p><strong>逻辑回归的应用优势</strong><br>逻辑回归技术是最成熟、应用最广泛的。</p><p><strong>逻辑回归应用中的注意事项</strong></p><ul><li>建模数据量不能太少，目标变量中每个类别所对应的样本数量要足够充分才能支持建模。</li><li>要注意排除自变量中的共线性。</li><li>异常值会对模型造成干扰。</li><li>逻辑回归模型不能处理缺失值，所以要对缺失值进行适当的处理，或赋值、或替换、或删除。</li></ul><h3 id="10-4-多元线性回归技术"><a href="#10-4-多元线性回归技术" class="headerlink" title="10.4 多元线性回归技术"></a>10.4 多元线性回归技术</h3><p>线性回归是逻辑回归的基础，同时，线性回归也是数据挖掘中常用的处理预测问题的有效方法。线性回归与逻辑回归最大的不同在于目标变量的类型，线性回归所针对的目标变量是区间型的，逻辑回归针对的是类别性的变量。</p><p><strong>线性回归模型与逻辑回归模型的区别</strong></p><ul><li>线性回归模型的目标变量与自变量之间的关系假设是线性关系，而逻辑回归模型中的目标变量与自变量之间的关系是非线性的。</li><li>在线性分布通常假设，对应于自变量X的某个值，目标变量Y的观察值是服从正态分布的，而在逻辑回归中，目标变量Y是服从二项分布（0,1分布）或者多项分布的。</li><li>在逻辑回归中，不存在线性回归里常见的残差。</li><li>在参数的估值上，线性回归通常采用的是最小平方法，而逻辑回归通常采用的是最大似然法。</li></ul><p><strong>线性回归的优势</strong></p><ul><li>通俗易懂，多元线性回归非常容易被解读</li><li>速度快，效率高</li><li>可以作为查找异常值的有效工具</li></ul><p><strong>线性回归应用中的注意事项</strong></p><ul><li>算法对噪声和异常值比较敏感</li><li>该算法只适合处理线性关系，或者可以对自变量进行一定的转换</li><li>多元线性回归的应用还有一些前提假设：自变量是确定的变量，而不是随机变量，并且自变量之间是没有线性相关的，随机误差项均有均值为0和等方差性，随机误差呈正态分布</li></ul><h3 id="10-5-模型过拟合及对策"><a href="#10-5-模型过拟合及对策" class="headerlink" title="10.5 模型过拟合及对策"></a>10.5 模型过拟合及对策</h3><p>模型的过拟合是指模型在训练集离得表现非常令人满意，但是一旦应用到实际的业务场景中，效果会大打折扣。</p><p><strong>过拟合产生的原因</strong></p><ul><li>建模样本抽取错误，包括但不限于样本数量太少，抽样方法错误，抽样时没有足够这鞥缺的考虑业务场景和特点</li><li>样本里的噪声数据干扰过大</li><li>决策树模型的搭建过程中，没有合理的限制和修建</li><li>建模时的逻辑假设到了应用模型时已经不能成立的</li><li>使用了太多的输入变量</li></ul><p><strong>从技术层次上解决过拟合</strong></p><ul><li>最基本的技术手段就是合理、有效的抽样，包括分层抽样、过抽样等从而用不同的样本去检验模型</li><li>事先准备几个不同时间窗口、不同范围的测试数据集和验证数据集，在不同的数据集分别对模型进行交叉检验</li><li>建模时目标观测值的数量太少</li><li>如果数据太少，请谨慎使用神经网络模型，只有拥有足够多的数据，神经网络模型才能够有效的防止过拟合现象的发生</li></ul><h3 id="10-7-预测响应模型的应用步骤"><a href="#10-7-预测响应模型的应用步骤" class="headerlink" title="10.7 预测响应模型的应用步骤"></a>10.7 预测响应模型的应用步骤</h3><ol><li>基本的数据摸底</li><li>建模数据的抽取和清洗</li><li>初步的相关性检验和共线性排查</li><li>潜在自变量的分布转换</li><li>自变量的筛选</li><li>响应模型的搭建和优化</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20200726172700318.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center&quot; alt=&quot;在这里插入图片描述&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="产品/运营" scheme="http://yoursite.com/categories/%E4%BA%A7%E5%93%81-%E8%BF%90%E8%90%A5/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="数据化运营" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%8C%96%E8%BF%90%E8%90%A5/"/>
    
  </entry>
  
  <entry>
    <title>《数据挖掘与数据化运营实战》（第9章）</title>
    <link href="http://yoursite.com/2020/07/26/%E3%80%8A%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%8C%96%E8%BF%90%E8%90%A5%E5%AE%9E%E6%88%98%E3%80%8B%EF%BC%88%E7%AC%AC9%E7%AB%A0%EF%BC%89/"/>
    <id>http://yoursite.com/2020/07/26/%E3%80%8A%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%8C%96%E8%BF%90%E8%90%A5%E5%AE%9E%E6%88%98%E3%80%8B%EF%BC%88%E7%AC%AC9%E7%AB%A0%EF%BC%89/</id>
    <published>2020-07-26T07:42:12.000Z</published>
    <updated>2020-07-26T07:42:25.799Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://img-blog.csdnimg.cn/2020072615393543.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><a id="more"></a><h2 id="9-聚类分析的典型应用和技术小窍门"><a href="#9-聚类分析的典型应用和技术小窍门" class="headerlink" title="9 聚类分析的典型应用和技术小窍门"></a>9 聚类分析的典型应用和技术小窍门</h2><h3 id="9-1-聚类分析的典型应用场景"><a href="#9-1-聚类分析的典型应用场景" class="headerlink" title="9.1 聚类分析的典型应用场景"></a>9.1 聚类分析的典型应用场景</h3><ul><li>目标用户群体分类</li><li>不同产品的价值组合</li><li>探测、发现孤立点、异常值。孤立点就是指相对于整体数据对象而言的少数数据对象，这些对象的行为特征与整体的数据行为特征很不一致，虽然在一般的数据处理过程中，会把孤立点作为噪声数据清理出去，但是在很多业务数据领域，孤立点的价值也很重要</li></ul><h3 id="9-2-主要聚类算法的分类"><a href="#9-2-主要聚类算法的分类" class="headerlink" title="9.2 主要聚类算法的分类"></a>9.2 主要聚类算法的分类</h3><h4 id="9-2-1-划分方法"><a href="#9-2-1-划分方法" class="headerlink" title="9.2.1 划分方法"></a>9.2.1 划分方法</h4><p>给定具有n个对象的数据集，采用划分方法对数据集进行k个划分，每个划分代表一个簇，k&lt;=n，并且每个划分至少包含一个对象，划分方法一般要做一个初始划分，然后采用迭代重新定位技术，通过让对象在不同组间的移动来改进划分的准确度和精度，一个好的划分原则是，同一个簇中对象之间的相似性很高，不同簇之间对象的相异性很高。</p><p>目前主流的划分方法如下：</p><ul><li><strong>K-Means算法</strong>，又叫K均值算法，在给定一个数据集合需要划分的数目k后，该算法可以根据数据划分到k个簇中，直到收敛为止，K-Means算法用的是簇中对象的平均值来划分，大致步骤就是，首先从随机抽取的k个数据点作为初始的聚类中心（种子中心），然后计算每个数据点到每个种子中心的距离，并把每个数据点分配到距离它最近的种子中心，一旦所有的数据点都被分配完成，每个聚类的聚类中心（种子中心）按照本聚类的现有数据点进行重新计算，不断重复，直到收敛，既满足某个终止条件，最常见的终止条件就是误差平方和（SSE）局部最小。</li><li><strong>K-Medoids算法</strong>，又叫K中心点算法，该算法用最接近簇中心的一个对象表示划分的每个簇，划分过程相似，与K-Means算法最大的不同在于，K-Medoids算法是用簇中最接近中心点的一个真实数据对象来代表簇，而K-Means算法是用计算出来的簇中对象的平均值来代表该簇的。</li></ul><h4 id="9-2-2-层次方法"><a href="#9-2-2-层次方法" class="headerlink" title="9.2.2 层次方法"></a>9.2.2 层次方法</h4><p>在给定n个对象的数据集后，可用层次方法对数据集进行层次分解，直到满足某种收敛条件为止。</p><p>层次方法有可分为：</p><ul><li><strong>凝聚层次聚类</strong>：又叫自底向上方法，一开始把每个对象作为单独的一类，然后相继合并与之相似的对象或者类，直到所有小的类别合并成一个类，达到收敛。</li><li><strong>分裂层次聚类</strong>：又叫自顶向下方法，一开始将所有的对象置于一个簇中，在迭代的每一步中，类会被分裂成更小的一类，直到最终每个对象在单独的一个类中，收敛达到终止条件。</li></ul><p>层次方法最大的缺陷是，合并或分裂点的选择比较困难，对于局部来说，好的合并或分裂点的选择往往并不能保证会得到高质量的全局的聚类结果，而且一旦一个步骤完成，它就不能撤销。</p><h4 id="9-2-3-基于密度的方法"><a href="#9-2-3-基于密度的方法" class="headerlink" title="9.2.3 基于密度的方法"></a>9.2.3 基于密度的方法</h4><p>传统的聚类方法都是基于对象之间的距离，但是这些基于距离的方法只能发现球状类型的数据，对于非球状类型的数据，只根据距离是不够的。</p><p>基于密度的方法原理是：只要邻近区域里的密度超过了某个阈值，就继续聚类，换言之，给定某个簇中的每个数据点，在一定范围内必须包含一定数量的其他对象。该算法从数据对象的分布密度触发，把密度足够大的区域连接在一起，因此可以发现任意形状的累，同时该算法还能够过滤噪声数据，典型算法是DBSCAN以及扩展算法OPTICS，但是最大的缺点是，该算法需要用户确定输入参数，而且对参数十分敏感。</p><h4 id="9-2-4-基于网格的方法"><a href="#9-2-4-基于网格的方法" class="headerlink" title="9.2.4 基于网格的方法"></a>9.2.4 基于网格的方法</h4><p>将对象空间量化为有限数目的单元，而这些单元则形成了网格结构，所有的聚类操作都是在这个网格结构中进行的，该算法的优点是处理速度快，其处理时间常常独立于数据对象的数目，只跟量化空间每一维的单元数目有关。</p><p>典型算法是STING，该算法是一种基于网格的多分辨率局累计数，将空间区域划分为不同分辨率级别的矩形单元，并形成一个层次结构，且高层的低分辨率单元会被划分为多个低一层次的较高分辨率单元，这种算法从最底层的网格开始逐渐向上计算网格内数据的统计信息并储存，网格建立完成后，则用类似于DBSCAN的方法对网格进行聚类。</p><h3 id="9-3-聚类分析在实践应用中的重点注意事项"><a href="#9-3-聚类分析在实践应用中的重点注意事项" class="headerlink" title="9.3 聚类分析在实践应用中的重点注意事项"></a>9.3 聚类分析在实践应用中的重点注意事项</h3><h4 id="9-3-1-处理数据噪声和异常值"><a href="#9-3-1-处理数据噪声和异常值" class="headerlink" title="9.3.1 处理数据噪声和异常值"></a>9.3.1 处理数据噪声和异常值</h4><p>数据化运营中聚类算法主要是K-Means算法，但其对噪声和异常值非常敏感（K-Means算法用的是平均值来聚类）</p><p>常见的处理方法：</p><ul><li>直接删除那些比其他任何数据点都要远离聚类中心点的异常值，为了防止误删除，需要在多次的聚类循环中监控异常值，根据业务逻辑与多次的循环结果进行比较，然后决定删除。</li><li>随机抽样的方法也能够较好的规避数据噪声的影响，因为随机抽样，所以，被抽中的几率很小，不仅可以避免数据噪声的误导和干扰，而且聚类后的结果作为聚类模型可以应用到剩余的数据集中————直接用该聚类模型对剩余的数据集进行判断、利用监督学习的分类器原理，每个聚类作为一个类别，用于判断剩余的那些数据点最适合放进哪个类别或者聚类群体中。</li></ul><h4 id="9-3-2-数据标准化"><a href="#9-3-2-数据标准化" class="headerlink" title="9.3.2 数据标准化"></a>9.3.2 数据标准化</h4><p>数据标准化是聚类分析中最重要的一个数据预处理步骤，它即可以为聚类计算中的各个属性赋予相同的权重，还可以有效化解不同属性因度量单位不统一所带来的潜在的数量等级的差异。</p><p>数据标准化有很多不同方式，标准差标准化最常用（Z-Score标准化），处理后数据符合标准正态分布，即均值为0，标准差为1，转化公式如下：<br><img src="https://img-blog.csdnimg.cn/20200726142033616.PNG#pic_center" alt="在这里插入图片描述"></p><h4 id="9-3-3-聚类变量少而精"><a href="#9-3-3-聚类变量少而精" class="headerlink" title="9.3.3 聚类变量少而精"></a>9.3.3 聚类变量少而精</h4><ul><li>紧紧围绕具体分析目的和业务需求挑选聚类变量</li><li>通过相关性检测，防止相关性高的变量同时进入聚类计算（如：登录次数、在线时长、PV浏览量等变量之间都是明显相关的，选取一个即可）</li><li>衍生变量</li><li>主成分分析</li></ul><h3 id="9-4-聚类分析的扩展应用"><a href="#9-4-聚类分析的扩展应用" class="headerlink" title="9.4 聚类分析的扩展应用"></a>9.4 聚类分析的扩展应用</h3><h4 id="9-4-1-聚类的核心指标与非聚类的业务指标相辅相成"><a href="#9-4-1-聚类的核心指标与非聚类的业务指标相辅相成" class="headerlink" title="9.4.1 聚类的核心指标与非聚类的业务指标相辅相成"></a>9.4.1 聚类的核心指标与非聚类的业务指标相辅相成</h4><p>一方面坚持参与聚类的变量少而精的原则，另一方面把非聚类的业务指标与聚类结果一起拿来分析、提炼、挖掘，这种相辅相成的做法在聚类分析的应用实践中已经得到了普遍的认可和采用。</p><p>具体来说，先通过用户行为属性里的核心字段进行聚类分群，在得到比较满意的聚类分析结果后，针对每个具体细分的对象群体，再分别考察用户的会员属性。</p><h4 id="9-4-2-数据的探索和清理工具"><a href="#9-4-2-数据的探索和清理工具" class="headerlink" title="9.4.2 数据的探索和清理工具"></a>9.4.2 数据的探索和清理工具</h4><ul><li>聚类技术的聚类类别可以作为一个新的字段加入到其他的模型搭建中，可能会提高建模的效率和增强效果。</li><li>在合适的场景中，可以作为细分群体的建模依据，细分建模的精度比整体建模的精度要高一些。</li><li>聚类技术的应用本身就是数据探索和熟悉的过程。</li><li>聚类技术对变量的聚类是精简变量的有效方法。</li><li>聚类技术可以用来检查数据的共线性问题。</li></ul><h4 id="9-4-3-个性推荐的应用"><a href="#9-4-3-个性推荐的应用" class="headerlink" title="9.4.3 个性推荐的应用"></a>9.4.3 个性推荐的应用</h4><h3 id="9-5-聚类分析的优势和缺点"><a href="#9-5-聚类分析的优势和缺点" class="headerlink" title="9.5 聚类分析的优势和缺点"></a>9.5 聚类分析的优势和缺点</h3><p><strong>优点</strong></p><ul><li>聚类技术已经比较成熟，算法比较可靠，是一个不错的数据群体细分的工具和方法</li><li>不仅是一种模型技术，而且也可以作为数据摸底和数据清洗的工具</li><li>如果聚类技术使用的好，聚类的结果比较容易用商业和业务的逻辑来解释</li><li>算法简洁、高效，时间复杂度O(tkn)，t是循环的次数，k是聚类的个数，n是数据点的个数，由于n远远大于k和t，因此K-Means的算法时间复杂度与数据记得大小是线性相关</li><li>K-Means算法是一个不依赖顺序的算法</li></ul><p><strong>缺点</strong></p><ul><li>需要实现指定K，在实践中，需要测试多个不同的k才能最终确定最合适的K</li><li>对数据噪声和异常值比较敏感</li></ul><h3 id="9-6-聚类分析结果的评价体系和评价指标"><a href="#9-6-聚类分析结果的评价体系和评价指标" class="headerlink" title="9.6 聚类分析结果的评价体系和评价指标"></a>9.6 聚类分析结果的评价体系和评价指标</h3><p>A：业务专家的评估</p><p>B：聚类技术上的评价指标</p><ul><li>RMSSTD：群体中所有变量的综合标准差，RMSSTD越小，群体内（簇内）个体的相似程度越高，聚类效果越好</li><li>R-Square：聚类后群体间差异的大小，R-Square越大，群体间相异性越高，聚类效果越好</li><li>SPR：适合于层次方法中的凝聚层次聚类算法，表示当原来两个群体合并成新群体后，所损失的群内相似性的比例，SPR越小，表明合并成新的群体时，损失的群内相似性比例越小，新群体内的相似性越高，聚类效果越好</li><li>Dinstance Between Clusters：适合凝聚层次聚类方法。表示再要合并两个细分群体的时候，分别计算两个群体的中心，以求得两个群体的距离，一般来说，距离越小说明两个群体越适合合并成一个新群体</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/2020072615393543.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center&quot; alt=&quot;在这里插入图片描述&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="产品/运营" scheme="http://yoursite.com/categories/%E4%BA%A7%E5%93%81-%E8%BF%90%E8%90%A5/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="数据化运营" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%8C%96%E8%BF%90%E8%90%A5/"/>
    
  </entry>
  
  <entry>
    <title>《数据挖掘与数据化运营实战》（第8章）</title>
    <link href="http://yoursite.com/2020/07/25/%E3%80%8A%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%8C%96%E8%BF%90%E8%90%A5%E5%AE%9E%E6%88%98%E3%80%8B%EF%BC%88%E7%AC%AC8%E7%AB%A0%EF%BC%89/"/>
    <id>http://yoursite.com/2020/07/25/%E3%80%8A%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%8C%96%E8%BF%90%E8%90%A5%E5%AE%9E%E6%88%98%E3%80%8B%EF%BC%88%E7%AC%AC8%E7%AB%A0%EF%BC%89/</id>
    <published>2020-07-25T11:00:08.000Z</published>
    <updated>2020-07-25T11:00:12.848Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://img-blog.csdnimg.cn/2020072518565969.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><a id="more"></a><h2 id="8-常见的数据处理技巧"><a href="#8-常见的数据处理技巧" class="headerlink" title="8 常见的数据处理技巧"></a>8 常见的数据处理技巧</h2><h3 id="8-1-数据的抽取要正确反映业务需求"><a href="#8-1-数据的抽取要正确反映业务需求" class="headerlink" title="8.1 数据的抽取要正确反映业务需求"></a>8.1 数据的抽取要正确反映业务需求</h3><p>如何尽量保证数据的抽取正确反映业务需求？</p><ul><li>真正熟悉业务背景</li><li>确保抽取的数据所对应的当时业务背景与现在的业务需求即将应用的业务背景没有明显的重大改变</li></ul><h3 id="8-2-数据抽样"><a href="#8-2-数据抽样" class="headerlink" title="8.2 数据抽样"></a>8.2 数据抽样</h3><p>“抽样”对于数据分析和挖掘来说是一种常见的前期数据处理技术和手段</p><ul><li>主要原因是如果<strong>数据全集的规模太大</strong>，针对数据全集进行分析计算不但会消耗更多的运算资源，还会显著增加运算分析的时间，甚至太大的数据量在数据分析挖掘软件运行时崩溃，采用采样措施可以有效显著的降低这些负面影响；</li><li>另一个常见使用场景就是，在有很多<strong>小概率事件、稀有事件</strong>的预测建模过程中，如果按照原始的数据全集，原始的稀有占比进行分析挖掘，0.2%的稀有事件很难通过数据挖掘得到有意义的预测和结论 ，因此需要通过采样措施，认为增加样本中的“稀有事件”的浓度和在样本中的占比。</li></ul><p>在抽样的操作中，需注意：</p><ul><li>样本中<strong>输入变量的值域</strong>要与数据全集中输入变量的值域一致</li><li>样本中<strong>输入变量的分布</strong>与数据全集中输入变量的分布保持一致，或高度相似</li><li>样本中<strong>因变量的值域或者种类分布</strong>也要与数据全集中的目标变量值域或者种类的分布保持一致</li><li><strong>缺失值的分布</strong>，样本中缺失值的分布要与数据全集中缺失集的分布保持一致或者高度相似</li><li>针对<strong>稀有事件</strong>建模师要采用抽样措施，由于抽样所造成的目标事件在样本中的浓度被认为放大了，样本中的事件与非事件的比例与数据全集中两者的比例不一致，因此，需要记得使用<strong>加权的方法</strong>恢复新样本对全体数据集的代表性，当然现在主流的数据挖掘软件，对这种加权恢复已经做了自动处理</li></ul><h3 id="8-3-分析数据的规模有哪些具体的要求"><a href="#8-3-分析数据的规模有哪些具体的要求" class="headerlink" title="8.3 分析数据的规模有哪些具体的要求"></a>8.3 分析数据的规模有哪些具体的要求</h3><p>分析数据的规模，主要指用于数据分析挖掘建模时最起码的数据规模大小。重点是考量目标变量所对应的目标事件的数量。</p><p>一般情况下，数据挖掘建模过程中会将样本划分为三个子样本集：训练集、验证集、测试集，或者划分成两个子样本集：训练集和验证集，训练记得数量大概占总样本数量级的40%-70%，在理想的情况下，训练记得目标事件的数量应该有1000个，因为太少的目标事件样本基础上开发的模型缺乏稳定性，如果少于1000个，根据业务分析判断可行的话，也可以进行数据挖掘，只是需要更加关注模型的稳定性。</p><p>预测模型的自变量应该控制在8-20个之间，因为太少的自变量会对模型的稳定性造成威胁，任何一个自变量的缺失都会引起模型结果的显著变动，太多的自变量会因为模型变得复杂而不稳定。</p><p>训练集目标事件最好要在1000个以上，在此基础上，训练集的样本的规模应该在自变量数量的10倍以上，并且被预测的目标事件至少是自变量数目的6-8倍。</p><h3 id="8-4-如何处理缺失值和异常值"><a href="#8-4-如何处理缺失值和异常值" class="headerlink" title="8.4 如何处理缺失值和异常值"></a>8.4 如何处理缺失值和异常值</h3><p>数据缺失主要是因为数据存储错误，或者原始数据的缺少。在大多数情况下，需要对缺失数据进行处理，在个别情况下，比如应用决策树算法，该算法本身允许数据缺失值直接进入分析挖掘，因为在这种情况下缺失值本身已经被看做是一个特定的属性类别。</p><h4 id="8-4-1-缺失值的常见处理方法"><a href="#8-4-1-缺失值的常见处理方法" class="headerlink" title="8.4.1 缺失值的常见处理方法"></a>8.4.1 缺失值的常见处理方法</h4><ol><li>首先需要分析找到数据缺失的原因，然后进行判断是否需要对缺失值进行处理</li><li>如果要进行处理，可以选择直接删除带有缺失值的数据元组。这样做的好处在于留下来的数据都是有完整记录的，数据很干净，删除的步骤很简单，但是，如果缺失的比例很大的话，留下来的数据量就很小，不足以进行有效的数据挖掘，这种方法仅适用于缺失值比例很小，并且后期打分应用中数据的缺失值比例也很少的情况</li><li><strong>直接删除</strong>有大量缺失值的变量。针对于那些确实只占比超过相当比例变量，比如缺失值超过20%或者更多</li><li>对缺失值进行<strong>替换</strong>。可以利用全集中的代表性属性，诸如众数或者均值等，或者人为定义一个数据去代替缺失值的情况，对于类别型变量，用众数或者一个崭新的类别进行代替，对于次序型变量和区间型变量，用中间值、众数、最大值、最小值、用户定义的任意其他值、平均值或仅针对区间型变量来代替缺失值</li><li>对缺失值进行<strong>赋值</strong>。通过诸如回归模型、决策树模型、贝叶斯定理等去预测最近替代值，就是把缺失数据所对应的变量作为目标变量，把其他的输入变量作为自变量，为每个需要进行缺失值复制的字段分别建立预测模型，从理论上看，这种方法最严谨，但是成本较高，包括时间成本和分析资源的投入成本</li></ol><h4 id="8-4-2-异常值的判断和处理"><a href="#8-4-2-异常值的判断和处理" class="headerlink" title="8.4.2 异常值的判断和处理"></a>8.4.2 异常值的判断和处理</h4><p>数据成本中的异常值（outlier）指的是一个类别型变量里某个类别值出现的次数太少、太稀有，或者一个区间型变量里某些取值太大。</p><p>异常值的判断：</p><ul><li>对于类别型的变量，某个类别值出现的频率太小，太稀有，可能是异常值</li><li>对于区间型变量，最简单有效的方法就是把所有的观察对象按照变量的取值从小到大进行排列，然后从最大的数值进行倒推0.1%甚至更多，这些最大的数值可能就是异常值，另外一个常用的判断方法就是标准差，根据不同的业务背景和变量的业务含义，把超过均值n个标准差以上的取值定义为异常值</li></ul><p>对于异常值，在多数情况下，异常值的删除可以有效的降低数据的波动，使模型更加稳定，但是在某些业务场景下，异常值的应用缺失另一个专门的业务方向。</p><h3 id="8-5-数据转换"><a href="#8-5-数据转换" class="headerlink" title="8.5 数据转换"></a>8.5 数据转换</h3><p>由于原始数据，主要是指区间型变量的分布不平滑（有噪声）、不对称分布，使得数据转换成为一种非常重要的技术手段，根据转化逻辑和转换目的，主要分为四类：</p><p>A.生成衍生变量<br>通过对原始数据进行简单、适当的数学公式推导，产生更加具有商业意义的新变量。如：用户月均、年均消费金额，特定商品类目消费金额占其全部消费金额的比例，家庭人均年收入等。</p><p>B.改善变量分布的转换<br>通过各种数学转换，使得自变量的分布呈现（或近似）正态分布，并形成倒钟型曲线，常见的改善分布的转换措施有:取对数、开平方根、取倒数、开平方、取指数。</p><p>C.区间型变量的分箱转换<br>分箱转换就是区间型变量就是把区间型变量转换成次序型变量，主要目的是降低变量的复杂性、简化数据，提升自变量的预测能力，如果分箱恰当，是可以有效提升自变量和因变量的相关性的，可以显著提升模型的预测效率和效果，尤其是当自变量和因变量之间有比较明显的非线性关系，分享操作跟是不错的手段，另外，当自变量的偏度很大时，也是一个值得积极尝试的方法。</p><p>D.针对区间型变量进行标准化操作<br>数据标准化转化的主要目的是将数据按照比例进行缩放，使之落入一个小的区间范围之内，使得不同的变量经过标准化后可以有平等的分析和比较的基础。</p><p>最简单的数据标准化转换是Min-Max标准化，也叫离差标准化，是对原始数据进行线性转换，使得结果在【0,1】区间<br>x*=（x-min）/（max-min）</p><h3 id="8-6-筛选有效的输入变量"><a href="#8-6-筛选有效的输入变量" class="headerlink" title="8.6 筛选有效的输入变量"></a>8.6 筛选有效的输入变量</h3><h4 id="8-6-1-为什么要筛选有效的输入变量？"><a href="#8-6-1-为什么要筛选有效的输入变量？" class="headerlink" title="8.6.1 为什么要筛选有效的输入变量？"></a>8.6.1 为什么要筛选有效的输入变量？</h4><p>筛选有效的输入变量既可以提高模型稳定性，也是提高模型预测能力的需要，过多的输入变量会产生共线性问题。</p><p>在筛选变量之前，可以直接删除明显的无明显的变量，比如：常数变量或者只有一个值的变量、缺失值比例很高的变量（例如缺失值高达95%），取值太泛的类别型变量。</p><h4 id="8-6-2-筛选的思路"><a href="#8-6-2-筛选的思路" class="headerlink" title="8.6.2 筛选的思路"></a>8.6.2 筛选的思路</h4><p><strong>A:结合业务经验进行先行筛选</strong></p><p><strong>B: 用线性相关性指标进行初步筛选</strong><br>最简单、最常用的方法就是通过自变量之间的线性相关性指标进行初步筛选。其中，皮尔逊相关系数（Pearson Correlation）最为常用，其主要用于比例型变量与比例型变量、区间型变量与区间型变量，以及二元变量与区间型变量之间的线性关系描述。其公式如下：<br><img src="https://img-blog.csdnimg.cn/20200725180918670.PNG#pic_center" alt="r=（x与y的协方差）/（x的标准差与y的标准差的乘积）"></p><ul><li>|r|&lt;0.3 表示低度线性相关</li><li>0.3&lt;=|r|&lt;0.5 表示中低度线性相关</li><li>0.5&lt;=|r|&lt;0.8 表示中度线性相关</li><li>0.8&lt;=|r|&lt;1.0 表示高度线性相关</li></ul><p>如果自变量属于中度以上线性相关（&gt;0.6以上），多个变量只需要保留一个就可以了。</p><p>来自样本的统计结果，需要通过显著性检验才能知道其是否适用于针对总体数据的相关性。</p><p>显著性检验：针对我们队总体所做的假设做检验，其原理就是“小概率事件实际不可能性原理”来接受或者否定。</p><p>尽管有时候上述公式计算出来的相关系数r等于0，但是也只能说明线性关系不存在，不能排除变量之间存在其他形式的相关关系，比如曲线关系。</p><p>某个自变量和因变量的线性相关性很小，但是可以通过跟其他自变量结合在一起让其成为预测力很强的自变量。</p><h4 id="8-6-3-R平方"><a href="#8-6-3-R平方" class="headerlink" title="8.6.3 R平方"></a>8.6.3 R平方</h4><p>R平方（R-Square）该方法将借鉴多元线性回归的分析算法来判断和选择对目标变量有重要预测意义及价值的自变量。通俗的说，就是R平方表示模型输入的各自变量在多大程度上可以解释目标变量的可变性，取值范围在【0,1】之间，R<sup>2</sup>越大，说明模型的拟合越好。<br><img src="https://img-blog.csdnimg.cn/20200725182156866.PNG#pic_center" alt="在这里插入图片描述"></p><ul><li>SSE称为残差平方和，由其他因素引起的</li><li>SSR称为回归平方和，由自变量引起的</li><li>SST称为总平方和，反映的是因变量Y的波动程度</li></ul><p>在回归方程中，回归平方和越大，回归效果越好，<br>统计量<img src="https://img-blog.csdnimg.cn/20200725182712930.PNG#pic_center" alt="F=（SSR/p） / (SSE/N-p-1)"></p><h4 id="8-6-4-卡方检验"><a href="#8-6-4-卡方检验" class="headerlink" title="8.6.4 卡方检验"></a>8.6.4 卡方检验</h4><p>卡方检验在统计学里属于非参数检验，主要用来度量类别性变量，包括次序型变量等定性变量之间的关联性以及比较两个或者两个以上的样本率，基本思想就是比较理论频数和实际频数的吻合程度或拟合度，主要用于二元变量。</p><h4 id="8-6-5-IV和WOE"><a href="#8-6-5-IV和WOE" class="headerlink" title="8.6.5 IV和WOE"></a>8.6.5 IV和WOE</h4><p>当目标变量是二元变量，自变量是区间型变量时，可以通过IV和WOE进行自变量的判断和取舍，在应用IV和WOE的时候，需要把区间型变量转换成类别性（次序型）自变量，同时要强调目标变量必须是二元变量，这两点是使用IV和WOE的前提条件。</p><p>一个变量的总的预测能力是通过IV来表现的，它是该变量的各个属性的WOE的加权总和，IV代表了该变量区分目标变量中的事件与非事件的能力。</p><p>与IV有相似左右的一个变量是Gini分数，其计算步骤如下：</p><ol><li>根据该字段的每个属性包含的预测事件和非事件的比率，按照各属性的比率的降序进行排列</li><li>针对排序后的每个组，分别计算该组内的事件数量和非事件数量</li><li>计算Gini分数</li></ol><p>应用IV、WOE、Gini Score 指标时，可以在数据挖掘实践中实现以下目标：</p><ul><li>通过WOE的变化来调整出最佳的分箱阈值</li><li>通过IV或者Gini分数，筛选出有较高预测价值的自变量，投入模型的训练中</li></ul><h4 id="8-6-6-部分建模算法自身的筛选功能"><a href="#8-6-6-部分建模算法自身的筛选功能" class="headerlink" title="8.6.6 部分建模算法自身的筛选功能"></a>8.6.6 部分建模算法自身的筛选功能</h4><p>可供“借力”的算法或者模型：包括决策树模型、回归模型等。</p><h4 id="8-6-7-降维的方法"><a href="#8-6-7-降维的方法" class="headerlink" title="8.6.7 降维的方法"></a>8.6.7 降维的方法</h4><p>主成分分析和变量聚类</p><h3 id="8-7-共线性问题"><a href="#8-7-共线性问题" class="headerlink" title="8.7 共线性问题"></a>8.7 共线性问题</h3><p>共线性问题是困扰模型预测能力的一个常见问题，所谓共线性，又叫多重共线性，是指当自变量之间存在较强的，甚至完全的线性相关关系，当自变量之间高度相关是，模型参数会变得不稳定，模型的预测能力会降低，理论上来讲，输入变量之间除了共线性关系，也有可能存在非线性关系，这些非线性关系很有可能如共线性关系一样影响模型的预测能力。</p><h4 id="8-7-1-如何发现共线性？"><a href="#8-7-1-如何发现共线性？" class="headerlink" title="8.7.1 如何发现共线性？"></a>8.7.1 如何发现共线性？</h4><p>A.相关系数的方法，最常见的就是皮尔逊系数<br>B. 通过模型结论的观察，比如回归模型中，如果回归系数的标准差过大，就有可能意味着变量之间存在着共线性问题<br>C. 主成分分析，在主成分分析方法中，主成分里的系数，也就是主成分在和大小能从一定程度上反映出各个变量的相关性，比如，第一成分中，某几个原始变量的主成分载荷系数较大，且数值相近，就有可能存在共线性关系<br>D.根据业务经验判断，原本应该没有预测作用的变量突然之间变得有很强的统计性，有可能隐藏着共线性问题<br>E. 对变量进行聚类，通过对区域型变量进行聚类，同一类中的变量之间具有较强的相似性，可能隐藏共线性关系</p><h4 id="8-7-2-如何处理共线性？"><a href="#8-7-2-如何处理共线性？" class="headerlink" title="8.7.2 如何处理共线性？"></a>8.7.2 如何处理共线性？</h4><p>模型拟合度高，样本量大的时候，轻微的共线性可以适当的采用视而不见的方法，但是，当样本量较少，很轻微的共线性问题都可能导致参数的不稳定。</p><p>如果发生严重的共线性问题，可以采取如下措施：<br>A.对相关变量进行取舍，高度共线性的变量，选择爆率对业务方最有价值最有意义的变量<br>B.对相关变量进行组合，生成一个新的综合型变量<br>C.当我们利用相关变量通过线性的方式衍生出新的变量时，记得两者之间的共线性问题，并且及时删除相关的原始变量<br>D.尝试对相关变量进行一些形式的转换，恰当的转换，可以在一定的程度上减少甚至去除共线性关系</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/2020072518565969.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center&quot; alt=&quot;在这里插入图片描述&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="产品/运营" scheme="http://yoursite.com/categories/%E4%BA%A7%E5%93%81-%E8%BF%90%E8%90%A5/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="数据化运营" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%8C%96%E8%BF%90%E8%90%A5/"/>
    
  </entry>
  
  <entry>
    <title>《数据挖掘与数据化运营实战》（第4-7章）</title>
    <link href="http://yoursite.com/2020/07/24/%E3%80%8A%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%8C%96%E8%BF%90%E8%90%A5%E5%AE%9E%E6%88%98%E3%80%8B%EF%BC%88%E7%AC%AC4-7%E7%AB%A0%EF%BC%89/"/>
    <id>http://yoursite.com/2020/07/24/%E3%80%8A%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%8C%96%E8%BF%90%E8%90%A5%E5%AE%9E%E6%88%98%E3%80%8B%EF%BC%88%E7%AC%AC4-7%E7%AB%A0%EF%BC%89/</id>
    <published>2020-07-24T09:48:10.000Z</published>
    <updated>2020-07-24T09:48:18.559Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://img-blog.csdnimg.cn/20200724174615417.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><a id="more"></a><h2 id="4-数据化运营是跨专业、跨团队的协调与合作"><a href="#4-数据化运营是跨专业、跨团队的协调与合作" class="headerlink" title="4 数据化运营是跨专业、跨团队的协调与合作"></a>4 数据化运营是跨专业、跨团队的协调与合作</h2><p>在线运营团队需具有的与数据相关的基本技能要求：</p><ul><li>提出业务分析需求并且能胜任基本的数据分析</li><li>提供业务经验和参考建议</li><li>策划和执行精细化运营方案</li><li>跟踪运营效果、反馈和总结</li></ul><h2 id="6-数据挖掘项目完整应用案例演示"><a href="#6-数据挖掘项目完整应用案例演示" class="headerlink" title="6 数据挖掘项目完整应用案例演示"></a>6 数据挖掘项目完整应用案例演示</h2><p><strong>项目流程：</strong></p><ol><li>项目背景和业务分析需求的提出</li><li>数据分析师参与需求讨论</li><li>制定需求分析框架和分析计划</li><li>抽取样本数据、熟悉数据、数据清洗和摸底</li><li>按计划初步搭建挖掘模型</li><li>与业务方讨论模型的初步结论，提出新的思路和模型优化方案</li><li>按优化方案重新抽取样本并建模，提炼结论并验证模型</li><li>完成分析报告和落地应用建议</li><li>制定具体的落地应用方案和评估方案<ol start="10"><li>业务方实施落地应用方案并跟踪、评估结果</li><li>落地应用方案在实际效果评估后，不断修正完善</li><li>不同运营方案的评估、总结和反馈</li><li>项目应用后的总结和反思</li></ol></li></ol><h2 id="7-数据挖掘建模的优化和限度"><a href="#7-数据挖掘建模的优化和限度" class="headerlink" title="7 数据挖掘建模的优化和限度"></a>7 数据挖掘建模的优化和限度</h2><h3 id="7-1-数据挖掘模型的优化要遵循有效、适度的原则"><a href="#7-1-数据挖掘模型的优化要遵循有效、适度的原则" class="headerlink" title="7.1 数据挖掘模型的优化要遵循有效、适度的原则"></a>7.1 数据挖掘模型的优化要遵循有效、适度的原则</h3><p>在模型优化和资源投入之间，在投入数据分析资源和满足特定业务需求之间，又有一个微妙的平衡点——性价比，这个平衡点决定了模型的优化和完善是有限度的。</p><p>任何一个数据挖掘模型都是针对一个特定业务需求的，围绕着一个具体的业务需求，数据挖掘模型总是可以有办法不断完善、不断提升，即提升精确度、提升转化率等。</p><ul><li>有效原则：模型的结论或者应用效果是否满足当初业务需求，还需要考虑时效性</li><li>适度原则：投入产出性价比</li></ul><h3 id="7-2-如何有效地优化模型"><a href="#7-2-如何有效地优化模型" class="headerlink" title="7.2 如何有效地优化模型"></a>7.2 如何有效地优化模型</h3><h4 id="7-2-1-从业务思路上优化"><a href="#7-2-1-从业务思路上优化" class="headerlink" title="7.2.1 从业务思路上优化"></a>7.2.1 从业务思路上优化</h4><p>从业务思路上优化模型是最重要的模型优化措施。可从以下几个层面进行考虑：</p><ul><li>有没有更加明显且直观的规则、指标可以代替复杂的建模</li><li>有没有一些明显的业务逻辑（业务假设）在前期的建模阶段被疏忽</li><li>通过前期的初步建模和数据熟悉，是否有新的发现，甚至能颠覆之前的业务推测或者业务直觉</li><li>目标变量的定义是否稳定（在不同的时间点抽样验证）</li></ul><h4 id="7-2-2-从建模的技术思路上优化"><a href="#7-2-2-从建模的技术思路上优化" class="headerlink" title="7.2.2 从建模的技术思路上优化"></a>7.2.2 从建模的技术思路上优化</h4><p>建模的总体技术思路包括不同的建模算法、不同的抽样方法、有没有必要通过细分群体来分别建模等。</p><ul><li><strong>建模算法</strong>：预测响应（或分类）模型思路里的不同算法——逻辑回归算法、决策树算法、神经网络算法、支持向量机算法等。基本统计分析方法。</li><li><strong>抽样方法</strong>：是否抽样、如何抽样、过抽样。</li><li><strong>细分群体</strong>：细分建模有时候会通过故意漏掉一小部分目标用户，从而可以针对剩下的绝大多数目标用户进行更有效的预测。</li></ul><h4 id="7-2-3-从建模的技术技巧上优化"><a href="#7-2-3-从建模的技术技巧上优化" class="headerlink" title="7.2.3 从建模的技术技巧上优化"></a>7.2.3 从建模的技术技巧上优化</h4><p>在建模过程中，业务思路上的优化比建模技术思路上的优化更重要，而建模技术思路上的优化又比单纯的建模技巧的优化更重要。</p><h3 id="7-3-模型效果评价的主要指标体系（二元目标变量）"><a href="#7-3-模型效果评价的主要指标体系（二元目标变量）" class="headerlink" title="7.3 模型效果评价的主要指标体系（二元目标变量）"></a>7.3 模型效果评价的主要指标体系（二元目标变量）</h3><h4 id="7-3-1-评价模型准确度和精度的系列指标"><a href="#7-3-1-评价模型准确度和精度的系列指标" class="headerlink" title="7.3.1 评价模型准确度和精度的系列指标"></a>7.3.1 评价模型准确度和精度的系列指标</h4><ul><li>True Positive(TP)：指模型预测为正（1）的，并且实际上也的确是正（1）的观察对象数量</li><li>True Negative(TN)：指模型预测为负（0）的，并且实际上也的确是正（0）的观察对象数量</li><li>False Positive(FP)：指模型预测为正（1）的，并且实际上是负（0）的观察对象数量</li><li>False Negative(FN)：指模型预测为负（0）的，并且实际上是正（1）的观察对象数量</li></ul><p><img src="https://img-blog.csdnimg.cn/20200724165722819.PNG#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200724170007879.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200724170017577.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200724170054806.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h4 id="7-3-2-ROC曲线"><a href="#7-3-2-ROC曲线" class="headerlink" title="7.3.2 ROC曲线"></a>7.3.2 ROC曲线</h4><p>ROC曲线是一种有效比较两个二元分类模型的可视工具，它显示了给定模型的灵敏性真正率和假正率之间的比较评定。真正率的增加是以假正率的增加为代价的，ROC曲线下面的面积就是比较模型准确度的指标和依据，面积大的模型对应的模型准确度要高，也就是要择优应用的模型，面积越接近0.5，对应的模型的准确率就越低。</p><p>要绘制ROC曲线，首先要对模型所做的判断即对于的数据做排序，把经过模型判断后的观察值预测为正（1）的概率从高到低进行排序，ROC曲线的纵轴表示真正率，ROC曲线的横轴表示假正率。具体绘制时，从左下角开始，此时真正率和假正率都为0，按照刚才概率从高到低的顺序，依次针对每个观察值实际的“正”或“负”绘制，如果它是真正的“正”（预测正确），则ROC曲线向上移动并绘制一个点；如果它是真正的“负”，则ROC曲线向右移动一个点。<br><img src="https://img-blog.csdnimg.cn/20200724170831729.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h4 id="7-3-3-KS值"><a href="#7-3-3-KS值" class="headerlink" title="7.3.3 KS值"></a>7.3.3 KS值</h4><p>如果KS值越大，表示模型能够将正（1）、负（0）客户区分开来的程度越大，模型预测的准确率也越高。通常来讲，KS大于0.2就表示模型有较好的预测准确性了。</p><p>KS曲线绘制步骤如下：</p><ol><li>将测试集里所有观察对象经过模型打分预测为正（1）的对象按概率从高到低排序。</li><li>分别计算每个概率分数所对应的实际上为正（1）、负（0）的观察对象累计值，以及它们分别占全体总数，实际正（1）、负（0）的总数量的百分比。</li><li>将这两种累计的百分比与评分分数绘制在同一张图上，得到KS曲线，如下图：<br><img src="https://img-blog.csdnimg.cn/20200724171116142.PNG#pic_center" alt="在这里插入图片描述"></li><li>各分数对应下的累计的、真正的正（1）观察对象的百分比与累计的、真正的负（0）观察对象的百分比之差的最大值就是KS值。</li></ol><h4 id="7-3-4-lift值"><a href="#7-3-4-lift值" class="headerlink" title="7.3.4 lift值"></a>7.3.4 lift值</h4><p>在二元预测模型在具体的业务场景中，都有一个random rate，所谓random rate，是指在不使用模型的时候，基于已有业务效果的正比例，也就是不使用模型之前“正”的实际观察对象在总体观察对象中的占比。如果经过建模，有一个不错的模型，那么这个模型就可以比较有效地锁定群体了，所谓有效，是指在预测概率的数值从高到低的排序中，排名靠前的观察值中，真正的“正”观察值在累计的总观察值里的占比应该是高于random rate的。<br><img src="https://img-blog.csdnimg.cn/20200724173253994.PNG#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200724173344855.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>从上述lift公式中，引出了在模型评估中常用的两个评价指标，分别是响应率（%response）和捕获率（%captured response）。首先要把经过模型预测后为正（1）的观察对象按照预测概率从高到低排序，然后对这些观察对象按照均等的数量划分为10个区间，每个区间里观察对象的数量一致，这样各个区间可以被命名为排序最高的前10%的对象排序最高的前20%对象等。</p><ul><li>响应率是指按上述概率分数排序后的某区间段或累计区间观察对象中，实际属于正（1）的观察对象占该区间或该累计区间总体观察对象数量的百分比。很明显，响应率越高说明该区间预测准确率越高。</li><li>捕获率是指上述排序区间的观察对象中，实际属于正（1）的观察对象占全体观察对象中属于正（1）的总数的百分比，同样是越高越好。</li></ul><h4 id="7-3-5-模型稳定性的评估"><a href="#7-3-5-模型稳定性的评估" class="headerlink" title="7.3.5 模型稳定性的评估"></a>7.3.5 模型稳定性的评估</h4><p>考察稳定性最好的办法就是抽取另外一个时间段（时间窗口）的数据，最好是最新时间的数据，通过模型对这些新的数据、新对象进行预测（打分），然后与实际情况进行比较，并且跟模型在测试集和验证集里的表现相比较，看模型是否稳定，其效果衰减的幅度是否可以接受。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20200724174615417.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center&quot; alt=&quot;在这里插入图片描述&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="产品/运营" scheme="http://yoursite.com/categories/%E4%BA%A7%E5%93%81-%E8%BF%90%E8%90%A5/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="数据化运营" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%8C%96%E8%BF%90%E8%90%A5/"/>
    
  </entry>
  
  <entry>
    <title>《数据挖掘与数据化运营实战》（第3章）</title>
    <link href="http://yoursite.com/2020/07/23/%E3%80%8A%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%8C%96%E8%BF%90%E8%90%A5%E5%AE%9E%E6%88%98%E3%80%8B%EF%BC%88%E7%AC%AC3%E7%AB%A0%EF%BC%89/"/>
    <id>http://yoursite.com/2020/07/23/%E3%80%8A%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%8C%96%E8%BF%90%E8%90%A5%E5%AE%9E%E6%88%98%E3%80%8B%EF%BC%88%E7%AC%AC3%E7%AB%A0%EF%BC%89/</id>
    <published>2020-07-23T09:18:45.000Z</published>
    <updated>2020-07-23T09:19:02.365Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://img-blog.csdnimg.cn/20200723171623777.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><a id="more"></a><h2 id="3-数据化运营中常见的数据分析项目类型"><a href="#3-数据化运营中常见的数据分析项目类型" class="headerlink" title="3 数据化运营中常见的数据分析项目类型"></a>3 数据化运营中常见的数据分析项目类型</h2><h3 id="3-1-用户特征分析及用户分层模型"><a href="#3-1-用户特征分析及用户分层模型" class="headerlink" title="3.1 用户特征分析及用户分层模型"></a>3.1 用户特征分析及用户分层模型</h3><p>不同于过去的粗放式运营，在精细化运营的要求下，可以通过数据分析挖掘不同用户细分群体的特点，针对不同群体提供精细化、个性化的运营或服务，提高用户满意度，最终提高产品变现能力。</p><p>这类以挖掘用户特征为目标的项目，可以统称为用户特征分析。项目的核心是用户分层，一般需要对用户分层，在分层时或分层后，分析细分群体的特征。</p><p>分层模型常用的技术既包括统计分析技术（比如相关性分析、主成分分析等），又可以含有预测（响应、分类）模型的技术（比如通过搭建预测模型发现最重要的输入变量及其排序情况，然后根据这些变量对分层进行大致的划分，并通过实际数据进行验证），这要视具体的分析目的、业务背景和数据结构而定，同时要强调的是，一个好的分层模型的搭建一定是需要业务方的参与和贡献的，而且其中的业务逻辑和业务思考远远胜过分析技术本身。</p><p>用户特征分析根据不同的侧重点，有几个典型的业务场景：</p><ul><li><strong>寻找目标用户</strong>，不同行为习惯的用户，定位高质量用户及其特征</li><li><strong>寻找运营的抓手</strong>，帮助寻找运营方式，以提高或改善用户满意度</li><li><strong>用户群体细分</strong></li><li><strong>新产品开发依据</strong>，即判断新产品要针对谁、满足什么需求，实现PMF（产品-市场匹配）</li></ul><p>用户特征分析项目中有以下几种常见分析思路和方法：</p><ul><li><strong>RFM模型</strong>（Recency、Frequency、Monetary），一种根据三个核心变量，对用户进行分组的方法</li><li><strong>聚类算法</strong>，用户分层是聚类算法的重要应用，可以根据核心变量进行聚类，根据聚类结果对用户分层，再针对细分群体分析其余特征</li><li><strong>监督模型的核心变量</strong>，一些算法模型可以帮助定位核心变量，如随机森林的特征重要性指标、决策树的划分规则等，根据模型定位到的核心变量，对核心变量进行大致划分得到分层结果</li></ul><h3 id="3-2-目标客户的预测（响应、分类）模型"><a href="#3-2-目标客户的预测（响应、分类）模型" class="headerlink" title="3.2 目标客户的预测（响应、分类）模型"></a>3.2 目标客户的预测（响应、分类）模型</h3><p>预测（响应、分类）模型包括流失预警模型、付费预测模型、续费预测模型、运营活动响应模型等。它是数据挖掘中最常用的一种模型类型，几乎成了数据挖掘技术应用的一个主要代名词。响应模型的核心就是响应概率，响应概率又是数据化运营六要素里的核心要素—概率（Probability）。</p><p>预测（响应、分类）模型基于真实业务场景产生的数据而进行的预测（响应、分类）模型搭建，其中涉及的主要数据挖掘技术包括逻辑回归、决策树、神经网络、支持向量机等。有没有一个算法总是优先于其他算法呢？答案是否定的，没有哪个算法在任何场景下都总能最优胜任响应模型的搭建，所以通常在建模过程中，会尝试多种不同的算法，然后根据随后的验证效果以及具体业务项目的资源和价值进行权衡，并做出最终的选择。</p><p>根据建模数据中实际响应比例的大小进行分类，响应模型还可以细分为普通响应模型和稀有事件响应模型，一般来讲，如果响应比例低于1%，则应当作为稀有事件响应模型来进行处理，其中的核心就是抽样，通过抽样技术人为放大分析数据样本里响应事件的比例，增加响应事件的浓度，从而在建模过程中更好地捕捉、拟合其中自变量与因变量的关系。</p><p>预测（响应、分类）模型除了可以有效预测个体响应的概率之外，模型本身显示出的重要输入变量与目标变量的关系也有重要的业务价值，比如说可以转化成伴随（甚至导致）发生响应（生成事件）的关联因素、重要因素的提炼。</p><h3 id="3-3-运营群体的活跃度定义"><a href="#3-3-运营群体的活跃度定义" class="headerlink" title="3.3 运营群体的活跃度定义"></a>3.3 运营群体的活跃度定义</h3><p>活跃度的定义没有统一的描述，一般都是根据特定的业务场景和运营需求来量身订做的。但是，其中最重要、最常见的两个基本点如下：</p><ul><li>活跃度的组成指标应该是该业务场景中最核心的行为因素。</li><li>衡量活跃度的定义合适与否的重要判断依据是其能否有效回答业务需求的终极目标。</li></ul><p>以活跃为例，举两个不同场景的例子——</p><blockquote><p>为了产品的日常监控，需要定义用户活跃的口径，即如何判定用户是否活跃，用于统计DAU等指标。</p></blockquote><p>除了核心行为的条件，该业务场景要求指标简单、容易理解，因此用户活跃应该只用到尽量少的指标，通常是1个，如是否登陆、观看视频数&gt;1等。</p><blockquote><p>为了提高产品付费率，需要定义用户的活跃度，使得满足一定活跃度分值的用户比较容易转化为付费用户。</p></blockquote><p>该场景下，要求将多个核心行为转化为分值，涉及的主要技术有两种：</p><ul><li><strong>数据标准化/归一化</strong>，通过数据标准化/归一化，将不同的指标缩放至相同区间，不同指标才能平等比较，加权转化为一个综合分数</li><li><strong>主成分分析</strong>，是一种降维算法，在这里将多个核心行为转化为一个或几个主成分，最终转化为一个综合分数</li></ul><h3 id="3-4-用户路径分析"><a href="#3-4-用户路径分析" class="headerlink" title="3.4 用户路径分析"></a>3.4 用户路径分析</h3><p>用户路径分析主要是分析用户在网页上流转的规律和特点，发现频繁访问的路径模式，这些路径的发现可以有很多业务用途，包括提炼特定用户群体的主流路径、网页设计的优化和改版、用户可能浏览的下一个页面的预测、特定群体的浏览特征等。另外，路径分析所用的数据主要是Web服务器中的日志数据。</p><p>路径分析常用的分析技术有两类，一类是有算法支持的，另一类是严格按照步骤顺序遍历主要路径的。</p><p>在运营团队看来，路径分析的主要用途：</p><ul><li>监控运营活动（或者目标客户）的典型路径，看是否与当初的运营设想一致。如果不一致，就继续深入分析原因，调整运营思路或页面布局，最终目的就是提升用户点击页面的效率；</li><li>提炼新的有价值的频繁路径模式，并且在以后的运营中对这些模式加以应用，提升运营的效率和特定效果。</li></ul><h3 id="3-5-交叉销售模型"><a href="#3-5-交叉销售模型" class="headerlink" title="3.5 交叉销售模型"></a>3.5 交叉销售模型</h3><p>交叉销售，其背后的理论依据是一旦客户购买了商品（或者成为付费用户），企业就会想方设法保留和延长这些客户在企业的生命周期和客户的利润贡献，一般有两种思路：</p><ul><li><strong>延缓用户流失</strong>，这种思路通常使用流失预警模型，对可能流失的用户指定关怀策略等<ul><li><strong>让用户消费更多商品或服务</strong>，找出用户感兴趣的商品或服务，挖掘用户的消费或使用需求，这种思路主要就涉及到<strong>交叉销售模型</strong></li></ul></li></ul><p>交叉销售模型通过对用户历史消费数据的分析挖掘，找出有明显关联性质的商品组合，然后用不同的建模方法，去构建消费者购买这些关联商品组合的可能性模型，再用其中优秀的模型去预测新客户中购买特定商品组合的可能性。如捆绑销售、精准营销推广等。“啤酒与尿布”的案例就是比较经典的交叉销售模型应用。</p><p>主要思路：</p><ul><li>一是按照关联技术（Association Analysis），也即通常所说的购物篮分析，发现那些有较大可能被一起采购的商品，将它们进行有针对性的促销和捆绑，这就是交叉销售；</li><li>二是借鉴响应模型的思路，为某几种重要商品分别建立预测模型，对潜在消费者通过这些特定预测模型进行过滤，然后针对最有可能的前5%的消费者进行精确的营销推广；</li><li>三是仍然借鉴预测响应模型的思路，让重要商品两两组合，找出那些最有可能消费的潜在客户；</li><li>四是通过决策树清晰的树状规则，发现基于具体数据资源的具体规则（有的多，有的少），很多营销方案的制订和执行实际上都是通过这种方式找到灵感和思路的。</li></ul><p>相应的建模技术主要包括关联分析（Association Analysis）、序列分析（Sequence Analysis），即在关联分析的基础上，增加了先后顺序的考虑，以及预测（响应、分类）模型技术，诸如逻辑回归、决策树等。</p><h3 id="3-6-信息质量模型"><a href="#3-6-信息质量模型" class="headerlink" title="3.6 信息质量模型"></a>3.6 信息质量模型</h3><p>无论是B2C（如当当网、凡客网），还是C2C（如淘宝网），或者是B2B（如阿里巴巴），只要是以商业为目的，以交易为目的的，都需要采用有效手段去提升海量商业信息（商品目录、商品Offer、商品展示等）的质量和结构，从而促进交易。</p><p>构建信息质量模型所涉及的主要还是常规的数据挖掘技术，比如回归算法、决策树等。但是对于信息质量模型的需求，由于其目标变量具有一定的特殊性，因此它与目标客户预测（响应）模型在思路和方法上会有一些不同之处，具体内容如下。</p><p>任何模型的搭建都是用于响应特定的业务场景和业务需求的，有时候搭建信息质量模型的目标变量是该信息（如商品Offer）是否在特定的时间段产生了交易，此时，目标变量就是二元的，即是与否；更多时候，信息质量模型的目标变量与是否交易没有直接关系（这其实很容易理解，因为影响成交的因素太多），甚至有些时候信息质量本身是主观的判断，在这种情况下，没有明确的来自实际数据的目标变量。那如何定义目标变量呢？<strong>专家打分，模型拟合</strong>是一个比较合适的变通策略。</p><h3 id="3-7-服务保障模型"><a href="#3-7-服务保障模型" class="headerlink" title="3.7 服务保障模型"></a>3.7 服务保障模型</h3><p>服务保障模型主要是站在为客户服务的角度来说的，出发点是为了让客户（平台的卖家）更好地做生意，达成更多的交易，我们（平台）应该为他们提供哪些有价值的服务去支持、保障卖家生意的发展，这里的服务方向就可以有很多的空间去想象了。无论是产品武装，还是宣传帮助，都属于服务保障的范畴，都是服务保障模型可以并且应该出力的方向。</p><h3 id="3-8-信用风险模型"><a href="#3-8-信用风险模型" class="headerlink" title="3.8 信用风险模型"></a>3.8 信用风险模型</h3><p>信用风险模型主要是应对在产品中可能遇到的风险，并作相应的预警。如欺诈预警、高危用户判断、违禁信息过滤等。</p><p>信用风险模型本质上也是一个预测模型，与常规数据挖掘项目的算法和思路都是通用的，但由于其特殊的场景，通常有如下的特点：</p><ul><li>分析结论或者欺诈识别模型的时效更短，需要优化（更新）的频率更高。网络上骗子的行骗手法经常会变化，导致分析预警行骗欺诈的模型也要因此持续更新。</li><li>行骗手段的变化很大程度上是随机性的，所以这对欺诈预警模型的及时性和准确性提出了严重的挑战。</li><li>对根据预测模型提炼出的核心因子进行简单的规则梳理和罗列，这样就可在风控管理的初期阶段有效锁定潜在的目标群体。</li></ul><h3 id="3-9-商品推荐模型"><a href="#3-9-商品推荐模型" class="headerlink" title="3.9 商品推荐模型"></a>3.9 商品推荐模型</h3><p>推荐系统在互联网产品有很多应用，如淘宝的商品推荐、知乎的问题推荐、微视的短视频推荐等等。</p><p>推荐系统的常用算法有：</p><ul><li><strong>基于关联分析进行推荐</strong>，如<strong>Apriori算法</strong>，从数据中找到商品（或其他推荐的对象）的关联规则，基于该关联规则进行推荐</li><li><strong>协同过滤算法</strong>，有<strong>基于用户</strong>（User-based）的协同过滤和<strong>基于项目</strong>（Item-based）的协同过滤，核心思想是相似的用户会喜欢相似的东西</li></ul><h4 id="3-9-1-Apriori算法"><a href="#3-9-1-Apriori算法" class="headerlink" title="3.9.1 Apriori算法"></a>3.9.1 Apriori算法</h4><p>Apriori算法主要包含两个步骤：首先找出数据集中所有的频繁项集，这些项集出现的频繁性要大于或等于最小支持度；然后根据频繁项集产生强关联规则，这些规则必须满足最小支持度和最小置信度。</p><p>事实上，在关联规则中用于度量规则质量的两个主要指标即为支持度和置信度。<br><img src="https://img-blog.csdnimg.cn/20200723170242444.PNG#pic_center" alt="在这里插入图片描述"></p><h4 id="3-9-2-协同过滤算法"><a href="#3-9-2-协同过滤算法" class="headerlink" title="3.9.2 协同过滤算法"></a>3.9.2 协同过滤算法</h4><p>协同过滤是迄今为止最成功的推荐系统技术，被应用在很多成功的推荐系统中。协同过滤算法主要分为基于启发式和基于模型式两种。其中，基于启发式的协同过滤算法，又可以分为基于用户的协同过滤算法和基于项目的协同过滤算法。启发式协同过滤算法主要包含3个步骤：1）收集用户偏好信息；2）寻找相似的商品或者用户；3）产生推荐。</p><p>协同过滤的输入数据集主要是用户评论数据集或者行为数据集。这些数据集主要又分为显性数据和隐性数据两种类型。其中，显性数据主要是用户打分数据，譬如用户对商品的打分；而隐性数据主要是指用户点击行为、购买行为和搜索行为等，这些数据隐性地揭示了用户对商品的喜好。</p><h3 id="3-10-数据产品"><a href="#3-10-数据产品" class="headerlink" title="3.10 数据产品"></a>3.10 数据产品</h3><p>数据产品是指数据分析师为了响应数据化运营的号召，提高企业全员数据化运营的效率，以及提升企业全员使用数据、分析数据的能力而设计和开发的一系列有关数据分析应用的工具。数据产品如：银行账户交易明细，购物收藏等。</p><h3 id="3-11-决策支持"><a href="#3-11-决策支持" class="headerlink" title="3.11 决策支持"></a>3.11 决策支持</h3><p>数据分析挖掘所承担的决策支持主要是指通过数据分析结论、数据模型对管理层的管理、决策提供响应和支持，从而帮助决策层提高决策水平和质量。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20200723171623777.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center&quot; alt=&quot;在这里插入图片描述&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="产品/运营" scheme="http://yoursite.com/categories/%E4%BA%A7%E5%93%81-%E8%BF%90%E8%90%A5/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="数据化运营" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%8C%96%E8%BF%90%E8%90%A5/"/>
    
  </entry>
  
  <entry>
    <title>《数据挖掘与数据化运营实战》（第1-2章）</title>
    <link href="http://yoursite.com/2020/07/23/%E3%80%8A%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%8C%96%E8%BF%90%E8%90%A5%E5%AE%9E%E6%88%98%E3%80%8B%EF%BC%88%E7%AC%AC1-2%E7%AB%A0%EF%BC%89/"/>
    <id>http://yoursite.com/2020/07/23/%E3%80%8A%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%8C%96%E8%BF%90%E8%90%A5%E5%AE%9E%E6%88%98%E3%80%8B%EF%BC%88%E7%AC%AC1-2%E7%AB%A0%EF%BC%89/</id>
    <published>2020-07-23T06:45:34.000Z</published>
    <updated>2020-07-23T07:05:33.236Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://img-blog.csdnimg.cn/20200723150511532.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><a id="more"></a><h2 id="1-数据化运营概述"><a href="#1-数据化运营概述" class="headerlink" title="1 数据化运营概述"></a>1 数据化运营概述</h2><h3 id="1-1-从4P到4C再到3P3C"><a href="#1-1-从4P到4C再到3P3C" class="headerlink" title="1.1 从4P到4C再到3P3C"></a>1.1 从4P到4C再到3P3C</h3><p>4P指的是Product（产品）、Price（价格）、Place（渠道）和Promotion（促销）。<br><img src="https://img-blog.csdnimg.cn/20200723124615988.PNG#pic_center" alt="在这里插入图片描述"></p><ul><li>Product：表示注重产品功能，强调独特卖点。</li><li>Price：指根据不同的市场定位，制定不同的价格策略。</li><li>Place：指要注重分销商的培养和销售网络的建设。</li><li>Promotion：指企业可以通过改变销售行为来刺激消费者，以短期的行为（如让利、买一送一、调动营销现场气氛等）促成消费的增长，吸引其他品牌的消费者前来消费，或者促使老主顾提前来消费，从而达到销售增长的目的。</li></ul><p>4P理论的核心是Product（产品）。因此，以4P理论为核心营销思想的企业营销战略又可以简称为“以产品为中心”的营销战略。</p><p>在当今，传统的4P营销组合已经无法适应时代发展的需求，营销界开始研究新的营销理论和营销要素。其中，最具代表性的理论就是4C理论，这里的4C包括Consumer（消费者）、Cost（成本）、Convenience（方便性）和Communication（沟通交流）。<br><img src="https://img-blog.csdnimg.cn/20200723125246548.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><ul><li>消费者的需求与愿望（Customer’s Needs and Wants）。</li><li>消费者得到满足的成本（Cost and Value toSatisfy Consumer’s Needs and Wants）。<ul><li>用户购买的方便性（Convenience to Buy）。</li><li>与用户的沟通交流（Communication with Consumer）。</li></ul></li></ul><p>4C理论的核心是Consumer消费者。因此，以4C理论为核心营销思想的企业营销战略又可以简称为“以消费者为中心”的营销战略。</p><p>4C理论虽然成功找到了从“以产品为中心”转化为“以消费者为中心”的思路和要素，但是随着社会的进步，科技的发展，大数据时代的来临，4C理论再次落后于时代发展的需要。于是在基本思路上融合了4P理论和4C理论的nPnC形式的理论出现了。</p><p>虽然学术界对于到底是几个P和几个C仍存在着争议，没有定论，这里以3P3C为例概述互联网行业运营的典型理论探索。<br><img src="https://img-blog.csdnimg.cn/20200723125845698.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><ul><li>Probability（概率）：营销、运营活动以概率为核心，追求精细化和精准率。</li><li>Product（产品）：注重产品功能，强调产品卖点。</li><li>Prospects（消费者，目标用户）。</li><li>Creative（创意，包括文案、活动等）。</li><li>Channel（渠道）。</li><li>Cost/Price（成本/价格）。</li></ul><p>而在这其中，以数据分析挖掘所支撑的目标响应概率（Probability）是核心，在此基础上将会围绕产品功能优化、目标用户细分、活动（文案）创意、渠道优化、成本的调整等重要环节和要素，共同促使数据化运营持续完善，直至成功。</p><h3 id="1-2-数据化运营的主要内容"><a href="#1-2-数据化运营的主要内容" class="headerlink" title="1.2 数据化运营的主要内容"></a>1.2 数据化运营的主要内容</h3><p>数据化运营是“以企业级海量数据的存储和分析挖掘应用为核心支持的，企业全员参与的，以精准、细分和精细化为特点的企业运营制度和战略”。</p><p>针对互联网运营部门的数据化运营，具体包括“网站流量监控分析、目标用户行为研究、网站日常更新内容编辑、网络营销策划推广”等，并且，这些内容是在以企业级海量数据的存储、分析、挖掘和应用为核心技术支持的基础上，通过可量化、可细分、可预测等一系列精细化的方式来进行的。</p><p>数据化运营，首先是要有企业全员参与意识，要达成这种全员的数据参与意识比单纯地执行数据挖掘技术显然是要困难得多，也重要得多的。其次是一种常态化的制度和流程，包括企业各个岗位和工种的数据收集和数据分析应用的框架和制度等。更是来自企业决策者、高层管理者的直接倡导和实质性的持续推动。</p><h3 id="1-3-数据化运营的原因及必要条件"><a href="#1-3-数据化运营的原因及必要条件" class="headerlink" title="1.3 数据化运营的原因及必要条件"></a>1.3 数据化运营的原因及必要条件</h3><p><strong>原因：</strong></p><ul><li>首先是现代企业竞争白热化、商业环境变成以消费者为主的“买方市场”等一系列竞争因素所呼唤的管理革命和技术革命。</li><li>其次，数据化运营是飞速发展的数据挖掘技术、数据存储技术等诸多先进数据技术直接推动的结果。</li><li>更是互联网企业得天独厚的“神器”。互联网行业与生俱来的特点就是大数据，而信息时代最大的财富也正是海量的大数据。</li></ul><p><strong>必要条件：</strong></p><ul><li>企业级海量数据存储的实现</li><li>精细化运营的需求</li><li>数据分析和数据挖掘技术的有效应用</li><li>企业决策层的倡导与持续支持</li></ul><h2 id="2-数据挖掘概述"><a href="#2-数据挖掘概述" class="headerlink" title="2 数据挖掘概述"></a>2 数据挖掘概述</h2><h3 id="2-1-统计分析与数据挖掘的主要区别"><a href="#2-1-统计分析与数据挖掘的主要区别" class="headerlink" title="2.1 统计分析与数据挖掘的主要区别"></a>2.1 统计分析与数据挖掘的主要区别</h3><p>在企业的商业实战中，数据分析师分析问题、解决问题时，首先考虑的是思路，其次才会对与思路匹配的分析挖掘技术进行筛选，而不是先考虑到底是用统计技术还是用数据挖掘技术来解决这个问题。</p><p>从两者的理论来源来看，它们在很多情况下都是同根同源的。比如，在属于典型的数据挖掘技术的决策树里，CART、CHAID等理论和方法都是基于统计理论所发展和延伸的；并且数据挖掘中的技术有相当比例是用统计学中的多变量分析来支撑的。</p><p>相对于传统的统计分析技术，数据挖掘有如下一些特点：</p><ul><li>数据挖掘特别擅长于处理大数据，尤其是几十万行、几百万行，甚至更多更大的数据。</li><li>数据挖掘在实践应用中一般都会借助数据挖掘工具。不过，基本的统计知识和技能是必需的。</li><li>在信息化时代，数据分析应用的趋势是从大型数据库中抓取数据，并通过专业软件进行分析，所以数据挖掘工具的应用更加符合企业实践和实战的需要。</li><li>从操作者来看，数据挖掘技术更多是企业的数据分析师、业务分析师在使用，而不是统计学家用于检测。</li></ul><h3 id="2-2-数据挖掘的主要成熟技术及应用"><a href="#2-2-数据挖掘的主要成熟技术及应用" class="headerlink" title="2.2 数据挖掘的主要成熟技术及应用"></a>2.2 数据挖掘的主要成熟技术及应用</h3><h4 id="2-2-1-决策树"><a href="#2-2-1-决策树" class="headerlink" title="2.2.1 决策树"></a>2.2.1 决策树</h4><p>决策树（Decision Tree）是一种非常成熟的、普遍采用的数据挖掘技术。在决策树里，所分析的数据样本先是集成为一个树根，然后经过层层分枝，最终形成若干个结点，每个结点代表一个结论。</p><p>决策树算法之所以在数据分析挖掘应用中如此流行，主要原因在于决策树的构造不需要任何领域的知识，很适合探索式的知识发掘，并且可以处理高维度的数据。它所产生的一系列从树根到树枝（或树叶）的规则，可以很容易地被分析师和业务人员理解，而且这些典型的规则甚至不用整理（或稍加整理），就是现成的可以应用的业务优化策略和业务优化路径。另外，决策树技术对数据的分布甚至缺失非常宽容，不容易受到极值的影响。</p><p>目前，最常用的3种决策树算法分别是CHAID、CART和ID3（包括后来的C4.5，乃至C5.0）。</p><ul><li>CHAID( Chi-square Automatic Interaction Detector)算法，中文简称为卡方自动相互关系检测。CHAID依据局部最优原则，利用卡方检验来选择对因变量最有影响的自变量，CHAID应用的前提是因变量为类别型变量（Category）。</li><li>CART( Classification and Regression Tree)算法，中文简称为分类与回归树，CART的分割逻辑与CHAID相同，每一层的划分都是基于对所有自变量的检验和选择上的。但是，CART采用的检验标准不是卡方检验，而是基尼系数（Gini）等不纯度的指标。两者最大的区别在于CHAID采用的是局部最优原则，即结点之间互不相干，一个结点确定了之后，下面的生长过程完全在结点内进行。而CART则着眼于总体优化，即先让树尽可能地生长，然后再回过头来对树进行修剪（Prune），这一点非常类似统计分析中回归算法里的反向选择（Backward Selection）。CART所生产的决策树是二分的，每个结点只能分出两枝，并且在树的生长过程中，同一个自变量可以反复使用多次（分割），这些都是不同于CHAID的特点。另外，如果是自变量存在数据缺失（Missing）的情况，CART的处理方式将会是寻找一个替代数据来代替（填充）缺失值，而CHAID则是把缺失数值作为单独的一类数值。</li><li>ID3（Iterative Dichotomiser）算法，中文简称为迭代的二分器，其最大的特点在于自变量的挑选标准是：基于信息增益的度量选择具有最高信息增益的属性作为结点的分裂（分割）属性，其结果就是对分割后的结点进行分类所需的信息量最小，这也是一种划分纯度的思想。至于之后发展起来的C4.5可以理解为ID3的发展版（后继版），两者的主要区别在于C4.5采用信息增益率（Gain Ratio）代替了ID3中的信息增益度量，如此替换的主要原因是信息增益度量有个缺点，就是倾向于选择具有大量值的属性。这里给个极端的例子，对于Member_Id 的划分，每个Id都是一个最纯的组，但是这样的划分没有任何实际意义。而C4.5 所采用的信息增益率就可以较好地克服这个缺点，它在信息增益的基础上，增加了一个分裂信息（SplitInformation）对其进行规范化约束。</li></ul><p>决策树技术在数据化运营中的主要用途体现在：作为分类、预测问题的典型支持技术，它在用户划分、行为预测、规则梳理等方面具有广泛的应用前景，决策树甚至可以作为其他建模技术前期进行变量筛选的一种方法，即通过决策树的分割来筛选有效地输入自变量。</p><h4 id="2-2-2-神经网络"><a href="#2-2-2-神经网络" class="headerlink" title="2.2.2 神经网络"></a>2.2.2 神经网络</h4><p>神经网络（Neural Network）是通过数学算法来模仿人脑思维的，它是数据挖掘中机器学习的典型代表。神经网络是人脑的抽象计算模型，数据挖掘中的“神经网络”也是由大量并行分布的人工神经元（微处理单元）组成的，它有通过调整连接强度从经验知识中进行学习的能力，并可以将这些知识进行应用。</p><p>简单来讲，“神经网络”就是通过输入多个非线性模型以及不同模型之间的加权互联（加权的过程在隐蔽层完成），最终得到一个输出模型。其中，隐蔽层所包含的就是非线性函数。</p><p>目前最主流的“神经网络”算法是反馈传播（Backpropagation），该算法在多层前向型（Multilayer Feed-Forward）神经网络上进行学习，而多层前向型神经网络又是由一个输入层、一个或多个隐蔽层以及一个输出层组成的，“神经网络”的典型结构如图所示。<br><img src="https://img-blog.csdnimg.cn/20200723134507907.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>由于“神经网络”拥有特有的大规模并行结构和信息的并行处理等特点，因此它具有良好的自适应性、自组织性和高容错性，并且具有较强的学习、记忆和识别功能。</p><p>“神经网络”的主要缺点就是其知识和结果的不可解释性，没有人知道隐蔽层里的非线性函数到底是如何处理自变量的，“神经网络”应用中的产出物在很多时候让人看不清其中的逻辑关系。但是，它的这个缺点并没有影响该技术在数据化运营中的广泛应用，甚至可以这样认为，正是因为其结果具有不可解释性，反而更有可能促使我们发现新的没有认识到的规律和关系。</p><p>在利用“神经网络”技术建模的过程中，有以下5个因素对模型结果有着重大影响：</p><ul><li>层数。</li><li>每层中输入变量的数量。</li><li>联系的种类。</li><li>联系的程度。</li><li>转换函数，又称激活函数或挤压函数。</li></ul><p>“神经网络”技术在数据化运营中的主要用途体现在：作为分类、预测问题的重要技术支持，在用户划分、行为预测、营销响应等诸多方面具有广泛的应用前景。</p><h4 id="2-2-3-回归"><a href="#2-2-3-回归" class="headerlink" title="2.2.3 回归"></a>2.2.3 回归</h4><p>回归（Regression）分析包括线性回归（Linear Regression），这里主要是指多元线性回归和逻辑斯蒂回归（Logistic Regression）。其中，在数据化运营中更多使用的是逻辑斯蒂回归，它又包括响应预测、分类划分等内容。</p><p><strong>多元线性回归</strong><br>多元线性回归主要描述一个因变量如何随着一批自变量的变化而变化，其回归公式（回归方程）就是因变量与自变量关系的数据反映。因变量的变化包括两部分：系统性变化与随机变化，其中，系统性变化是由自变量引起的（自变量可以解释的），随机变化是不能由自变量解释的，通常也称作残值。</p><p>在用来估算多元线性回归方程中自变量系数的方法中，最常用的是最小二乘法，即找出一组对应自变量的相应参数，以使因变量的实际观测值与回归方程的预测值之间的总方差减到最小。</p><p>对多元线性回归方程的参数估计，是基于下列假设的：</p><ul><li>输入变量是确定的变量，不是随机变量，而且输入的变量间无线性相关，即无共线性。</li><li>随机误差的期望值总和为零，即随机误差与自变量不相关。</li><li>随机误差呈现正态分布。</li></ul><p>如果不满足上述假设，就不能用最小二乘法进行回归系数的估算了。</p><p><strong>逻辑斯蒂回归</strong><br>逻辑斯蒂回归（Logistic Regression）相比于线性回归来说，在数据化运营中有更主流更频繁的应用，主要是因为该分析技术可以很好地回答诸如预测、分类等数据化运营常见的分析项目主题。简单来讲，凡是预测“两选一”事件的可能性（比如，“响应”还是“不响应”；“买”还是“不买”；“流失”还是“不流失”），都可以采用逻辑斯蒂回归方程。</p><p>逻辑斯蒂回归预测的因变量是介于0和1之间的概率，如果对这个概率进行换算，就可以用线性公式描述因变量与自变量的关系了，具体公式如下：<br><img src="https://img-blog.csdnimg.cn/20200723135550214.PNG#pic_center" alt="在这里插入图片描述"><br>与多元线性回归所采用的最小二乘法的参数估计方法相对应，最大似然法是逻辑斯蒂回归所采用的参数估计方法，其原理是找到这样一个参数，可以让样本数据所包含的观察值被观察到的可能性最大。这种寻找最大可能性的方法需要反复计算，对计算能力有很高的要求。最大似然法的优点是在大样本数据中参数的估值稳定、偏差小，估值方差小。</p><h4 id="2-2-4-关联规则"><a href="#2-2-4-关联规则" class="headerlink" title="2.2.4 关联规则"></a>2.2.4 关联规则</h4><p>关联规则（Association Rule）是在数据库和数据挖掘领域中被发明并被广泛研究的一种重要模型，关联规则数据挖掘的主要目的是找出数据集中的频繁模式（Frequent Pattern），即多次重复出现的模式和并发关系（Cooccurrence Relationships），即同时出现的关系，频繁和并发关系也称作关联（Association）。</p><p>应用关联规则最经典的案例就是购物篮分析（Basket Analysis），通过分析顾客购物篮中商品之间的关联，可以挖掘顾客的购物习惯，从而帮助零售商更好地制定有针对性的营销策略。</p><p>以下列举一个简单的关联规则的例子：</p><p>婴儿尿不湿→啤酒[支持度=10%， 置信度=70%]</p><p>这个规则表明，在所有顾客中，有10%的顾客同时购买了婴儿尿不湿和啤酒，而在所有购买了婴儿尿不湿的顾客中，占70%的人同时还购买了啤酒。发现这个关联规则后，超市零售商决定把婴儿尿不湿和啤酒摆放在一起进行促销，结果明显提升了销售额，这就是发生在沃尔玛超市中“啤酒和尿不湿”的经典营销案例。</p><p>上面的案例是否让你对支持度和置信度有了一定的了解？事实上，支持度（Support）和置信度（Confidence）是衡量关联规则强度的两个重要指标，它们分别反映着所发现规则的有用性和确定性。其中支持度：规则X→Y的支持度是指事物全集中包含X∪Y的事物百分比。支持度主要衡量规则的有用性，如果支持度太小，则说明相应规则只是偶发事件。在商业实战中，偶发事件很可能没有商业价值；置信度：规则X→Y的置信度是指既包含了X又包含了Y的事物数量占所有包含了X的事物数量的百分比。置信度主要衡量规则的确定性（可预测性），如果置信度太低，那么从X就很难可靠地推断出Y来，置信度太低的规则在实践应用中也没有太大用处。</p><p>在众多的关联规则数据挖掘算法中，最著名的就是Apriori算法，该算法具体分为以下两步进行：</p><ol><li>生成所有的频繁项目集。一个频繁项目集（Frequent Itemset）是一个支持度高于最小支持度阀值（min-sup）的项目集。</li><li>从频繁项目集中生成所有的可信关联规则。这里可信关联规则是指置信度大于最小置信度阀值（min-conf）的规则。</li></ol><p>关联规则算法不但在数值型数据集的分析中有很大用途，而且在纯文本文档和网页文件中，也有着重要用途。比如发现单词间的并发关系以及Web的使用模式等，这些都是Web数据挖掘、搜索及推荐的基础。</p><h4 id="2-2-5-聚类"><a href="#2-2-5-聚类" class="headerlink" title="2.2.5 聚类"></a>2.2.5 聚类</h4><p>聚类（Clustering）分析有一个通俗的解释和比喻，那就是“物以类聚，人以群分”。针对几个特定的业务指标，可以将观察对象的群体按照相似性和相异性进行不同群组的划分。经过划分后，每个群组内部各对象间的相似度会很高，而在不同群组之间的对象彼此间将具有很高的相异度。</p><p>聚类分析的算法可以分为划分的方法（Partitioning Method）、层次的方法（Hierarchical Method）、基于密度的方法（Density-based Method）、基于网格的方法（Grid-based Method）、基于模型的方法（Model-based Method）等，其中，前面两种方法最为常用。</p><p>对于划分的方法（Partitioning Method），当给定m个对象的数据集，以及希望生成的细分群体数量K后，即可采用这种方法将这些对象分成K组（K≤m），使得每个组内对象是相似的，而组间的对象是相异的。最常用的划分方法是K-Means方法，其具体原理是：首先，随机选择K个对象，并且所选择的每个对象都代表一个组的初始均值或初始的组中心值；对剩余的每个对象，根据其与各个组初始均值的距离，将它们分配给最近的（最相似）小组；然后，重新计算每个小组新的均值；这个过程不断重复，直到所有的对象在K组分布中都找到离自己最近的组。</p><p>层次的方法（Hierarchical Method）则是指依次让最相似的数据对象两两合并，这样不断地合并，最后就形成了一棵聚类树。</p><p>聚类技术在数据分析和数据化运营中的主要用途表现在：既可以直接作为模型对观察对象进行群体划分，为业务方的精细化运营提供具体的细分依据和相应的运营方案建议，又可在数据处理阶段用作数据探索的工具，包括发现离群点、孤立点，数据降维的手段和方法，通过聚类发现数据间的深层次的关系等。</p><h4 id="2-2-6-贝叶斯分类方法"><a href="#2-2-6-贝叶斯分类方法" class="headerlink" title="2.2.6 贝叶斯分类方法"></a>2.2.6 贝叶斯分类方法</h4><p>贝叶斯分类方法（Bayesian Classifier）是非常成熟的统计学分类方法，它主要用来预测类成员间关系的可能性。比如通过一个给定观察值的相关属性来判断其属于一个特定类别的概率。贝叶斯分类方法是基于贝叶斯定理的，已经有研究表明，朴素贝叶斯分类方法作为一种简单贝叶斯分类算法甚至可以跟决策树和神经网络算法相媲美。</p><p>贝叶斯定理的公式如下：<br><img src="https://img-blog.csdnimg.cn/20200723141132253.PNG#pic_center" alt="在这里插入图片描述"><br>其中，X表示n个属性的测量描述； H为某种假设，比如假设某观察值X属于某个特定的类别C；对于分类问题，希望确定P(H|X)，即能通过给定的X的测量描述，来得到H成立的概率，也就是给出X的属性值，计算出该观察值属于类别C的概率。因为P(H|X)是后验概率（Posterior Probability），所以又称其为在条件X下，H的后验概率。</p><p>贝叶斯定理是朴素贝叶斯分类法（Naive Bayesian Classifier）的基础，如果给定数据集里有M个分类类别，通过朴素贝叶斯分类法，可以预测给定观察值是否属于具有最高后验概率的特定类别，也就是说，朴素贝叶斯分类方法预测X属于类别Ci时，表示当且仅当<br><img src="https://img-blog.csdnimg.cn/20200723141458372.PNG#pic_center" alt="P(Ci | X)&gt;P(Cj | X)  1≤j≤m，ji"><br>此时如果最大化P(Ci|X)，其P(Ci|X)最大的类Ci被称为最大后验假设，根据贝叶斯定理<br><img src="https://img-blog.csdnimg.cn/20200723141701294.PNG#pic_center" alt="在这里插入图片描述"><br>可知，由于P(X)对于所有的类别是均等的，因此只需要P(X|Ci)P(Ci)取最大即可。</p><p>为了预测一个未知样本X的类别，可对每个类别Ci估算相应的P(X|Ci)P(Ci)。样本X归属于类别Ci，当且仅当<br><img src="https://img-blog.csdnimg.cn/20200723141803769.png#pic_center" alt="在这里插入图片描述"><br>贝叶斯分类方法在数据化运营实践中主要用于分类问题的归类等应用场景。</p><h4 id="2-2-7-支持向量机"><a href="#2-2-7-支持向量机" class="headerlink" title="2.2.7 支持向量机"></a>2.2.7 支持向量机</h4><p>支持向量机（Support Vector Machine）与传统的神经网络技术相比，支持向量机不仅结构简单，而且各项技术的性能也明显提升，因此它成为当今机器学习领域的热点之一。</p><p>作为一种新的分类方法，支持向量机以结构风险最小为原则。在线性的情况下，就在原空间寻找两类样本的最优分类超平面。在非线性的情况下，它使用一种非线性的映射，将原训练集数据映射到较高的维上。在新的维上，它搜索线性最佳分离超平面。使用一个适当的对足够高维的非线性映射，两类数据总可以被超平面分开。</p><p>支持向量机的基本概念如下：</p><p>设给定的训练样本集为{(x1, y1), (x2, y2), …, (xn, yn)}，其中xi ∈R<sup>n</sup>, y∈{-1,1}。再假设该训练集可被一个超平面线性划分，设该超平面记为(w, x)+b=0。<br><img src="https://img-blog.csdnimg.cn/20200723142736451.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>如图中圆形和方形代表两类样本，H为分类线，H1、H2，分别为过各类样本中离分类线最近的样本并且平行于分类线的直线，它们之间的距离叫做分类间隔（Margin）。所谓的最优分类线就是要求分类线不但能将两类正确分开（训练错误为0），而且能使分类间隔最大。推广到高维空间，最优分类线就成了最优分类面。</p><p>其中，距离超平面最近的一类向量被称为支持向量（Support Vector），一组支持向量可以唯一地确定一个超平面。通过学习算法，SVM可以自动寻找出那些对分类有较好区分能力的支持向量，由此构造出的分类器则可以最大化类与类的间隔，因而有较好的适应能力和较高的分类准确率。</p><p>支持向量机的缺点是训练数据较大，但是，它的优点也是很明显的—对于复杂的非线性的决策边界的建模能力高度准确，并且也不太容易过拟合。</p><p>支持向量机主要用在预测、分类这样的实际分析需求场景中。</p><h4 id="2-2-8-主成分分析"><a href="#2-2-8-主成分分析" class="headerlink" title="2.2.8 主成分分析"></a>2.2.8 主成分分析</h4><p>主成分分析（Principal Components Analysis）会通过线性组合将多个原始变量合并成若干个主成分，这样每个主成分都变成了原始变量的线性组合。这种转变的目的，一方面是可以大幅降低原始数据的维度，同时也在此过程中发现原始数据属性之间的关系。</p><p>主成分分析的主要步骤如下：</p><ol><li>通常要先进行各变量的标准化工作，标准化的目的是将数据按照比例进行缩放，使之落入一个小的区间范围之内，从而让不同的变量经过标准化处理后可以有平等的分析和比较基础。</li><li>选择协方差阵或者相关阵计算特征根及对应的特征向量。</li><li>计算方差贡献率，并根据方差贡献率的阀值选取合适的主成分个数。</li><li>根据主成分载荷的大小对选择的主成分进行命名。</li><li>根据主成分载荷计算各个主成分的得分。</li></ol><p>将主成分进行推广和延伸即成为因子分析（Factor Analysis），因子分析在综合原始变量信息的基础上将会力图构筑若干个意义较为明确的公因子；也就是说，采用少数几个因子描述多个指标之间的联系，将比较密切的变量归为同一类中，每类变量即是一个因子。之所以称其为因子，是因为它们实际上是不可测量的，只能解释。</p><p>主成分分析是因子分析的一个特例，两者的区别和联系主要表现在以下方面：</p><ul><li>主成分分析会把主成分表示成各个原始变量的线性组合，而因子分析则把原始变量表示成各个因子的线性组合。这个区别最直观也最容易记住。</li><li>主成分分析的重点在于解释原始变量的总方差，而因子分析的重点在于解释原始变量的协方差。</li><li>在主成分分析中，有几个原始变量就有几个主成分，而在因子分析中，因子个数可以根据业务场景的需要人为指定，并且指定的因子数量不同，则分析结果也会有差异。</li><li>在主成分分析中，给定的协方差矩阵或者相关矩阵的特征值是唯一时，主成分也是唯一的，但是在因子分析中，因子不是唯一的，并且通过旋转可以得到不同的因子。</li></ul><p>主成分分析和因子分析在数据化运营实践中主要用于数据处理、降维、变量间关系的探索等方面，同时作为统计学里的基本而重要的分析工具和分析方法，它们在一些专题分析中也有着广泛的应用。</p><h4 id="2-2-9-假设检验"><a href="#2-2-9-假设检验" class="headerlink" title="2.2.9 假设检验"></a>2.2.9 假设检验</h4><p>假设检验（Hypothesis Test）的基本原理就是小概率事件原理，即观测小概率事件在假设成立的情况下是否发生。如果在一次试验中，小概率事件发生了，那么说明假设在一定的显著性水平下不可靠或者不成立；如果在一次试验中，小概率事件没有发生，那么也只能说明没有足够理由相信假设是错误的，但是也并不能说明假设是正确的，因为无法收集到所有的证据来证明假设是正确的。</p><p>假设检验的结论是在一定的显著性水平下得出的。因此，当采用此方法观测事件并下结论时，有可能会犯错，这些错误主要有两大类：</p><ul><li>第Ⅰ类错误：当原假设为真时，却否定它而犯的错误，即拒绝正确假设的错误，也叫弃真错误。犯第Ⅰ类错误的概率记为，通常也叫α错误，α=1-置信度。</li><li>第Ⅱ类错误：当原假设为假时，却肯定它而犯的错误，即接受错误假设的错误，也叫纳伪错误。犯第Ⅱ类错误的概率记为，通常也叫β错误。</li></ul><p>上述这两类错误在其他条件不变的情况下是相反的，即α增大时，β就减小；α减小时，β就增大。错误容易受数据分析人员的控制，因此在假设检验中，通常会先控制第Ⅰ类错误发生的概率，具体表现为：在做假设检验之前先指定一个的具体数值，通常取0.05，也可以取0.1或0.001。</p><p>在数据化运营的商业实践中，假设检验最常用的场景就是用于“运营效果的评估”上。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20200723150511532.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center&quot; alt=&quot;在这里插入图片描述&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="产品/运营" scheme="http://yoursite.com/categories/%E4%BA%A7%E5%93%81-%E8%BF%90%E8%90%A5/"/>
    
    
      <category term="数据挖掘" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
      <category term="数据化运营" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E5%8C%96%E8%BF%90%E8%90%A5/"/>
    
  </entry>
  
  <entry>
    <title>【Machine Learning】16 图片文字识别(Application Example_Photo OCR)</title>
    <link href="http://yoursite.com/2020/07/22/[Machine%20Learning]%2016%20%E5%9B%BE%E7%89%87%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB(Application%20Example_%20Photo%20OCR)/"/>
    <id>http://yoursite.com/2020/07/22/[Machine%20Learning]%2016%20%E5%9B%BE%E7%89%87%E6%96%87%E5%AD%97%E8%AF%86%E5%88%AB(Application%20Example_%20Photo%20OCR)/</id>
    <published>2020-07-22T07:47:08.000Z</published>
    <updated>2020-07-22T07:57:09.687Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://img-blog.csdnimg.cn/20200722154505432.PNG#pic_center" alt="在这里插入图片描述"></p><a id="more"></a><h2 id="16-Application-Example-Photo-OCR-图片文字识别"><a href="#16-Application-Example-Photo-OCR-图片文字识别" class="headerlink" title="16 Application Example: Photo OCR(图片文字识别)"></a>16 Application Example: Photo OCR(图片文字识别)</h2><h3 id="16-1-Problem-Description-and-Pipeline"><a href="#16-1-Problem-Description-and-Pipeline" class="headerlink" title="16.1 Problem Description and Pipeline"></a>16.1 Problem Description and Pipeline</h3><p>图像文字识别应用所作的事是，从一张给定的图片中识别文字。这比从一份扫描文档中识别文字要复杂的多。<br><img src="https://img-blog.csdnimg.cn/20200722145838803.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>为了完成这样的工作，需要采取如下步骤：<br>1.文字侦测（Text detection）—— 将图片上的文字与其他环境对象分离开来<br>2.字符切分（Character segmentation）—— 将文字分割成一个个单一的字符<br>3.字符分类（Character classification）—— 确定每一个字符是什么</p><p><img src="https://img-blog.csdnimg.cn/20200722145956698.PNG#pic_center" alt="在这里插入图片描述"></p><h3 id="16-2-Sliding-Windows"><a href="#16-2-Sliding-Windows" class="headerlink" title="16.2 Sliding Windows"></a>16.2 Sliding Windows</h3><p>滑动窗口是一项用来从图像中抽取对象的技术。假设需要在一张图片中识别行人，首先要做的是用许多固定尺寸的图片来训练一个能够准确识别行人的模型。然后用之前训练识别行人的模型时所采用的图片尺寸在要进行行人识别的图片上进行剪裁，接着将剪裁得到的切片交给模型，让模型判断是否为行人，再在图片上滑动剪裁区域重新进行剪裁，将新剪裁的切片也交给模型进行判断，如此循环直至将图片全部检测完。一旦完成后，按比例放大剪裁的区域，再以新的尺寸对图片进行剪裁，将新剪裁的切片按比例缩小至模型所采纳的尺寸，交给模型进行判断，如此循环。<br><img src="https://img-blog.csdnimg.cn/20200722151434591.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>滑动窗口技术也被用于文字识别，首先训练模型能够区分字符与非字符，然后，运用滑动窗口技术识别字符，一旦完成了字符的识别，将识别得出的区域进行一些扩展，然后将重叠的区域进行合并。接着以宽高比作为过滤条件，过滤掉高度比宽度更大的区域（认为单词的长度通常比高度要大）。下图中绿色的区域是经过这些步骤后被认为是文字的区域，而红色的区域是被忽略的。<br><img src="https://img-blog.csdnimg.cn/20200722151528509.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>以上便是文字侦测阶段。 下一步是训练一个模型来完成将文字分割成一个个字符的任务，需要的训练集由单个字符的图片和两个相连字符之间的图片来训练模型。<br><img src="https://img-blog.csdnimg.cn/20200722151642739.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>模型训练完后，我们仍然是使用滑动窗口技术来进行字符识别。</p><p>以上便是字符切分阶段。 最后一个阶段是字符分类阶段，利用神经网络、支持向量机或者逻辑回归算法训练一个分类器即可。</p><h3 id="16-3-Getting-Lots-of-Data-and-Artificial-Data"><a href="#16-3-Getting-Lots-of-Data-and-Artificial-Data" class="headerlink" title="16.3 Getting Lots of Data and Artificial Data"></a>16.3 Getting Lots of Data and Artificial Data</h3><p>如果模型是低方差的，那么获得更多的数据用于训练模型，是能够有更好的效果的。问题在于，怎样获得数据，数据不总是可以直接获得的，有可能需要人工地创造一些数据。</p><p>以文字识别应用为例，我们可以字体网站下载各种字体，然后利用这些不同的字体配上各种不同的随机背景图片创造出一些用于训练的实例，这让我们能够获得一个无限大的训练集。这是从零开始创造实例。另一种方法是，利用已有的数据，然后对其进行修改，例如将已有的字符图片进行一些扭曲、旋转、模糊处理。只要我们认为实际数据有可能和经过这样处理后的数据类似，我们便可以用这样的方法来创造大量的数据。</p><p>有关获得更多数据的几种方法：<br>1.人工数据合成<br>2.手动收集、标记数据<br>3.众包</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20200722154505432.PNG#pic_center&quot; alt=&quot;在这里插入图片描述&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>【Machine Learning】15 大规模机器学习(Large Scale Machine Learning)</title>
    <link href="http://yoursite.com/2020/07/22/[Machine%20Learning]%2015%20%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(Large%20Scale%20Machine%20Learning)/"/>
    <id>http://yoursite.com/2020/07/22/[Machine%20Learning]%2015%20%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0(Large%20Scale%20Machine%20Learning)/</id>
    <published>2020-07-22T06:54:03.000Z</published>
    <updated>2020-07-22T06:55:40.613Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://img-blog.csdnimg.cn/20200722144708307.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><a id="more"></a><h2 id="15-Large-Scale-Machine-Learning-大规模机器学习"><a href="#15-Large-Scale-Machine-Learning-大规模机器学习" class="headerlink" title="15 Large Scale Machine Learning(大规模机器学习)"></a>15 Large Scale Machine Learning(大规模机器学习)</h2><h3 id="15-1-Learning-With-Large-Dataset"><a href="#15-1-Learning-With-Large-Dataset" class="headerlink" title="15.1 Learning With Large Dataset"></a>15.1 Learning With Large Dataset</h3><p>如果有一个低方差的模型，增加数据集的规模可以帮助你获得更好的结果。那么，如果有 1亿条记录的训练集，该如何应对？</p><p>以线性回归模型为例，每一次梯度下降迭代，都需要计算训练集的误差的平方和，如果我们的学习算法需要有 20 次迭代，这便已经是非常大的计算代价。</p><p>首先应该做的事是去检查一个这么大规模的训练集是否真的必要，也许只用 1000个训练集也能获得较好的效果，可以通过绘制学习曲线来帮助判断。<br><img src="https://img-blog.csdnimg.cn/20200722133854323.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="15-2-Stochastic-Gradient-Descent-随机梯度下降法"><a href="#15-2-Stochastic-Gradient-Descent-随机梯度下降法" class="headerlink" title="15.2 Stochastic Gradient Descent(随机梯度下降法)"></a>15.2 Stochastic Gradient Descent(随机梯度下降法)</h3><p>如果我们一定需要一个大规模的训练集，则可以尝试使用随机梯度下降法（SGD）来代替批量梯度下降法。</p><p>在随机梯度下降法中，我们定义代价函数为一个单一训练实例的代价：<br><img src="https://img-blog.csdnimg.cn/20200722134203793.png#pic_center" alt="在这里插入图片描述"></p><p>随机梯度下降算法为：首先对训练集随机“洗牌”，然后：<br><img src="https://img-blog.csdnimg.cn/20200722134335136.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>随机梯度下降算法在每一次计算之后便更新参数 𝜃 ，而不需要首先将所有的训练集求和，在梯度下降算法还没有完成一次迭代时，随机梯度下降算法便已经走出了很远。但是这样的算法存在的问题是，不是每一步都是朝着”正确”的方向迈出的。因此算法虽然会逐渐走向全局最小值的位置，但是可能无法站到那个最小值的那一点，而是在最小值点附近徘徊。<br><img src="https://img-blog.csdnimg.cn/20200722134513813.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="15-3-Mini-Batch-Gradient-Descent-小批量梯度下降"><a href="#15-3-Mini-Batch-Gradient-Descent-小批量梯度下降" class="headerlink" title="15.3 Mini-Batch Gradient Descent(小批量梯度下降)"></a>15.3 Mini-Batch Gradient Descent(小批量梯度下降)</h3><p>小批量梯度下降算法是介于批量梯度下降算法和随机梯度下降算法之间的算法，每计算常数𝑏次训练实例，便更新一次参数 𝜃 。<br><img src="https://img-blog.csdnimg.cn/20200722135245864.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200722135950392.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>通常会令 𝑏 在 2-100 之间。这样做的好处在于，可以用向量化的方式来循环𝑏个训练实例，如果用的线性代数函数库比较好，能够支持平行处理，那么算法的总体表现将不受影响（与随机梯度下降相同）。</p><h3 id="15-4-Stochastic-Gradient-Descent-Convergence-随机梯度下降收敛"><a href="#15-4-Stochastic-Gradient-Descent-Convergence-随机梯度下降收敛" class="headerlink" title="15.4 Stochastic Gradient Descent Convergence(随机梯度下降收敛)"></a>15.4 Stochastic Gradient Descent Convergence(随机梯度下降收敛)</h3><p>在批量梯度下降中，可以令代价函数𝐽为迭代次数的函数，绘制图表，根据图表来判断梯度下降是否收敛。但是，在大规模的训练集的情况下，这是不现实的，因为计算代价太大了。</p><p>在随机梯度下降中，在每一次更新 𝜃 之前都计算一次代价，然后每𝑥次迭代后，求出这𝑥次对训练实例计算代价的平均值，然后绘制这些平均值与𝑥次迭代的次数之间的函数图表。<br><img src="https://img-blog.csdnimg.cn/20200722140830226.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>当在绘制这样的图表时，可能会得到一个颠簸不平但是不会明显减少的函数图像（如上面左下图中蓝线所示）。可以通过增加𝛼来使得函数更加平缓，也许便能看出下降的趋势了（如上面左下图中红线所示）；或者可能函数图表仍然是颠簸不平且不下降的（如洋红色线所示），那么我们的模型本身可能存在一些错误。</p><p>如果我们得到的曲线如上面右下方所示，不断地上升，那么我们可能会需要选择一个较小的学习率𝛼。</p><p>我们也可以令学习率随着迭代次数的增加而减小，例如令：<br><img src="https://img-blog.csdnimg.cn/20200722141613805.PNG#pic_center" alt="在这里插入图片描述"><br>随着我们不断地靠近全局最小值，通过减小学习率，迫使算法收敛而非在最小值附近徘徊。 但是通常我们不需要这样做便能有非常好的效果了，对𝛼进行调整所耗费的计算通常不值得。<br><img src="https://img-blog.csdnimg.cn/20200722141743805.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>总结下，近似地监测出随机梯度下降算法在最优化代价函数中的表现，这种方法不需要定时地扫描整个训练集，来算出整个样本集的代价函数，而是只需要每次对最后/前 1000 个，或者多少个样本，求一下平均值。应用这种方法，你既可以保证随机梯度下降法正在正常运转和收敛，也可以用它来调整学习速率𝛼的大小。</p><h3 id="15-5-Online-Learning"><a href="#15-5-Online-Learning" class="headerlink" title="15.5 Online Learning"></a>15.5 Online Learning</h3><p>在线学习机制让我们可以模型化问题。今天，许多大型网站或者许多大型网络公司，使用不同版本的在线学习机制算法，从大批的涌入又离开网站的用户身上进行学习。特别要提及的是，如果你有一个由连续的用户流引发的连续的数据流，进入你的网站，你能做的是使用一个在线学习机制，从数据流中学习用户的偏好，然后使用这些信息来优化一些关于网站的决策。在线学习算法指的是对数据流而非离线的静态数据集的学习。许多在线网站都有持续不断的用户流，对于每一个用户，网站希望能在不将数据存储到数据库中便顺利地进行算法学习。</p><p>假定你有一个提供运输服务的公司，用户们来向你询问把包裹从 A 地运到 B 地的服务，同时假定你有一个网站，让用户们可多次登陆，然后他们告诉你，他们想从哪里寄出包裹，以及包裹要寄到哪里去，然后你的网站开出运输包裹的的服务价格。比如，我会收取$50 来运输你的包裹，我会收取$20 之类的，然后根据你开给用户的这个价格，用户有时会接受这个运输服务，那么这就是个正样本，有时他们会走掉，然后他们拒绝购买你的运输服务，所以，让我们假定我们想要一个学习算法来帮助我们，优化我们想给用户开出的价格。</p><p>假使我们正在经营一家物流公司，每当一个用户询问从地点 A 至地点 B 的快递费用时，我们给用户一个报价，该用户可能选择接受（𝑦 = 1）或不接受（𝑦 = 0）。通过构建一个模型，来预测用户接受报价并使用我们物流服务的可能性。因此报价是一个特征，其他特征为距离，起始地点，目标地点以及特定的用户数据。模型的输出是:𝑝(𝑦 = 1)。</p><p>在线学习的算法与随机梯度下降算法有些类似，对单一的实例进行学习，而非对一个提前定义的训练集进行循环。<br><img src="https://img-blog.csdnimg.cn/20200722142804159.PNG" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200722142804162.png" alt="在这里插入图片描述"><br>一旦对一个数据的学习完成了，便可以丢弃该数据，不需要再存储它了。这种方式的好处在于，算法可以很好的适应用户的倾向性，同时可以针对用户的当前行为不断地更新模型以适应该用户。</p><p>实际上，每次交互事件并不只产生一个数据集，例如，我们一次给用户提供 3 个物流选项，用户选择 2 项，我们实际上可以获得 3 个新的训练实例，因此算法可以一次从 3 个实例中学习并更新模型。</p><p>这些问题中的任何一个都可以被归类到标准的，拥有一个固定的样本集的机器学习问题中。或许，你可以运行一个你自己的网站，尝试运行几天，然后保存一个数据集，一个固定的数据集，然后对其运行一个学习算法。但是这些是实际的问题，在这些问题里，你会看到大公司会获取如此多的数据，真的没有必要来保存一个固定的数据集，取而代之的是你可以使用一个在线学习算法来连续的学习，从这些用户不断产生的数据中来学习。这就是在线学习机制，然后就像我们所看到的，我们所使用的这个算法与随机梯度下降算法非常类似，唯一的区别的是，我们不会使用一个固定的数据集，会做的是获取一个用户样本，从那个样本中学习，然后丢弃那个样本并继续下去，而且如果你对某一种应用有一个连续的数据流，这样的算法可能会非常值得考虑。当然，在线学习的一个优点就是，如果你有一个变化的用户群，又或者你在尝试预测的事情，在缓慢变化，就像你的用户的品味在缓慢变化，这个在线学习算法，可以慢慢地调试你所学习到的假设，将其调节更新到最新的用户行为。</p><h3 id="15-6-Map-Reduce-and-Data-Parallelism-映射化简和数据并行"><a href="#15-6-Map-Reduce-and-Data-Parallelism-映射化简和数据并行" class="headerlink" title="15.6 Map Reduce and Data Parallelism(映射化简和数据并行)"></a>15.6 Map Reduce and Data Parallelism(映射化简和数据并行)</h3><p>映射化简和数据并行对于大规模机器学习问题而言是非常重要的概念。之前提到，如果用批量梯度下降算法来求解大规模数据集的最优解，需要对整个训练集进行循环，计算偏导数和代价，再求和，计算代价非常大。如果能够将数据集分配给不多台计算机，让每一台计算机处理数据集的一个子集，然后再将计所的结果汇总在求和。这样的方法叫做<label style="color:red">映射简化</label>。</p><p>具体而言，如果任何学习算法能够表达为，对训练集的函数的求和，那么便能将这个任务分配给多台计算机（或者同一台计算机的不同 CPU 核心），以达到加速处理的目的。<br><img src="https://img-blog.csdnimg.cn/20200722144329440.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>例如，有 400 个训练实例，则可以将批量梯度下降的求和任务分配给 4 台计算机进行处理：<br><img src="https://img-blog.csdnimg.cn/20200722144441674.PNG#pic_center" alt="在这里插入图片描述"><br>很多高级的线性代数函数库已经能够利用多核 CPU 的多个核心来并行地处理矩阵运算，这也是算法的向量化实现如此重要的缘故（比调用循环快）。<br><img src="https://img-blog.csdnimg.cn/20200722144753711.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20200722144708307.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center&quot; alt=&quot;在这里插入图片描述&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>【Machine Learning】14 推荐系统(Recommender Systems)</title>
    <link href="http://yoursite.com/2020/07/21/[Machine%20Learning]%2014%20%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F(Recommender%20Systems)/"/>
    <id>http://yoursite.com/2020/07/21/[Machine%20Learning]%2014%20%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F(Recommender%20Systems)/</id>
    <published>2020-07-21T08:23:44.000Z</published>
    <updated>2020-07-21T08:24:12.062Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://img-blog.csdnimg.cn/20200721162108275.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><a id="more"></a><h2 id="14-Recommender-Systems-推荐系统"><a href="#14-Recommender-Systems-推荐系统" class="headerlink" title="14 Recommender Systems(推荐系统)"></a>14 Recommender Systems(推荐系统)</h2><h3 id="14-1-Problem-Formulation"><a href="#14-1-Problem-Formulation" class="headerlink" title="14.1 Problem Formulation"></a>14.1 Problem Formulation</h3><p>举个例子：<br>假如我们是一个电影供应商，有 5 部电影和 4 个用户，我们要求用户为电影打分。<br><img src="https://img-blog.csdnimg.cn/20200721152241844.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>前三部电影是爱情片，后两部则是动作片，我们可以看出 Alice 和 Bob 似乎更倾向与爱情片，而 Carol 和 Dave 似乎更倾向与动作片。并且没有一个用户给所有的电影都打过分。</p><p>我们希望构建一个算法来预测他们每个人可能会给他们没看过的电影打多少分，并以此作为推荐的依据。</p><p>字符的含义：<br>𝑛<sub>𝑢</sub> 代表用户的数量<br>𝑛<sub>𝑚</sub> 代表电影的数量<br>𝑟(𝑖,𝑗) 如果用户 j 给电影 𝑖 评过分则 𝑟(𝑖,𝑗) = 1<br>𝑦<sup>(𝑖,𝑗)</sup> 代表用户 𝑗 给电影 i 的评分<br>𝑚<sub>𝑗</sub>代表用户 𝑗 评过分的电影的总数</p><h3 id="14-2-Content-Based-Recommendations"><a href="#14-2-Content-Based-Recommendations" class="headerlink" title="14.2 Content Based Recommendations"></a>14.2 Content Based Recommendations</h3><p>在一个基于内容的推荐系统算法中，我们假设对于希望推荐的东西有一些数据，这些数据是有关这些东西的特征。</p><p>在例子中，假设每部电影都有两个特征，如𝑥1代表电影的浪漫程度，𝑥2代表电影的动作程度。<br><img src="https://img-blog.csdnimg.cn/20200721153145145.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>则每部电影都有一个特征向量，如𝑥<sup>(1)</sup>是第一部电影的特征向量为[0.9 0]。</p><p>下面我们要基于这些特征来构建一个推荐系统算法。 假设我们采用线性回归模型，可以针对每一个用户都训练一个线性回归模型，如𝜃<sup>(1)</sup>是第一个用户的模型的参数。 于是，我们有：<br>𝜃<sup>(𝑗)</sup>用户 𝑗 的参数向量<br>𝑥<sup>(𝑖)</sup>电影 𝑖 的特征向量<br>对于用户 𝑗 和电影 𝑖，我们预测评分为：(𝜃<sup>(𝑗)</sup>)<sup>𝑇</sup>𝑥<sup>(𝑖)</sup></p><p>代价函数<br>针对用户 𝑗，该线性回归模型的代价为预测误差的平方和，加上正则化项：<br><img src="https://img-blog.csdnimg.cn/20200721153819982.PNG#pic_center" alt="在这里插入图片描述"><br>其中 𝑖: 𝑟(𝑖,𝑗)表示我们只计算那些用户 𝑗 评过分的电影。在一般的线性回归模型中，误差项和正则项应该都是乘以1/2𝑚，在这里我们将𝑚去掉。并且我们不对方差项𝜃0进行正则化处理。</p><p>上面的代价函数只是针对一个用户的，为了学习所有用户，我们将所有用户的代价函数求和：<br><img src="https://img-blog.csdnimg.cn/20200721153923665.PNG#pic_center" alt="在这里插入图片描述"><br>如果要用梯度下降法来求解最优解，计算代价函数的偏导数后得到梯度下降的更新公式为：<br><img src="https://img-blog.csdnimg.cn/20200721154007768.PNG#pic_center" alt="在这里插入图片描述"></p><h3 id="14-3-Collaborative-Filtering-协同过滤"><a href="#14-3-Collaborative-Filtering-协同过滤" class="headerlink" title="14.3 Collaborative Filtering(协同过滤)"></a>14.3 Collaborative Filtering(协同过滤)</h3><p>在之前的基于内容的推荐系统中，对于每一部电影，我们都掌握了可用的特征，使用这些特征训练出了每一个用户的参数。相反地，如果我们拥有用户的参数，可以学习得出电影的特征。<br><img src="https://img-blog.csdnimg.cn/20200721154300881.PNG#pic_center" alt="在这里插入图片描述"><br>但是如果我们既没有用户的参数，也没有电影的特征，这两种方法都不可行了。协同过滤算法可以同时学习这两者。</p><p>我们的优化目标便改为同时针对𝑥和𝜃进行。<br><img src="https://img-blog.csdnimg.cn/20200721154357151.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>对代价函数求偏导数的结果如下：<br><img src="https://img-blog.csdnimg.cn/20200721154439704.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>注：在协同过滤从算法中，我们通常不使用方差项，如果需要的话，算法会自动学得。</p><p>协同过滤算法使用步骤如下：</p><ol><li>初始 𝑥<sup>(1)</sup>, 𝑥<sup>(2)</sup>, . . . 𝑥<sup>(𝑛<sub>𝑚</sub>)</sup>, 𝜃<sup>(1)</sup>, 𝜃<sup>(2)</sup>, . . . , 𝜃<sup>(𝑛<sub>𝑢</sub>)</sup>为一些随机小值</li><li>使用梯度下降算法最小化代价函数</li><li>在训练完算法后，我们预测(𝜃<sup>(𝑗)</sup>)<sup>𝑇</sup>𝑥<sup>(𝑖)</sup>为用户 𝑗 给电影 𝑖 的评分</li></ol><p>通过这个学习过程获得的特征矩阵包含了有关电影的重要数据，这些数据不总是人能读懂的，但是我们可以用这些数据作为给用户推荐电影的依据。例如，如果一位用户正在观看电影 𝑥<sup>(𝑖)</sup>，我们可以寻找另一部电影𝑥<sup>(𝑗)</sup>，依据两部电影的特征向量之间的距离||𝑥<sup>(𝑖)</sup> − 𝑥<sup>(𝑗)</sup>||的大小。</p><p><strong>协同过滤算法</strong><br><img src="https://img-blog.csdnimg.cn/20200721155201366.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200721155202605.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="14-4-Vectorization-Low-Rank-Matrix-Factorization-向量化：低秩矩阵分解"><a href="#14-4-Vectorization-Low-Rank-Matrix-Factorization-向量化：低秩矩阵分解" class="headerlink" title="14.4 Vectorization_ Low Rank Matrix Factorization(向量化：低秩矩阵分解)"></a>14.4 Vectorization_ Low Rank Matrix Factorization(向量化：低秩矩阵分解)</h3><p>协同过滤算法的向量化实现。</p><p>举例子：<br>1.当给出一件产品时，你能否找到与之相关的其它产品。<br>2.一位用户最近看上一件产品，有没有其它相关的产品，你可以推荐给他。<br>要做的是：实现一种选择的方法，写出协同过滤算法的预测情况。</p><p>有五部电影，以及四位用户，那么 这个矩阵 𝑌 就是一个 5 行 4 列的矩阵，它将这些电影的用户评分数据都存在矩阵里：<br><img src="https://img-blog.csdnimg.cn/2020072116095750.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200721161314842.PNG#pic_center" alt="在这里插入图片描述"><br>找到相关影片：<br><img src="https://img-blog.csdnimg.cn/20200721161351278.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>总结一下，当用户在看某部电影 𝑖 的时候，如果你想找 5 部与电影非常相似的电影，为了能给用户推荐 5 部新电影，你需要做的是找出电影 𝑗，在这些不同的电影中与我们要找的电影 𝑖 的距离最小，这样你就能给你的用户推荐几部不同的电影了。</p><h3 id="14-5-Implementational-Detail-Mean-Normalization"><a href="#14-5-Implementational-Detail-Mean-Normalization" class="headerlink" title="14.5 Implementational Detail_ Mean Normalization"></a>14.5 Implementational Detail_ Mean Normalization</h3><p>下面的用户评分数据：<br><img src="https://img-blog.csdnimg.cn/2020072116095750.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>如果新增一个用户 Eve，并且 Eve 没有为任何电影评分，那么以什么为依据为 Eve 推荐电影呢？</p><p>首先需要对结果 𝑌矩阵进行均值归一化处理，将每一个用户对某一部电影的评分减去所有用户对该电影评分的平均值：<br><img src="https://img-blog.csdnimg.cn/20200721161803459.PNG#pic_center" alt="在这里插入图片描述"><br>然后利用这个新的 𝑌 矩阵来训练算法。 如果我们要用新训练出的算法来预测评分，则需要将平均值重新加回去，预测(𝜃<sup>(𝑗)</sup>)<sup>𝑇</sup>𝑥<sup>(𝑖)</sup>+ 𝜇<sub>𝑖</sub>，对于 Eve，新模型会认为她给每部电影的评分都是该电影的平均分。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20200721162108275.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center&quot; alt=&quot;在这里插入图片描述&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>【Machine Learning】13 异常检测(Anomaly Detection)</title>
    <link href="http://yoursite.com/2020/07/21/[Machine%20Learning]%2013%20%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B(Anomaly%20Detection)/"/>
    <id>http://yoursite.com/2020/07/21/[Machine%20Learning]%2013%20%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B(Anomaly%20Detection)/</id>
    <published>2020-07-21T06:52:02.000Z</published>
    <updated>2020-07-21T06:52:24.936Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://img-blog.csdnimg.cn/20200721144739196.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><a id="more"></a><h2 id="13-Anomaly-Detection-异常检测"><a href="#13-Anomaly-Detection-异常检测" class="headerlink" title="13 Anomaly Detection(异常检测)"></a>13 Anomaly Detection(异常检测)</h2><h3 id="13-1-Problem-Motivation"><a href="#13-1-Problem-Motivation" class="headerlink" title="13.1 Problem Motivation"></a>13.1 Problem Motivation</h3><p>异常检测(Anomaly detection)问题，是机器学习算法的一个常见应用。这种算法的一个有趣之处在于：它虽然主要用于非监督学习问题，但从某些角度看，它又类似于一些监督学习问题。</p><p>什么是异常检测呢？<br>举个例子：假设你是一个飞机引擎制造商，当你生产的飞机引擎从生产线上流出时，你需要进行QA(质量控制测试)，而作为这个测试的一部分，你测量了飞机引擎的一些特征变量，比如引擎运转时产生的热量，或者引擎的振动等等。<br><img src="https://img-blog.csdnimg.cn/20200721105251707.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>如果生产了𝑚个引擎的话，得到一个从𝑥<sup>(1)</sup>到𝑥<sup>(𝑚)</sup>的数据集，然后将这些数据绘制成图表，看起来就是上图这样：每个叉都是无标签数据。这样，异常检测问题可以定义如下：我们假设后来有一天，你有一个新的飞机引擎从生产线上流出，而你的新飞机引擎有特征变量𝑥<sub>𝑡𝑒𝑠𝑡</sub>。所谓的异常检测问题就是：我们希望知道这个新的飞机引擎是否有某种异常，或者说，我们希望判断这个引擎是否需要进一步测试。因为，如果它看起来像一个正常的引擎，那么我们可以直接将它运送到客户那里，而不需要进一步的测试。</p><p>给定数据集 𝑥<sup>(1)</sup>, 𝑥<sup>(2)</sup>, . . , 𝑥<sup>(𝑚)</sup>，假设数据集是正常的，我们希望知道新的数据 𝑥<sub>𝑡𝑒𝑠𝑡</sub>是不是异常的，即这个测试数据不属于该组数据的几率如何。我们所构建的模型应该能根据该测试数据的位置告诉我们其属于一组数据的可能性 𝑝(𝑥)。<br><img src="https://img-blog.csdnimg.cn/20200721110500301.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>上图中，在蓝色圈内的数据属于该组数据的可能性较高，而越是偏远的数据，其属于该组数据的可能性就越低。</p><p>这种方法称为密度估计，表达如下：<br><img src="https://img-blog.csdnimg.cn/20200721110613636.PNG#pic_center" alt="在这里插入图片描述"><br>欺诈检测：𝑥<sup>(𝑖)</sup> = 用户的第 𝑖个活动特征<br>模型𝑝(𝑥) 为我们其属于一组数据的可能性，通过𝑝(𝑥) &lt; 𝜀检测非正常用户。</p><p>异常检测主要用来识别欺骗。例如在线采集而来的有关用户的数据，一个特征向量中可能会包含如：用户多久登录一次，访问过的页面，在论坛发布的帖子数量，甚至是打字速度等。尝试根据这些特征构建一个模型，可以用这个模型来识别那些不符合该模式的用户。再一个例子是检测一个数据中心，特征可能包含：内存使用情况，被访问的磁盘数量，CPU 的负载，网络的通信量等。根据这些特征可以构建一个模型，用来判断某些计算机是不是有可能出错了。</p><h3 id="13-2-Gaussian-Distribution-高斯分布"><a href="#13-2-Gaussian-Distribution-高斯分布" class="headerlink" title="13.2 Gaussian Distribution(高斯分布)"></a>13.2 Gaussian Distribution(高斯分布)</h3><p>高斯分布，也称为正态分布。<br><img src="https://img-blog.csdnimg.cn/20200721111023447.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200721111103835.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>注：机器学习中对于方差通常只除以𝑚而非统计学中的(𝑚 − 1)。这里顺便提一下，在实际使用中，到底是选择使用1/𝑚还是1/(𝑚 − 1)其实区别很小，只要你有一个还算大的训练集，在机器学习领域大部分人更习惯使用1/𝑚这个版本的公式。这两个版本的公式在理论特性和数学特性上稍有不同，但是在实际使用中，他们的区别甚小，几乎可以忽略不计。</p><h3 id="13-3-Algorithm"><a href="#13-3-Algorithm" class="headerlink" title="13.3 Algorithm"></a>13.3 Algorithm</h3><p>异常检测算法：<br>对于给定的数据集 𝑥<sup>(1)</sup>, 𝑥<sup>(2)</sup>, . . . , 𝑥<sup>(𝑚)</sup>，针对每一个特征计算 𝜇 和 𝜎<sup>2</sup> 的估计值。<br><img src="https://img-blog.csdnimg.cn/20200721124039696.PNG#pic_center" alt="在这里插入图片描述"><br>一旦获得了平均值和方差的估计值，给定新的一个训练实例，根据模型计算𝑝(𝑥)：<br><img src="https://img-blog.csdnimg.cn/20200721124146373.PNG#pic_center" alt="在这里插入图片描述"></p><p>当𝑝(𝑥) &lt; 𝜀时，为异常。</p><p>下图是一个由两个特征的训练集，以及特征的分布情况：<br><img src="https://img-blog.csdnimg.cn/20200721124345468.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>下面的三维图表表示的是密度估计函数，𝑧轴为根据两个特征的值所估计𝑝(𝑥)值：<br><img src="https://img-blog.csdnimg.cn/20200721124528244.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>选择一个𝜀，将𝑝(𝑥) = 𝜀作为我们的判定边界，当𝑝(𝑥) &gt; 𝜀时预测数据为正常数据，否则为异常。</p><h3 id="13-4-Developing-and-Evaluating-an-Anomaly-Detection-System"><a href="#13-4-Developing-and-Evaluating-an-Anomaly-Detection-System" class="headerlink" title="13.4 Developing and Evaluating an Anomaly Detection System"></a>13.4 Developing and Evaluating an Anomaly Detection System</h3><p>异常检测算法是一个非监督学习算法，意味着无法根据结果变量 𝑦 的值来告诉数据是否真的是异常的。我们需要另一种方法来帮助检验算法是否有效。当开发一个异常检测系统时，从带标记（异常或正常）的数据着手，选择其中一部分正常数据用于构建训练集，然后用剩下的正常数据和异常数据混合的数据构成交叉检验集和测试集。</p><p>例如：有 10000 台正常引擎的数据，有 20 台异常引擎的数据。 可以这样分配数<br>据：</p><ul><li>6000 台正常引擎的数据作为训练集</li><li>2000 台正常引擎和 10 台异常引擎的数据作为交叉检验集</li><li>2000 台正常引擎和 10 台异常引擎的数据作为测试集</li></ul><p>具体的评价方法如下：</p><ol><li>根据测试集数据，估计特征的平均值和方差并构建𝑝(𝑥)函数</li><li>对交叉检验集，尝试使用不同的𝜀值作为阀值，并预测数据是否异常，根据 F1 值或者查准率与查全率的比例来选择 𝜀</li><li>选出 𝜀 后，针对测试集进行预测，计算异常检验系统的𝐹1值，或者查准率与查全率之比。</li></ol><h3 id="13-5-Anomaly-Detection-vs-Supervised-Learning"><a href="#13-5-Anomaly-Detection-vs-Supervised-Learning" class="headerlink" title="13.5 Anomaly Detection vs. Supervised Learning"></a>13.5 Anomaly Detection vs. Supervised Learning</h3><table><thead><tr><th>异常检测</th><th>监督学习</th></tr></thead><tbody><tr><td>非常少量的正向类（异常数据 𝑦 = 1）, 大量的负向类（𝑦 = 0）</td><td>同时有大量的正向类和负向类</td></tr><tr><td>许多不同种类的异常，非常难。根据非常少量的正向类数据来训练算法。</td><td>有足够多的正向类实例，足够用于训练算法，未来遇到的正向类实例可能与训练集中的非常近似。</td></tr><tr><td>未来遇到的异常可能与已掌握的异常、非常的不同。</td><td></td></tr><tr><td>例如：欺诈行为检测、生产（例如飞机引擎）、检测数据中心的计算机运行状况</td><td>例如：邮件过滤器、天气预报、肿瘤分类</td></tr></tbody></table><p>另外，对于很多技术公司可能会遇到的一些问题，通常来说，正样本的数量很少，甚至有时候是 0，也就是说，出现了太多没见过的不同的异常类型，那么对于这些问题，通常应该使用的算法就是异常检测算法。</p><h3 id="13-5-Choosing-What-Features-to-Use"><a href="#13-5-Choosing-What-Features-to-Use" class="headerlink" title="13.5 Choosing What Features to Use"></a>13.5 Choosing What Features to Use</h3><p>对于异常检测算法，使用的特征是至关重要的，下面谈谈如何选择特征：</p><p>异常检测假设特征符合高斯分布，如果数据的分布不是高斯分布，异常检测算法也能够工作，但是最好还是将数据转换成高斯分布，例如使用对数函数：𝑥 = 𝑙𝑜𝑔(𝑥 + 𝑐)，其中 𝑐为非负常数； 或者 𝑥 = 𝑥<sup>𝑐</sup>，𝑐为 0-1 之间的一个分数，等方法。<br><img src="https://img-blog.csdnimg.cn/20200721133502124.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>注：在 python 中，通常用 np.log1p()函数，𝑙𝑜𝑔1𝑝就是 𝑙𝑜𝑔(𝑥 + 1)，可以避免<br>出现负数结果，反向函数就是 np.expm1()</p><p>误差分析：<br>一个常见的问题是一些异常的数据可能也会有较高的𝑝(𝑥)值，因而被算法认为是正常的。这种情况下误差分析能够帮助我们，我们可以分析那些被算法错误预测为正常的数据，观察能否找出一些问题。我们可能能从问题中发现我们需要增加一些新的特征，增加这些新特征后获得的新算法能够帮助我们更好地进行异常检测。</p><p>异常检测误差分析：<br><img src="https://img-blog.csdnimg.cn/20200721133550665.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>通常可以通过将一些相关的特征进行组合，来获得一些新的更好的特征（异常数据的该特征值异常地大或小），例如，在检测数据中心的计算机状况的例子中，我们可以用 CPU负载与网络通信量的比例作为一个新的特征，如果该值异常地大，便有可能意味着该服务器是陷入了一些问题中。</p><h3 id="13-7-Multivariate-Gaussian-Distribution"><a href="#13-7-Multivariate-Gaussian-Distribution" class="headerlink" title="13.7 Multivariate Gaussian Distribution"></a>13.7 Multivariate Gaussian Distribution</h3><p>假使我们有两个相关的特征，而且这两个特征的值域范围比较宽，这种情况下，一般的高斯分布模型可能不能很好地识别异常数据。其原因在于，一般的高斯分布模型尝试的是去同时抓住两个特征的偏差，因此创造出一个比较大的判定边界。</p><p>下图中是两个相关特征，洋红色的线（根据 ε 的不同其范围可大可小）是一般的高斯分布模型获得的判定边界，很明显绿色的 X 所代表的数据点很可能是异常值，但是其𝑝(𝑥)值却仍然在正常范围内。多元高斯分布将创建像图中蓝色曲线所示的判定边界。<br><img src="https://img-blog.csdnimg.cn/20200721135545231.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>在一般的高斯分布模型中，计算 𝑝(𝑥) 的方法是： 通过分别计算每个特征对应的几率然后将其累乘起来，在多元高斯分布模型中，我们将构建特征的协方差矩阵，用所有的特征一起来计算 𝑝(𝑥)。</p><p>首先计算所有特征的平均值，然后再计算协方差矩阵：<br><img src="https://img-blog.csdnimg.cn/20200721135639131.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>注:其中𝜇 是一个向量，其每一个单元都是原特征矩阵中一行数据的均值。最后我们计算多元高斯分布的𝑝(𝑥): <img src="https://img-blog.csdnimg.cn/20200721135725964.PNG#pic_center" alt="在这里插入图片描述"><br>其中：<br>|𝛴|是定矩阵，在 Octave 中用 det(sigma)计算</p><p>𝛴1 是逆矩阵，那么协方差矩阵是如何影响模型的？<br><img src="https://img-blog.csdnimg.cn/20200721135904511.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>上图是 5 个不同的模型，从左往右依次分析：</p><ol><li>是一个一般的高斯分布模型</li><li>通过协方差矩阵，令特征 1 拥有较小的偏差，同时保持特征 2 的偏差</li><li>通过协方差矩阵，令特征 2 拥有较大的偏差，同时保持特征 1 的偏差</li><li>通过协方差矩阵，在不改变两个特征的原有偏差的基础上，增加两者之间的正相关性</li><li>通过协方差矩阵，在不改变两个特征的原有偏差的基础上，增加两者之间的负相关性</li></ol><p>多元高斯分布模型与原高斯分布模型的关系：<br>可以证明的是，原本的高斯分布模型是多元高斯分布模型的一个子集，即像上图中的第1、2、3，3 个例子所示，如果协方差矩阵只在对角线的单位上有非零的值时，即为原本的高斯分布模型了。</p><p>原高斯分布模型和多元高斯分布模型的比较：</p><table><thead><tr><th>原高斯分布模型</th><th>多元高斯分布模型</th></tr></thead><tbody><tr><td>不能捕捉特征之间的相关性，但可以通过将特征进行组合的方法来解决</td><td>自动捕捉特征之间的相关性</td></tr><tr><td>计算代价低，能适应大规模的特征</td><td>计算代价较高，训练集较小时也同样适用</td></tr><tr><td></td><td>必须要有 𝑚 &gt; 𝑛，不然的话协方差矩阵，不可逆的，通常需要 𝑚 &gt; 10𝑛 另外特征冗余也会导致协方差矩阵不可逆</td></tr></tbody></table><p>原高斯分布模型被广泛使用着，如果特征之间在某种程度上存在相互关联的情况，我们可以通过构造新新特征的方法来捕捉这些相关性。</p><p>如果训练集不是太大，并且没有太多的特征，我们可以使用多元高斯分布模型。</p><h3 id="13-8-Anomaly-Detection-Using-the-Multivariate-Gaussian-Distribution"><a href="#13-8-Anomaly-Detection-Using-the-Multivariate-Gaussian-Distribution" class="headerlink" title="13.8 Anomaly Detection Using the Multivariate Gaussian Distribution"></a>13.8 Anomaly Detection Using the Multivariate Gaussian Distribution</h3><p>多元高斯分布和多元正态分布：<br><img src="https://img-blog.csdnimg.cn/20200721140737892.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>分布有两个参数， 𝜇 和 𝛴。其中𝜇是𝑛维向量和 𝛴 是协方差矩阵(𝑛 × 𝑛的矩阵)。而p(x)，通过调整 𝜇 和𝛴，可以得到一个范围不同的分布。</p><p>参数拟合/参数估计问题<br>有一组样本𝑥<sup>(1)</sup>, 𝑥<sup>(2)</sup>, . . . , 𝑥<sup>(𝑚)</sup>都是一个𝑛维向量，并且样本服从多元高斯分布。那么，如何尝试估计参数 𝜇 和 𝛴 ？</p><p>对于估计参数有一个标准公式<br>假设 𝜇 是你的训练样本的平均值：<br><img src="https://img-blog.csdnimg.cn/20200721141750265.png#pic_center" alt="在这里插入图片描述"><br>并设置𝛴：<img src="https://img-blog.csdnimg.cn/20200721141749912.png#pic_center" alt="在这里插入图片描述"></p><p>这和使用 PCA 算法时，写 𝛴 是一样的。所以只需将上述两个公式插入到p(x)中，就能估计参数 𝜇 和𝛴，并将其代入到异常检测算法。那么，如何把这些综合起来开发一个异常检测算法？</p><p>首先，用训练集来拟合模型，通过设定 𝜇 和𝛴来计算𝑝(𝑥)。<br>接下来，当有一个新样本x，即测试样本，用该样本计算p(x)，若p(x)很小就标记为异常。<br><img src="https://img-blog.csdnimg.cn/20200721143321455.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>如上图，该分布在中央最多，越到外面圈的范围越小，并且绿色这个点的概率非常低，即能够检测出绿色点为一个异常。</p><p>原始模型与多元高斯模型的关系如图：<br><img src="https://img-blog.csdnimg.cn/20200721143806670.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>原始模型和多元高斯分布比较如图：<br><img src="https://img-blog.csdnimg.cn/20200721144058852.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20200721144739196.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center&quot; alt=&quot;在这里插入图片描述&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>【Machine Learning】12 降维(Dimensionality Reduction)</title>
    <link href="http://yoursite.com/2020/07/20/[Machine%20Learning]%2012%20%E9%99%8D%E7%BB%B4(Dimensionality%20Reduction)/"/>
    <id>http://yoursite.com/2020/07/20/[Machine%20Learning]%2012%20%E9%99%8D%E7%BB%B4(Dimensionality%20Reduction)/</id>
    <published>2020-07-20T07:21:49.000Z</published>
    <updated>2020-07-20T07:22:25.476Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://img-blog.csdnimg.cn/20200720151913225.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><a id="more"></a><h2 id="12-Dimensionality-Reduction-降维"><a href="#12-Dimensionality-Reduction-降维" class="headerlink" title="12 Dimensionality Reduction(降维)"></a>12 Dimensionality Reduction(降维)</h2><h3 id="12-1-Motivation-I-Data-Compression"><a href="#12-1-Motivation-I-Data-Compression" class="headerlink" title="12.1 Motivation I_ Data Compression"></a>12.1 Motivation I_ Data Compression</h3><p>第二种无监督学习问题，称为降维。使用降维可以实现数据压缩，数据压缩不仅可以压缩数据，因而使用较少的计算机内存或磁盘空间，但它也加快了学习算法。</p><p>那么，什么是降维？举一个例子，有一个数据集，其含有许多特征。<br><img src="https://img-blog.csdnimg.cn/20200720133240451.PNG#pic_center" alt="在这里插入图片描述"><br>假设有两个未知的特征：𝑥1:长度：用厘米表示；𝑥2：是用英寸表示同一物体的长度。所以，这给了我们高度冗余表示，也许这两个特征𝑥1和𝑥2，能减少数据到一维。</p><p>将数据从二维降至一维： 假使要采用两种不同的仪器来测量一些东西的尺寸，其中一个仪器测量结果的单位是英寸，另一个仪器测量的结果是厘米，我们希望将测量的结果作为我们机器学习的特征。现在的问题的是，两种仪器对同一个东西测量的结果不完全相等（由于误差、精度等），而将两者都作为特征有些重复，因而，我们希望将这个二维的数据降至一维。<br><img src="https://img-blog.csdnimg.cn/20200720133953685.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>将数据从三维降至二维： 这个例子中我们要将一个三维的特征向量降至一个二维的特征向量。过程是与上面类似的，将三维向量投射到一个二维的平面上，强迫使得所有的数据都在同一个平面上，降至二维的特征向量。<br><img src="https://img-blog.csdnimg.cn/20200720134132906.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>这样的处理过程可以被用于把任何维度的数据降到任何想要的维度，例如将 1000 维的特征降至 100 维。</p><h3 id="12-2-Motivation-II-Visualization"><a href="#12-2-Motivation-II-Visualization" class="headerlink" title="12.2 Motivation II_ Visualization"></a>12.2 Motivation II_ Visualization</h3><p>在许多机器学习问题中，如果能将数据可视化，便能寻找到一个更好的解决方案，其中降维就有助于实现这一目的。<br><img src="https://img-blog.csdnimg.cn/20200720134505327.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>例如，一个数据集包含有许多不同国家的数据，每一个特征向量都有 50 个特征（如 GDP，人均 GDP，平均寿命等）。如果要将这个 50 维的数据可视化是不可能的。使用降维的方法将其降至 2 维，便可以将其可视化了。<br><img src="https://img-blog.csdnimg.cn/20200720134719309.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>这样做的问题在于，降维的算法只负责减少维数，新产生的特征的意义就必须由我们自己去发现了。</p><h3 id="12-3-Principal-Component-Analysis-Problem-Formulation"><a href="#12-3-Principal-Component-Analysis-Problem-Formulation" class="headerlink" title="12.3 Principal Component Analysis Problem Formulation"></a>12.3 Principal Component Analysis Problem Formulation</h3><p>主成分分析(PCA)是最常见的降维算法。</p><p>在 PCA 中，要做的是找到一个方向向量（Vector direction），当把所有的数据都投射到该向量上时，希望投射平均均方误差能尽可能地小。方向向量是一个经过原点的向量，而投射误差是从特征向量向该方向向量作垂线的长度。<br><img src="https://img-blog.csdnimg.cn/20200720142621705.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>主成分分析问题的描述：问题是要将𝑛维数据降至𝑘维，目标是找到向量𝑢<sup>(1)</sup>,𝑢<sup>(2)</sup>,…,𝑢<sup>(𝑘)</sup>使得总的投射误差最小。</p><p><strong>主成分分析与线性回归的比较</strong><br>主成分分析与线性回归是两种不同的算法。主成分分析最小化的是投射误差（Projected Error），而线性回归尝试的是最小化预测误差。线性回归的目的是预测结果，而主成分分析不作任何预测。<br><img src="https://img-blog.csdnimg.cn/20200720142903671.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>上图中，左边的是线性回归的误差（垂直于横轴投影），右边则是主要成分分析的误差（垂直于红线投影）。</p><p>PCA 将𝑛个特征降维到𝑘个，可以用来进行数据压缩，如果 100 维的向量最后可以用 10 维来表示，那么压缩率为 90%。同样图像处理领域的 KL 变换使用 PCA 做图像压缩。但 PCA 要保证降维后，还要保证数据的特性损失最小。</p><p>PCA 技术的一大好处是对数据进行降维的处理。可以对新求出的“主元”向量的重要性进行排序，根据需要取前面最重要的部分，将后面的维数省去，可以达到降维从而简化模型或是对数据进行压缩的效果。同时最大程度的保持了原有数据的信息。</p><p>PCA 技术的一个很大的优点是，它是完全无参数限制的。在 PCA 的计算过程中完全不需要人为的设定参数或是根据任何经验模型对计算进行干预，最后的结果只与数据相关，与用户是独立的。但是，这一点同时也可以看作是缺点。如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果，效率也不高。</p><h3 id="12-4-Principal-Component-Analysis-Algorithm"><a href="#12-4-Principal-Component-Analysis-Algorithm" class="headerlink" title="12.4 Principal Component Analysis Algorithm"></a>12.4 Principal Component Analysis Algorithm</h3><p><img src="https://img-blog.csdnimg.cn/202007201433127.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>PCA 减少𝑛维到𝑘维：<br>第一步是均值归一化。需要计算出所有特征的均值，然后令 𝑥𝑗 = 𝑥𝑗 − 𝜇𝑗。如果特征是在不同的数量级上，还需要将其除以标准差 𝜎<sup>2</sup>。<br>第二步是计算协方差矩阵（covariance matrix）𝛴： <img src="https://img-blog.csdnimg.cn/2020072014384614.png#pic_center" alt="在这里插入图片描述"></p><p>第三步是计算协方差矩阵𝛴的特征向量（eigenvectors）:<br>在 Octave 里我们可以利用奇异值分解（singular value decomposition）来求解，[U, S,V]= svd(sigma)。</p><p><img src="https://img-blog.csdnimg.cn/20200720144008580.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>对于一个 𝑛 × 𝑛维度的矩阵，上式中的𝑈是一个具有与数据之间最小投射误差的方向向量构成的矩阵。如果我们希望将数据从𝑛维降至𝑘维，只需要从𝑈中选取前𝑘个向量，获得一个𝑛 × 𝑘维度的矩阵，我们用𝑈𝑟𝑒𝑑𝑢𝑐𝑒表示，然后通过如下计算获得要求的新特征向量𝑧<sup>(𝑖)</sup>:<img src="https://img-blog.csdnimg.cn/2020072014414560.PNG#pic_center" alt="在这里插入图片描述"><br>其中𝑥是𝑛 × 1维的，因此结果为𝑘 × 1维度。注，不对方差特征进行处理。</p><h3 id="12-5-Choosing-The-Number-Of-Principal-Components"><a href="#12-5-Choosing-The-Number-Of-Principal-Components" class="headerlink" title="12.5 Choosing The Number Of Principal Components"></a>12.5 Choosing The Number Of Principal Components</h3><p>主要成分分析是减少投射的平均均方误差。</p><p>训练集的方差为：<img src="https://img-blog.csdnimg.cn/20200720145454169.PNG#pic_center" alt="在这里插入图片描述"><br>我们希望在平均均方误差与训练集方差的比例尽可能小的情况下选择尽可能小的𝑘值。如果我们希望这个比例小于 1%，就意味着原本数据的偏差有 99%都保留下来了，如果我们选择保留 95%的偏差，便能非常显著地降低模型中特征的维度了。<br><img src="https://img-blog.csdnimg.cn/20200720145626928.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>我们可以先令𝑘 = 1，然后进行主要成分分析，获得𝑈𝑟𝑒𝑑𝑢𝑐𝑒和𝑧，然后计算比例是否小于1%。如果不是的话再令𝑘 = 2，如此类推，直到找到可以使得比例小于 1%的最小𝑘 值（原因是各个特征之间通常情况存在某种相关性）。</p><p>还有一些更好的方式来选择𝑘，当我们在 Octave 中调用“svd”函数的时候，我们获得三个参数：[U, S, V] = svd(sigma)。<br><img src="https://img-blog.csdnimg.cn/20200720145716263.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>其中的𝑆是一个𝑛 × 𝑛的矩阵，只有对角线上有值，而其它单元都是 0，我们可以使用这个矩阵来计算平均均方误差与训练集方差的比例：<br><img src="https://img-blog.csdnimg.cn/20200720145822799.PNG#pic_center" alt="在这里插入图片描述"><br>也就是：<img src="https://img-blog.csdnimg.cn/20200720145856156.png#pic_center" alt="在这里插入图片描述"><br>在压缩过数据后，我们可以采用如下方法来近似地获得原有的特征：<br><img src="https://img-blog.csdnimg.cn/20200720145934745.PNG#pic_center" alt="在这里插入图片描述"></p><h3 id="12-6-Reconstruction-from-Compressed-Representation"><a href="#12-6-Reconstruction-from-Compressed-Representation" class="headerlink" title="12.6 Reconstruction from Compressed Representation"></a>12.6 Reconstruction from Compressed Representation</h3><p>PCA 作为压缩算法，可能把 1000 维的数据压缩 100 维特征，或具有三维数据压缩到一二维表示。所以，如果这是一个压缩算法，那么如何回到原有的高维数据的表示？<br><img src="https://img-blog.csdnimg.cn/20200720150332378.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>如图中样本𝑥<sup>(1)</sup>,𝑥<sup>(2)</sup>，这些样本在图中这个二维平面上，然后只使用一个实数，比如𝑧<sup>(1)</sup>，指定这些点的位置后他们被投射到这一个一维曲线上。那么，给定一个点𝑧<sup>(1)</sup>，如何能把这些点还原为原始的二维空间呢？𝑥为 2 维，z 为 1 维，𝑧 = 𝑈<sub>𝑟𝑒𝑑𝑢𝑐𝑒</sub><sup>𝑇</sup> 𝑥，相反的方程为：<br><img src="https://img-blog.csdnimg.cn/20200720150949625.PNG#pic_center" alt="𝑥𝑎𝑝𝑝𝑜𝑥 = 𝑈𝑟𝑒𝑑𝑢𝑐𝑒 ⋅ 𝑧,𝑥𝑎𝑝𝑝𝑜𝑥 ≈ 𝑥。"><br>所以，这就是从低维表示𝑧回到未压缩的表示，得到了数据之前的原始数据 𝑥，也把这个过程称为重建原始数据。</p><h3 id="12-7-Advice-for-Applying-PCA"><a href="#12-7-Advice-for-Applying-PCA" class="headerlink" title="12.7 Advice for Applying PCA"></a>12.7 Advice for Applying PCA</h3><p>假使正在针对一张 100×100 像素的图片进行某个计算机视觉的机器学习，即总共有 10000 个特征。</p><ol><li>第一步是运用主要成分分析将数据压缩至 1000 个特征</li><li>然后对训练集运行学习算法。</li><li>在预测时，采用之前学习而来的𝑈𝑟𝑒𝑑𝑢𝑐𝑒将输入的特征𝑥转换成特征向量𝑧，然后再进行预测</li></ol><p>注：如果我们有交叉验证集合测试集，也采用对训练集学习而来的𝑈𝑟𝑒𝑑𝑢𝑐𝑒。</p><p>错误的主要成分分析情况：一个常见错误使用主要成分分析的情况是，将其用于减少过拟合（减少了特征的数量）。这样做非常不好，不如尝试正则化处理。原因在于主要成分分析只是近似地丢弃掉一些特征，它并不考虑任何与结果变量有关的信息，因此可能会丢失非常重要的特征。然而当我们进行正则化处理时，会考虑到结果变量，不会丢掉重要的数据。</p><p>另一个常见的错误是，默认地将主要成分分析作为学习过程中的一部分，这虽然很多时候有效果，但最好还是从所有原始特征开始，只在有必要的时候（算法运行太慢或者占用太多内存）才考虑采用主要成分分析。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20200720151913225.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center&quot; alt=&quot;在这里插入图片描述&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
      <category term="PCA" scheme="http://yoursite.com/tags/PCA/"/>
    
  </entry>
  
  <entry>
    <title>【Machine Learning】11 聚类(Clustering)</title>
    <link href="http://yoursite.com/2020/07/19/[Machine%20Learning]%2011%20%E8%81%9A%E7%B1%BB(Clustering)/"/>
    <id>http://yoursite.com/2020/07/19/[Machine%20Learning]%2011%20%E8%81%9A%E7%B1%BB(Clustering)/</id>
    <published>2020-07-19T06:54:21.000Z</published>
    <updated>2020-07-19T06:56:01.549Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://img-blog.csdnimg.cn/20200719144836596.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><a id="more"></a><h2 id="11-Clustering-聚类"><a href="#11-Clustering-聚类" class="headerlink" title="11 Clustering(聚类)"></a>11 Clustering(聚类)</h2><h3 id="11-1-Unsupervised-Learning-Introduction"><a href="#11-1-Unsupervised-Learning-Introduction" class="headerlink" title="11.1 Unsupervised Learning Introduction"></a>11.1 Unsupervised Learning Introduction</h3><p>在一个典型的监督学习中，训练集是有标签的，目标是找到能够区分正样本和负样本的决策边界，需要据此拟合一个假设函数。<br><img src="https://img-blog.csdnimg.cn/20200719131513611.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>与此不同的是，在非监督学习中，数据没有附带任何标签。<br><img src="https://img-blog.csdnimg.cn/20200719131548265.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>这里的一系列点是没有标签的。因此，训练集可以写成只有𝑥<sup>(1)</sup>,𝑥<sup>(2)</sup>…..一直到𝑥<sup>(𝑚)</sup>。也就是说，在非监督学习中，需要将一系列无标签的训练数据，输入到一个算法中，然后告诉这个算法，快去为我们找找这个数据的内在结构给定数据。图上的数据看起来可以分成两个分开的点集（称为簇），一个能够找到圈出的这些点集的算法，就被称为聚类算法。</p><p>那么聚类算法一般用来做什么呢？<br><img src="https://img-blog.csdnimg.cn/20200719131923129.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><ul><li>市场分割。在数据库中存储了许多客户的信息，希望将他们分成不同的客户群，这样可以对不同类型的客户分别销售产品或者分别提供更适合的服务。</li><li>社交网络分析。事实上有许多研究人员正在研究这样一些内容，他们关注社交网络，例如 Facebook，Google+，或者是其他的一些信息，比如说：你经常跟哪些人联系，而这些人又经常给哪些人发邮件，由此找到关系密切的人群。因此，这可能需要另一个聚类算法，通过它发现社交网络中关系密切的朋友。</li><li>更好的组织计算机集群，或者更好的管理数据中心。因为如果知道数据中心中哪些计算机经常协作工作。那么，可以重新分配资源，重新布局网络。由此优化数据中心，优化数据通信。</li><li>了解星系的形成。然后用这个知识，了解一些天文学上的细节问题。</li></ul><h3 id="11-2-K-Means-Algorithm"><a href="#11-2-K-Means-Algorithm" class="headerlink" title="11.2 K-Means Algorithm"></a>11.2 K-Means Algorithm</h3><p>K-均值是最普及的聚类算法，算法接收一个未标记的数据集，然后将数据聚类成不同的组。</p><p>K-均值是一个迭代算法，假设我们想要将数据聚类成 n 个组，其方法为:</p><ol><li>首先选择𝐾个随机的点，称为聚类中心（cluster centroids）；</li><li>对于数据集中的每一个数据，按照距离𝐾个中心点的距离，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类。</li><li>计算每一个组的平均值，将该组所关联的中心点移动到平均值的位置。</li><li>重复步骤 2-4 直至中心点不再变化。</li></ol><p>下面是一个聚类示例：<br><img src="https://img-blog.csdnimg.cn/20200719140308547.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200719140330295.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>迭代多次以后，结果为：<img src="https://img-blog.csdnimg.cn/20200719140423526.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>用𝜇<sup>1</sup>,𝜇<sup>2</sup>,…,𝜇<sup>𝑘</sup> 来表示聚类中心，用𝑐<sup>(1)</sup>,𝑐<sup>(2)</sup>,…,𝑐<sup>(𝑚)</sup>来存储与第𝑖个实例数据最近的聚类中心的索引，K-均值算法的伪代码如下：<br><img src="https://img-blog.csdnimg.cn/20200719140716133.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>算法分为两个步骤，第一个 for 循环是赋值步骤，即：对于每一个样例𝑖，计算其应该属于的类。第二个 for 循环是聚类中心的移动，即：对于每一个类𝐾，重新计算该类的质心。</p><p>K-均值算法也可以很便利地用于将数据分为许多不同组，即使在没有非常明显区分的组群的情况下也可以。下图所示的数据集包含身高和体重两项特征构成的，利用 K-均值算法将数据分为三类，用于帮助确定将要生产的 T-恤衫的三种尺寸。<br><img src="https://img-blog.csdnimg.cn/20200719140859116.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="11-3-Optimization-Objective"><a href="#11-3-Optimization-Objective" class="headerlink" title="11.3 Optimization Objective"></a>11.3 Optimization Objective</h3><p>K-均值最小化问题，是要最小化所有的数据点与其所关联的聚类中心点之间的距离之和，因此 K-均值的代价函数（又称畸变函数 Distortion function）为：<br><img src="https://img-blog.csdnimg.cn/20200719141101932.PNG#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200719142033802.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>其中𝜇<sub>𝑐</sub>(𝑖)代表与𝑥(𝑖)最近的聚类中心点。 我们的优化目标便是找出使得代价函数最小的 𝑐<sup>(1)</sup>,𝑐<sup>(2)</sup>,…,𝑐<sup>(𝑚)</sup>和𝜇<sup>1</sup>,𝜇<sup>2</sup>,…,𝜇<sup>𝑘</sup> ：<br><img src="https://img-blog.csdnimg.cn/20200719141605420.PNG#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200719141817692.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>第一个循环是用于减小𝑐(𝑖)引起的代价，而第二个循环则是用于减小𝜇𝑖引起的代价。迭代的过程一定会是每一次迭代都在减小代价函数，不然便是出现了错误。</p><h3 id="11-4-Random-Initialization"><a href="#11-4-Random-Initialization" class="headerlink" title="11.4 Random Initialization"></a>11.4 Random Initialization</h3><p>在运行 K-均值算法的之前，首先要随机初始化所有的聚类中心点，具体步骤：</p><ol><li>应该选择𝐾 &lt; 𝑚，即聚类中心点的个数要小于所有训练集实例的数量</li><li>随机选择𝐾个训练实例，然后令𝐾个聚类中心分别与这𝐾个训练实例相等</li></ol><p>K-均值的一个问题在于，它有可能会停留在一个局部最小值处，而这取决于初始化的情况。<br><img src="https://img-blog.csdnimg.cn/20200719142726248.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>为了解决这个问题，通常需要多次运行 K-均值算法，每一次都重新进行随机初始化，最后再比较多次运行 K-均值的结果，选择代价函数最小的结果。这种方法在𝐾较小的时候（2–10）还是可行的，但是如果𝐾较大，这么做也可能不会有明显地改善。<br><img src="https://img-blog.csdnimg.cn/20200719143202386.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="11-5-Choosing-the-Number-of-Clusters"><a href="#11-5-Choosing-the-Number-of-Clusters" class="headerlink" title="11.5 Choosing the Number of Clusters"></a>11.5 Choosing the Number of Clusters</h3><p>没有所谓最好的选择聚类数的方法，通常是需要根据不同的问题，人工进行选择的。选择的时候思考运用 K-均值算法聚类的动机是什么，然后选择能最好服务于该目的标聚类数。</p><p>当人们在讨论，选择聚类数目的方法时，有一个可能会谈及的方法叫作“肘部法则”。关于“肘部法则”，我们所需要做的是改变𝐾值，也就是聚类类别数目的总数。用一个聚类来运行 K 均值聚类方法。这就意味着，所有的数据都会分到一个聚类里，然后计算成本函数或者计算畸变函数𝐽。<br><img src="https://img-blog.csdnimg.cn/20200719144322108.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>上图左边一条类似于一个人的肘部的曲线，这就是 <label style="color:red">“肘部法则”</label> 所做的。好像人的手臂，如果你伸出胳膊，那么这就是你的肩关节、肘关节、手。这就是“肘部法则”。在这种模式下，它的畸变值会迅速下降，从 1 到 2，从 2 到 3 之后，你会在 3 的时候达到一个肘点。在此之后，畸变值就下降的非常慢，看起来就像使用 3 个聚类来进行聚类是正确的，这是因为那个点是曲线的肘点，畸变值下降得很快，𝐾 = 3之后就下降得很慢，那么我们就选𝐾 = 3。当应用“肘部法则”的时候，如果得到了一个像上面这样的图，那么这将是一种用来选择聚类个数的合理方法。</p><p>例如，在 T-恤制造例子中，将用户按照身材聚类，可以分成 3 个尺寸:𝑆, 𝑀, 𝐿，也可以分成 5 个尺寸𝑋𝑆, 𝑆, 𝑀, 𝐿,𝑋𝐿，这样的选择是建立在回答“聚类后制造的 T-恤是否能较好地适合客户”这个问题的基础上作出的。<img src="https://img-blog.csdnimg.cn/20200719144333799.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="聚类参考资料"><a href="#聚类参考资料" class="headerlink" title="聚类参考资料"></a>聚类参考资料</h3><p><img src="https://img-blog.csdnimg.cn/20200719145128563.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200719145150498.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200719145204814.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20200719144836596.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center&quot; alt=&quot;在这里插入图片描述&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
      <category term="Clustering" scheme="http://yoursite.com/tags/Clustering/"/>
    
      <category term="K-Means" scheme="http://yoursite.com/tags/K-Means/"/>
    
  </entry>
  
  <entry>
    <title>【Machine Learning】10 支持向量机(Support Vector Machines)</title>
    <link href="http://yoursite.com/2020/07/18/[Machine%20Learning]%2010%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA(Support%20Vector%20Machines)/"/>
    <id>http://yoursite.com/2020/07/18/[Machine%20Learning]%2010%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA(Support%20Vector%20Machines)/</id>
    <published>2020-07-18T10:19:53.000Z</published>
    <updated>2020-07-19T06:55:16.544Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://img-blog.csdnimg.cn/20200718181732448.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><a id="more"></a><h2 id="10-Support-Vector-Machines-支持向量机"><a href="#10-Support-Vector-Machines-支持向量机" class="headerlink" title="10 Support Vector Machines(支持向量机)"></a>10 Support Vector Machines(支持向量机)</h2><h3 id="10-1-Optimization-Objective-优化目标"><a href="#10-1-Optimization-Objective-优化目标" class="headerlink" title="10.1  Optimization Objective(优化目标)"></a>10.1  Optimization Objective(优化目标)</h3><p>在监督学习中，许多学习算法的性能都非常类似，因此，重要的不是该选择使用学习算法 A 还是学习算法 B，而是应用这些算法时，所创建的大量数据表现情况通常依赖于你的水平。比如：你为学习算法所设计的特征量的选择，以及如何选择正则化参数，诸如此类的事。还有一个更加强大的算法广泛的应用于工业界和学术界，它被称为<label style="color:red">支持向量机(Support Vector Machine)</label>。与逻辑回归和神经网络相比，支持向量机，或者简称 SVM，在学习复杂的非线性方程时提供了一种更为清晰，更加强大的方式。</p><p>为了描述支持向量机，将从逻辑回归开始展示如何一点一点修改来得到本质上的支持向量机。<br><img src="https://img-blog.csdnimg.cn/20200716133202535.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>现在考虑下想要逻辑回归做什么：如果有一个 𝑦 = 1的样本，不管是在训练集中或是在测试集中，又或者在交叉验证集中，总之是 𝑦 = 1，现在希望ℎ𝜃(𝑥) 趋近 1。因为想要正确地将此样本分类，这就意味着当 ℎ𝜃(𝑥)趋近于 1 时，𝜃<sup>𝑇</sup>𝑥 应当远大于 0，此时逻辑回归的输出将趋近于 1。相反地，如果有另一个样本，即𝑦 = 0。希望假设函数的输出值将趋近于 0，这对应于𝜃<sup>𝑇</sup>𝑥 应远小于 0，对应的假设函数的输出值趋近 0。</p><p><img src="https://img-blog.csdnimg.cn/20200716133758296.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>如果进一步观察逻辑回归的代价函数，会发现每个样本 (𝑥, 𝑦)都会为总代价函数，增加这里的一项，<img src="https://img-blog.csdnimg.cn/20200716135237497.png#pic_center" alt="在这里插入图片描述"><br>因此，对于总代价函数通常会有对所有的训练样本求和，并且这里还有一个1/𝑚项，但是，在逻辑回归中，这里的这一项就是表示一个训练样本所对应的表达式。如果将完整定义的假设函数代入这里。那么，就会得到每一个训练样本都影响这一项。现在，先忽略 1/𝑚 这一项，一起来考虑两种情况：一种是𝑦等于 1 的情况；另一种是 𝑦 等于 0 的情况。</p><p>在第一种情况中，假设 𝑦 = 1 ，此时在目标函数中只需有第一项起作用，因为𝑦 = 1时，(1 − 𝑦)项将等于 0。因此，当在 𝑦 = 1 的样本中时，得到<img src="https://img-blog.csdnimg.cn/20200716135757484.PNG#pic_center" alt="在这里插入图片描述"><br>这样一项。</p><p>用 𝑧 表示𝜃<sup>𝑇</sup>𝑥 ，即： 𝑧 = 𝜃<sup>𝑇</sup>𝑥 。画出关于𝑧 的函数，得到左下角的这条曲线，可以看到，当𝑧 增大时，对应的值会变的非常小。对整个代价函数而言，影响也非常小。这也就解释了，为什么逻辑回归在观察到正样本𝑦 = 1时，试图将𝜃<sup>𝑇</sup>𝑥 设置得非常大。因为，在代价函数中的这一项会变的非常小。</p><p>现在开始建立支持向量机，从这个代价函数开始，也就是<img src="https://img-blog.csdnimg.cn/20200716135757484.PNG#pic_center" alt="在这里插入图片描述"><br>一点一点修改，取这里的𝑧 = 1点，先画出将要用的代价函数。新的代价函数（紫红色）将会水平的从z=1到右边，然后再在左边画一条同逻辑回归非常相似的直线。那么，到了这里已经非常接近逻辑回归中使用的代价函数了。只是这里是由两条线段组成，即位于右边的水平部分和位于左边的直线部分，先不考虑左边直线部分的斜率，这并不是很重要。<br><img src="https://img-blog.csdnimg.cn/20200716140253965.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>这同逻辑回归中类似的事情，并且为支持向量机，带来计算上的优势。例如，更容易计算股票交易的问题等等。</p><p>当𝑦 = 0时，此时代价函数只留下了第二项。<br><img src="https://img-blog.csdnimg.cn/20200716141707606.PNG#pic_center" alt="在这里插入图片描述"><br>并且，如果将这一项作为𝑧的函数，那么，就会得到横轴𝑧。同理得到如下图像。<br><img src="https://img-blog.csdnimg.cn/20200716141839387.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>如果用一个新的代价函数来代替，即这条从 0 点开始的水平直线，然后是一条斜线，像上图。那么，现在给这两个方程命名，左边的函数，称之为cos𝑡1(𝑧)，同时，右边函数我称它为cos𝑡0(𝑧)。这里的下标是指在代价函数中，对应的 𝑦 = 1 和 𝑦 = 0 的情况，拥有了这些定义后，现在就开始构建支持向量机。<br><img src="https://img-blog.csdnimg.cn/20200716142211885.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>这是在逻辑回归中使用代价函数𝐽(𝜃)，对于支持向量机而言，实质上要将这替换为cos𝑡<sub>1</sub>(𝑧)，也就是cos𝑡<sub>1</sub>(𝜃<sup>𝑇</sup>𝑥)，同样地，我也将这一项替换为cos𝑡<sub>0</sub>(𝑧)，也就是代价cos𝑡<sub>0</sub>(𝜃<sup>𝑇</sup>𝑥 )。因此，对于支持向量机，得到了这里的最小化问题，即:<br><img src="https://img-blog.csdnimg.cn/202007161432520.PNG#pic_center" alt="在这里插入图片描述"><br>首先，要除去1/𝑚这一项，这仅仅是由于人们使用支持向量机时，对比于逻辑回归而言，不同的习惯所致。1/𝑚 仅是个常量，因此，在这个最小化问题中，无论前面是否有1/𝑚 这一项，最终所得到的最优值𝜃都是一样的。</p><p>第二点概念上的变化，只是指在使用支持向量机时，一些如下的标准惯例，而不是逻辑回归。对于逻辑回归，在目标函数中，有两项：第一个是训练样本的代价，第二个是正则化项，我们不得不去用这一项来平衡。这就相当于想要最小化𝐴加上正则化参数𝜆，然后乘以其他项𝐵。这里的𝐴、B分别表示第一项和第二项，但不包括𝜆。这里实际上不是优化𝐴 + 𝜆 × 𝐵，而是通过设置不同正则参数𝜆达到优化目的。这样，就能够权衡对应的项，使得训练样本拟合的更好。即最小化𝐴。但对于支持向量机，使用C替换这里使用的𝜆来权衡这两项，同时改为优化目标，𝐶 × 𝐴 + 𝐵。在逻辑回归中，如果给定𝜆，一个非常大的值，意味着给予 B 更大的权重。而这里，就对应于将𝐶 设定为非常小的值，那么，相应的将会给𝐵比给𝐴更大的权重。因此，这只是一种不同的方式来控制这种权衡或者一种不同的方法，即用参数来决定是更关心第一项的优化，还是更关心第二项的优化。当然也可以把这里的参数𝐶 考虑成1/𝜆，同 1/𝜆所扮演的角色相同，并且这两个方程或这两个表达式并不相同，因为𝐶 = 1/𝜆，但是也并不全是这样，如果当𝐶 = 1/𝜆时，这两个优化目标应当得到相同的值，相同的最优值 𝜃。那么，𝜆用常数𝐶来代替，这就得到了在支持向量机中的整个优化目标函数，然后最小化这个目标函数，得到 SVM 学习到的参数𝐶。<img src="https://img-blog.csdnimg.cn/20200716143445357.PNG#pic_center" alt="在这里插入图片描述">)<img src="https://img-blog.csdnimg.cn/2020071614341664.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>最后有别于逻辑回归输出的概率。当最小化代价函数，获得参数𝜃时，支持向量机所做的是它来直接预测𝑦的值等于 1，还是等于 0。当𝜃<sup>𝑇</sup>𝑥大于或者等于 0 时，这个假设函数会预测 1；否则为0。</p><h3 id="10-2-Large-Margin-Intuition-直观上对大间距的理解"><a href="#10-2-Large-Margin-Intuition-直观上对大间距的理解" class="headerlink" title="10.2 Large Margin Intuition(直观上对大间距的理解)"></a>10.2 Large Margin Intuition(直观上对大间距的理解)</h3><p>人们有时将支持向量机看作是大间距分类器。<br><img src="https://img-blog.csdnimg.cn/20200716151046664.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>这是支持向量机模型的代价函数，左边是关于 z 的代价函cos𝑡1(𝑧)，此函数用于正样本，而在右边是关于𝑧的代价函数cos𝑡0(𝑧)。</p><p>现在思考一下，最小化这些代价函数的必要条件是什么。如果有一个正样本，𝑦 = 1，则只有在𝑧 &gt;= 1时，代价函数cos𝑡1(𝑧)才等于 0。反之，如果𝑦 = 0，函数cos𝑡0(𝑧)只有在𝑧 &lt;= 1的区间里函数值为 0。事实上，如果一个正样本𝑦 = 1，则其实仅仅要求𝜃<sup>𝑇</sup>𝑥大于等于 0，就能将该样本恰当分出，这是因为如果𝜃<sup>𝑇</sup>𝑥&gt;0 大的话，模型代价函数值为 0，类似地，如果有一个负样本，则仅需要𝜃<sup>𝑇</sup>𝑥&lt;=0 就会将负例正确分离，但是，支持向量机的要求更高，不仅仅要能正确分开输入的样本。即不仅仅要求𝜃<sup>𝑇</sup>𝑥&gt;0，要的是比 0 值大很多，比如大于等于1，也想这个比 0 小很多，比如希望它小于等于-1，这就相当于在支持向量机中嵌入了一个<label style="color:red">额外的安全因子</label>，或者说<label style="color:red">安全的间距因子</label>。</p><p>在支持向量机中，这个因子会导致什么结果。例如，将这个常数𝐶设置成一个非常大的值，比如假设𝐶的值为 100000 或者其它非常大的数，然后来观察支持向量机会给出什么结果？<br><img src="https://img-blog.csdnimg.cn/20200716154456301.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>如果 𝐶非常大，则最小化代价函数的时候，将会很希望找到一个使第一项为 0 的最优解。因此，假设第一项为 0的情形下理解该优化问题 。可以知道输入一个训练样本标签为𝑦 = 1，想令第一项为 0，需要做的是找到一个𝜃，使得𝜃<sup>𝑇</sup>𝑥 &gt;= 1，类似地，对于一个训练样本，标签为𝑦 = 0，为了使cos𝑡0(𝑧) 函数的值为 0，需要𝜃<sup>𝑇</sup>𝑥 &lt;= −1。现在考虑的优化问题，将选择参数使第一项为 0，得𝐶乘以 0 加上二分之一乘以第二项。这里第一项是𝐶乘以 0，因此可以将其删去。这将遵从以下的约束：如果 𝑦(𝑖)是等于 1 ，𝜃<sup>𝑇</sup>𝑥(𝑖) &gt;= 1；如果 𝑦(𝑖)是等于 0，𝜃<sup>𝑇</sup>𝑥(𝑖) &lt;= −1。这样当求解这个优化问题的时候，会得到一个非常有趣的决策边界。<br><img src="https://img-blog.csdnimg.cn/20200716155700363.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>具体而言，如果一个数据集中有正样本，也有负样本，可以看到这个数据集是线性可分的。即，存在一条直线把正负样本分开。当然有多条不同的直线，可以把正样本和负样本完全分开。</p><p>另外，也存在仅仅可以勉强将正样本和负样本分开的决策边界，这些决策边界看起来都不是特别好的选择。支持向量机将会选择这个黑色的决策边界，因为在分离正样本和负样本上，相较于粉色或者绿色画的决策界，黑色显得的更好。从数学的角度来说，这条黑线有更大的距离，这个距离叫做间距(margin)。<br><img src="https://img-blog.csdnimg.cn/20200716155952938.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>当画出两条额外的蓝线，可以看到黑色的决策边界和训练样本之间有更大的最短距离。然而粉线和蓝线离训练样本就非常近，在分离样本的时候就会比黑线表现差。因此，这个距离叫做<label style="color:red">支持向量机的间距</label>，而这是支持向量机具有鲁棒性的原因，因为它努力用一个最大间距来分离样本。因此支持向量机有时被称为大间距分类器。</p><blockquote><p>鲁棒是Robust的音译，也就是健壮和强壮的意思。它也是在异常和危险情况下系统生存的能力。比如说，计算机软件在输入错误、磁盘故障、网络过载或有意攻击情况下，能否不死机、不崩溃，就是该软件的鲁棒性。所谓“鲁棒性”，也是指控制系统在一定（结构，大小）的参数摄动下，维持其它某些性能的特性。</p></blockquote><p>如上所述，将这个大间距分类器中的正则化因子常数𝐶设置的非常大，将选择这样的决策界，从而最大间距地分离开正样本和负样本。那么在让代价函数最小化的过程中，希望找出在𝑦 = 1和𝑦 = 0两种情况下都使得代价函数中左边的这一项尽量为零的参数。如果找到了这样的参数，则最小化问题便转变成：<br><img src="https://img-blog.csdnimg.cn/20200716161707282.PNG#pic_center" alt="在这里插入图片描述"><br>事实上，支持向量机现在要比这个大间距分类器所体现得更成熟，尤其是当使用大间距分类器的时候，学习算法会受异常点(outlier) 的影响。比如加入一个额外的正样本。<br><img src="https://img-blog.csdnimg.cn/20200716161955550.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>此时，为了将样本用最大间距分开，也许最终会得到一条类似粉色这样的决策边界。仅仅基于一个异常值，仅仅基于一个样本，就将决策边界从黑线变成粉线，这实在是不明智的。事实上，将正则化参数𝐶设置的非常大，正是支持向量机将会做的。但是如果C设置的小一点，则最终会得到这条黑线，当然数据如果不是线性可分的，正样本和负样本混在一起，则支持向量机也会将它们恰当分开。因此，大间距分类器的描述，仅仅是从直观上给出了正则化参数𝐶非常大的情形，同时，记住𝐶的作用类似于1/𝜆，𝜆是之前使用过的正则化参数。这只是𝐶非常大的情形，或者等价地 𝜆 非常小的情形。最终会得到类似粉线这样的决策界，但是实际上应用支持向量机的时候，当𝐶不是非常非常大的时候，它可以忽略掉一些异常点的影响，得到更好的决策界。甚至当数据不是线性可分的时候，支持向量机也可以给出好的结果。</p><p>回顾 𝐶 = 1/𝜆，因此：</p><ul><li>𝐶 较大时，相当于 𝜆 较小，可能会导致过拟合，高方差。</li><li>𝐶 较小时，相当于 𝜆 较大，可能会导致低拟合，高偏差。</li></ul><p><img src="https://img-blog.csdnimg.cn/20200716161456209.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="10-3-Mathematics-Behind-Large-Margin-Classification"><a href="#10-3-Mathematics-Behind-Large-Margin-Classification" class="headerlink" title="10.3 Mathematics Behind Large Margin Classification"></a>10.3 Mathematics Behind Large Margin Classification</h3><p><img src="https://img-blog.csdnimg.cn/20200718143324125.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>下面就是先前给出的支持向量机模型中的目标函数。<br><img src="https://img-blog.csdnimg.cn/20200718143449168.PNG#pic_center" alt="在这里插入图片描述"><br>接下来忽略掉截距，令𝜃0 = 0，这样更容易画示意图。将特征数𝑛置为 2，因此仅有两个特征𝑥1, 𝑥2，式子可以写作：<img src="https://img-blog.csdnimg.cn/20200718144012599.PNG#pic_center" alt="在这里插入图片描述">只有两个参数𝜃1, 𝜃2。括号里面的这一项是向量𝜃的范数，或者说是向量𝜃的长度。即，如果将向量𝜃写出来，那么刚刚画红线的这一项就是向量𝜃的长度或范数。这里用的是之前学过的向量范数的定义，事实上这就等于向量𝜃的长度。<br><img src="https://img-blog.csdnimg.cn/20200718143708543.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>可以将其写作𝜃0, 𝜃1, 𝜃2，如果𝜃0 = 0，那就是𝜃1, 𝜃2的长度。忽略𝜃0，这样来写𝜃的范数，它仅仅和𝜃1, 𝜃2有关。但是，数学上不管你是否包含，其实并没有差别，因此在接下来的推导中去掉𝜃0不会有影响这意味着我们的目标函数是等于1/2 ||θ||<sup>2</sup>。因此支持向量机做的全部事情，就是<strong>极小化参数向量𝜃范数的平方，或者说长度的平方。</strong></p><p>可知，约束条件θ<sup>T</sup>x<sup>(i)</sup>&gt;=1或者θ<sup>T</sup>x<sup>(i)</sup>&lt;-1可以被p<sup>(i)</sup>·||θ||&gt;=1所替换。因为θ<sup>T</sup>x<sup>(i)</sup> = p<sup>(i)</sup>·||θ|| ，将其写入优化目标。将会得到没有了约束，θ<sup>T</sup>x<sup>(i)</sup>而变成了p<sup>(i)</sup>·||θ||。<br><img src="https://img-blog.csdnimg.cn/20200718144929142.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>比如下面这个样本，假设第一个样本是𝑥<sup>(1)</sup>，这个样本到参数𝜃的投影，是短的红线段，就等于𝑝<sup>(1)</sup>，它非常短。类似地，如果它恰好是𝑥<sup>(2)</sup>，则它到𝜃的投影是短的粉色线段是𝑝<sup>(2)</sup>，即第二个样本到参数向量𝜃的投影。𝑝<sup>(2)</sup>事实上是一个负值，𝑝<sup>(2)</sup>是在相反的方向，这个向量和参数向量𝜃的夹角大于 90 度。<br><img src="https://img-blog.csdnimg.cn/20200718151310395.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>可以发现这些𝑝<sup>(𝑖)</sup>将会是非常小的数，因此当考察优化目标函数的时候，对于正样本而言，需要p<sup>(i)</sup>·||θ|| &gt;= 1，但是如果 𝑝<sup>(𝑖)</sup>在这里非常小，那就意味着𝜃的范数非常大。因为如果 𝑝<sup>(1)</sup> 很小,而希望p<sup>(1)</sup>·||θ||&gt;= 1 ，令其实现的唯一的办法就是这两个数较大。如果 𝑝<sup>(1)</sup> 小，就希望𝜃的范数大。类似地，对于负样本而言需要p<sup>(2)</sup>·||θ||&lt;= −1，𝜃的范数变大。但是目标函数是希望找到一个参数𝜃，它的范数是小的。因此，这看起来不像是一个好的参数向量𝜃的选择。</p><p>相反地，看一个不同的决策边界。比如说，支持向量机选择了垂直线这个决策界，这就是相对应的参数𝜃的方向。根据线性代数的知识，可知这个绿色的决策界有一个垂直于它的向量𝜃。现在如果考察数据在横轴𝑥上的投影，比如样本𝑥<sup>(1)</sup>，当将它投影到横轴𝑥上，或投影到𝜃上，就会得到这样𝑝<sup>(1)</sup>，它的长度是𝑝<sup>(1)</sup>。另一个样本是𝑥<sup>(2)</sup>，投影发现，𝑝<sup>(2)</sup>的长度是负值。可以发现𝑝<sup>(1)</sup>和𝑝<sup>(2)</sup>这些投影长度比上例中长多了。如果仍然要满足约束，p<sup>(i)</sup>·||θ||&gt;1，则因为𝑝<sup>(1)</sup>变大了，𝜃的范数就可以变小了。因此这意味着通过选择右边的决策界，而不是左边的那个，支持向量机可以使参数𝜃的范数变小很多。因此，如果想令𝜃的范数变小，从而令𝜃范数的平方变小，就能让支持向量机选择右边的决策界。这就是支持向量机如何能有效地产生大间距分类的原因。<br><img src="https://img-blog.csdnimg.cn/20200718152948680.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>上图中这个绿色的决策界，满足正样本和负样本投影到𝜃的值大。这是大间距决策界来区分开正样本和负样本这个间距的值。这个间距的值就是𝑝<sup>(1)</sup>, 𝑝<sup>(2)</sup>, 𝑝<sup>(3)</sup>等等的值。通过让间距变大，即通过这些𝑝<sup>(1)</sup>, 𝑝<sup>(2)</sup>, 𝑝<sup>(3)</sup>等等的值，支持向量机最终可以找到一个较小的𝜃范数。这正是支持向量机中最小化目标函数的目的。</p><p>以上就是为什么支持向量机最终会找到大间距分类器的原因。因为它试图极大化这些𝑝<sup>(𝑖)</sup>的范数，它们是训练样本到决策边界的距离。最后一点，以上推导自始至终使用了简化假设——参数𝜃0 = 0。这个的作用是：𝜃0 = 0的意思是让决策界通过原点。如果令𝜃0不是 0 的话，含义就是希望决策界不通过原点。</p><p>另外，即便𝜃0不等于 0，支持向量机要做的事情都是优化这个目标函数对应着𝐶值非常大的情况，但是可以说明的是，即便𝜃0不等于 0，支持向量机仍然会找到正样本和负样本之间的大间距分隔。</p><h3 id="10-4-Kernels-核函数"><a href="#10-4-Kernels-核函数" class="headerlink" title="10.4 Kernels(核函数)"></a>10.4 Kernels(核函数)</h3><p><img src="https://img-blog.csdnimg.cn/20200718160658734.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>为了获得上图所示的判定边界，模型可能是𝜃0 + 𝜃1𝑥1 + 𝜃2𝑥2 + 𝜃3𝑥1𝑥2 + 𝜃4𝑥1<sup>2</sup> +𝜃5𝑥2<sup>2</sup> + ⋯的形式。</p><p>可以用一系列的新的特征 f 来替换模型中的每一项。例如令： 𝑓1 = 𝑥1, 𝑓2 = 𝑥2, 𝑓3 =𝑥1𝑥2, 𝑓4 = 𝑥1<sup>2</sup>, 𝑓5 = 𝑥2<sup>2</sup>…得到ℎ𝜃(𝑥) = 𝑓1 + 𝑓2+. . . +𝑓𝑛。然而，除了对原有的特征进行组合以外，有没有更好的方法来构造𝑓1, 𝑓2, 𝑓3？可以利用核函数来计算出新的特征。</p><p>给定一个训练实例 𝑥 ，我们利用 𝑥 的各个特征与我们预先选定的地标(landmarks) 𝑙<sup>(1)</sup>, 𝑙<sup>(2)</sup>, 𝑙<sup>(3)</sup>的近似程度来选取新的特征𝑓1, 𝑓2, 𝑓3。<br><img src="https://img-blog.csdnimg.cn/20200718161237110.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>例如，<img src="https://img-blog.csdnimg.cn/20200718161533756.PNG#pic_center" alt="在这里插入图片描述"><br>其中，<img src="https://img-blog.csdnimg.cn/20200718161600192.PNG#pic_center" alt="在这里插入图片描述"><br>为实例𝑥中所有特征与地标𝑙<sup>(1)</sup>之间的距离的和。</p><p>上例中的𝑠𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦(𝑥, 𝑙<sup>(1)</sup>)就是核函数，具体而言，这里是一个<label style="color:red">高斯核函数(Gaussian Kernel)</label>。注：这个函数与正态分布没什么实际上的关系，只是看上去像而已。</p><p>这些地标的作用是什么？如果一个训练实例𝑥与地标𝐿之间的距离近似于 0，则新特征f 近似于𝑒<sup>−0</sup> = 1，如果训练实例𝑥与地标𝐿之间距离较远，则𝑓近似于𝑒<sup>−(一个较大的数)</sup> = 0。</p><p>假设训练实例含有两个特征[𝑥1 𝑥2]，给定地标𝑙<sup>(1)</sup>与不同的𝜎值，见下图：<br><img src="https://img-blog.csdnimg.cn/2020071816570134.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>图中水平面的坐标为 𝑥1，𝑥2而垂直坐标轴代表𝑓。可以看出，只有当𝑥与𝑙<sup>(1)</sup>重合时𝑓才具有最大值。随着𝑥的改变𝑓值改变的速率受到𝜎<sup>2</sup>的控制。</p><p>在下图中，当实例处于洋红色的点位置处，因为其离𝑙<sup>(1)</sup>更近，但是离𝑙<sup>(2)</sup>和𝑙<sup>(3)</sup>较远，因此𝑓1接近 1，而𝑓2,𝑓3接近 0。因此ℎ𝜃(𝑥) = 𝜃0 + 𝜃1𝑓1 + 𝜃2𝑓2 + 𝜃1𝑓3 &gt; 0，预测𝑦 = 1。同理可以求出，对于离𝑙<sup>(2)</sup>较近的绿色点，也预测𝑦 = 1，但是对于蓝绿色的点，因为其离三个地标都较远，预测𝑦 = 0。<br><img src="https://img-blog.csdnimg.cn/20200718170436415.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>这样，图中红色的封闭曲线所表示的范围，便是依据一个单一的训练实例和选取的地标所得出的判定边界，在预测时，采用的特征不是训练实例本身的特征，而是通过核函数计算出的新特征𝑓1, 𝑓2, 𝑓3。</p><p><strong>如何选择地标？</strong><br>通常是根据训练集的数量选择地标的数量，即如果训练集中有𝑚个实例，则选<br>取𝑚个地标，并且令:𝑙<sup>(1)</sup> = 𝑥<sup>(1)</sup>, 𝑙<sup>(2)</sup> = 𝑥<sup>(2)</sup>, . . . . . , 𝑙<sup>(m)</sup> = 𝑥<sup>(m)</sup>。这样做的好处在于：得到的新特征是建立在原有特征与训练集中所有其他特征之间距离的基础之上的，即：<br><img src="https://img-blog.csdnimg.cn/20200718172124963.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200718172407635.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>下面将核函数运用到支持向量机中，修改支持向量机假设为：<br>给定𝑥，计算新特征𝑓，当𝜃<sup>𝑇</sup>𝑓 &gt;= 0 时，预测 𝑦 = 1，否则反之。</p><p>相应地修改代价函数为：<img src="https://img-blog.csdnimg.cn/20200718172655971.PNG#pic_center" alt="在这里插入图片描述"><br>，<br><img src="https://img-blog.csdnimg.cn/20200718172729787.PNG#pic_center" alt="在这里插入图片描述"><br>在具体实施过程中，还需要对最后的正则化项进行些微调整，在计算<img src="https://img-blog.csdnimg.cn/20200718172813904.PNG#pic_center" alt="在这里插入图片描述"><br>时，用𝜃<sup>𝑇</sup>𝑀𝜃代替𝜃<sup>𝑇</sup>𝜃，其中𝑀是根据选择的核函数而不同的一个矩阵。这样做的原因是为了简化计算。理论上讲，也可以在逻辑回归中使用核函数，但是上面使用 𝑀来简化计算的方法不适用与逻辑回归，因此计算将非常耗费时间。</p><p>另外，支持向量机也可以不使用核函数，不使用核函数又称为<label style="color:red">线性核函数(linear kernel)</label>，当不采用非常复杂的函数，或者训练集特征非常多而实例非常少的时候，可以采用这种不带核函数的支持向量机。</p><p>下面是支持向量机的两个参数𝐶和𝜎的影响：</p><ul><li>𝐶 = 1/𝜆</li><li>𝐶 较大时，相当于𝜆较小，可能会导致过拟合，高方差；</li><li>𝐶 较小时，相当于𝜆较大，可能会导致低拟合，高偏差；</li><li>𝜎较大时，可能会导致低方差，高偏差；</li><li>𝜎较小时，可能会导致低偏差，高方差。</li></ul><h3 id="10-5-Using-An-SVM"><a href="#10-5-Using-An-SVM" class="headerlink" title="10.5 Using An SVM"></a>10.5 Using An SVM</h3><p>在高斯核函数之外还有其他一些选择，如：<br>多项式核函数（Polynomial Kernel）<br>字符串核函数（String kernel）<br>卡方核函数（ chi-square kernel）<br>直方图交集核函数（histogram intersection kernel）<br>等等…<br>这些核函数的目标也都是根据训练集和地标之间的距离来构建新特征，这些核函数需要满足 Mercer’s 定理，才能被支持向量机的优化软件正确处理。</p><p><strong>多类分类问题</strong><br>假设利用之前介绍的一对多方法来解决一个多类分类问题。如果一共有𝑘个类，则需要𝑘个模型，以及𝑘个参数向量𝜃。同样也可以训练𝑘个支持向量机来解决多类分类问题。但是大多数支持向量机软件包都有内置的多类分类功能，我们只要直接使用即可。</p><p>尽管不需要自己写 SVM 的优化软件，但是也需要做几件事：<br>1、是提出参数𝐶的选择。<br>2、需要选择内核参数或想要使用的相似函数，其中一个选择是：选择不需要任何内核参数，没有内核参数的理念，也叫线性核函数。因此，如果有人说他使用了线性核的 SVM，这就意味这他使用了不带有核函数的 SVM。</p><p><strong>在逻辑回归模型和支持向量机模型之间，应该如何选择？</strong></p><p>下面是一些普遍使用的准则：<br>𝑛为特征数，𝑚为训练样本数。</p><ul><li>(1)如果相较于𝑚而言，𝑛要大许多，即训练集数据量不够支持我们训练一个复杂的非线性模型，我们选用逻辑回归模型或者不带核函数的支持向量机。</li><li>(2)如果𝑛较小，而且𝑚大小中等，例如𝑛在 1-1000 之间，而𝑚在 10-10000 之间，使用高斯核函数的支持向量机。</li><li>(3)如果𝑛较小，而𝑚较大，例如𝑛在 1-1000 之间，而𝑚大于 50000，则使用支持向量机会非常慢，解决方案是创造、增加更多的特征，然后使用逻辑回归或不带核函数的支持向量机。</li><li>值得一提的是，神经网络在以上三种情况下都可能会有较好的表现，但是训练神经网络可能非常慢，选择支持向量机的原因主要在于它的代价函数是凸函数，不存在局部最小值。</li><li>当有非常非常大的训练集，且用高斯核函数是在这种情况下，通常会尝试手动地创建，拥有更多的特征变量，然后用逻辑回归或者不带核函数的支持向量机。</li></ul><p>逻辑回归和不带核函数的支持向量机是非常相似的算法，不管是逻辑回归还是不带核函数的 SVM，通常都会做相似的事情，并给出相似的结果。根据实现的情况，其中一个可能会比另一个更加有效，但是在其中一个算法应用的地方，逻辑回归或不带核函数的SVM 另一个也很有可能很有效。随着 SVM 的复杂度增加，当使用不同的内核函数来学习复杂的非线性函数时。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://img-blog.csdnimg.cn/20200718181732448.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTM5OTA3NA==,size_16,color_FFFFFF,t_70#pic_center&quot; alt=&quot;在这里插入图片描述&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
      <category term="SVM" scheme="http://yoursite.com/tags/SVM/"/>
    
  </entry>
  
</feed>
